{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-10-24T11:07:25.310460Z",
     "iopub.status.busy": "2020-10-24T11:07:25.309571Z",
     "iopub.status.idle": "2020-10-24T11:07:26.277910Z",
     "shell.execute_reply": "2020-10-24T11:07:26.277310Z"
    },
    "papermill": {
     "duration": 0.993236,
     "end_time": "2020-10-24T11:07:26.278028",
     "exception": false,
     "start_time": "2020-10-24T11:07:25.284792",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "#sys.path.append('../iterative-stratification/iterative-stratification-master')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-10-24T11:08:27.448862Z",
     "iopub.status.busy": "2020-10-24T11:08:27.448012Z",
     "iopub.status.idle": "2020-10-24T11:08:27.794560Z",
     "shell.execute_reply": "2020-10-24T11:08:27.793499Z"
    },
    "papermill": {
     "duration": 0.387174,
     "end_time": "2020-10-24T11:08:27.794703",
     "exception": false,
     "start_time": "2020-10-24T11:08:27.407529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import optuna\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T11:08:27.851264Z",
     "iopub.status.busy": "2020-10-24T11:08:27.850256Z",
     "iopub.status.idle": "2020-10-24T11:08:34.871735Z",
     "shell.execute_reply": "2020-10-24T11:08:34.870966Z"
    },
    "papermill": {
     "duration": 7.052344,
     "end_time": "2020-10-24T11:08:34.871859",
     "exception": false,
     "start_time": "2020-10-24T11:08:27.819515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('../lish-moa/train_features.csv')\n",
    "train_targets_scored = pd.read_csv('../lish-moa/train_targets_scored.csv')\n",
    "\n",
    "#label_smoothing\n",
    "train_targets_scored_sigid_value = train_targets_scored.loc[:,['sig_id']]\n",
    "train_targets_scored_float_value = train_targets_scored.loc[:,'5-alpha_reductase_inhibitor':'wnt_inhibitor']\n",
    "train_targets_scored_float_value_smooth = train_targets_scored_float_value.clip(0.0005, 0.9995)\n",
    "train_targets_scored = pd.concat([train_targets_scored_sigid_value, train_targets_scored_float_value_smooth], axis=1)\n",
    "\n",
    "train_targets_scored_forCV = pd.concat([train_targets_scored_sigid_value, train_targets_scored_float_value], axis=1)\n",
    "\n",
    "train_targets_nonscored = pd.read_csv('../lish-moa/train_targets_nonscored.csv')\n",
    "test_features = pd.read_csv('../lish-moa/test_features.csv')\n",
    "\n",
    "sample_submission = pd.read_csv('../lish-moa/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T11:08:34.922928Z",
     "iopub.status.busy": "2020-10-24T11:08:34.920886Z",
     "iopub.status.idle": "2020-10-24T11:08:34.923635Z",
     "shell.execute_reply": "2020-10-24T11:08:34.924118Z"
    },
    "papermill": {
     "duration": 0.031016,
     "end_time": "2020-10-24T11:08:34.924264",
     "exception": false,
     "start_time": "2020-10-24T11:08:34.893248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in train_features.columns if col.startswith('c-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T11:08:34.987758Z",
     "iopub.status.busy": "2020-10-24T11:08:34.986588Z",
     "iopub.status.idle": "2020-10-24T11:08:34.994837Z",
     "shell.execute_reply": "2020-10-24T11:08:34.995988Z"
    },
    "papermill": {
     "duration": 0.048998,
     "end_time": "2020-10-24T11:08:34.996194",
     "exception": false,
     "start_time": "2020-10-24T11:08:34.947196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=1903):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=1903)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T11:08:35.083365Z",
     "iopub.status.busy": "2020-10-24T11:08:35.082470Z",
     "iopub.status.idle": "2020-10-24T11:08:37.660254Z",
     "shell.execute_reply": "2020-10-24T11:08:37.659223Z"
    },
    "papermill": {
     "duration": 2.62786,
     "end_time": "2020-10-24T11:08:37.660381",
     "exception": false,
     "start_time": "2020-10-24T11:08:35.032521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GENES\n",
    "n_comp = 28\n",
    "\n",
    "data = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\n",
    "data2 = (PCA(n_components=n_comp, random_state=1903).fit_transform(data[GENES]))\n",
    "train2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n",
    "\n",
    "train2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "test2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "\n",
    "# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\n",
    "train_features = pd.concat((train_features, train2), axis=1)\n",
    "test_features = pd.concat((test_features, test2), axis=1)\n",
    "\n",
    "#CELLS\n",
    "n_comp = 5\n",
    "\n",
    "data = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\n",
    "data2 = (PCA(n_components=n_comp, random_state=1903).fit_transform(data[CELLS]))\n",
    "train2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n",
    "\n",
    "train2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n",
    "test2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n",
    "\n",
    "# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\n",
    "train_features = pd.concat((train_features, train2), axis=1)\n",
    "test_features = pd.concat((test_features, test2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T11:08:37.716463Z",
     "iopub.status.busy": "2020-10-24T11:08:37.715568Z",
     "iopub.status.idle": "2020-10-24T11:08:39.389135Z",
     "shell.execute_reply": "2020-10-24T11:08:39.389808Z"
    },
    "papermill": {
     "duration": 1.708259,
     "end_time": "2020-10-24T11:08:39.390003",
     "exception": false,
     "start_time": "2020-10-24T11:08:37.681744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "\n",
    "var_thresh = VarianceThreshold(threshold=0.4)\n",
    "data = train_features.append(test_features)\n",
    "data_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n",
    "\n",
    "train_features_transformed = data_transformed[ : train_features.shape[0]]\n",
    "test_features_transformed = data_transformed[-test_features.shape[0] : ]\n",
    "\n",
    "\n",
    "train_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n",
    "                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "\n",
    "train_features_transformed = np.arcsinh(train_features_transformed)\n",
    "train_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n",
    "\n",
    "\n",
    "test_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n",
    "                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "\n",
    "test_features_transformed = np.arcsinh(test_features_transformed)\n",
    "test_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T11:08:39.460787Z",
     "iopub.status.busy": "2020-10-24T11:08:39.459196Z",
     "iopub.status.idle": "2020-10-24T11:08:39.975616Z",
     "shell.execute_reply": "2020-10-24T11:08:39.975033Z"
    },
    "papermill": {
     "duration": 0.556036,
     "end_time": "2020-10-24T11:08:39.975727",
     "exception": false,
     "start_time": "2020-10-24T11:08:39.419691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train_features.merge(train_targets_scored, on='sig_id')\n",
    "train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "\n",
    "target = train[train_targets_scored.columns]\n",
    "\n",
    "train = train.drop('cp_type', axis=1)\n",
    "test = test.drop('cp_type', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T11:08:40.045654Z",
     "iopub.status.busy": "2020-10-24T11:08:40.043814Z",
     "iopub.status.idle": "2020-10-24T11:08:40.046535Z",
     "shell.execute_reply": "2020-10-24T11:08:40.047016Z"
    },
    "papermill": {
     "duration": 0.047755,
     "end_time": "2020-10-24T11:08:40.047179",
     "exception": false,
     "start_time": "2020-10-24T11:08:39.999424",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_cols = target.drop('sig_id', axis=1).columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T11:08:40.103128Z",
     "iopub.status.busy": "2020-10-24T11:08:40.101888Z",
     "iopub.status.idle": "2020-10-24T11:08:42.209613Z",
     "shell.execute_reply": "2020-10-24T11:08:42.209020Z"
    },
    "papermill": {
     "duration": 2.139286,
     "end_time": "2020-10-24T11:08:42.209730",
     "exception": false,
     "start_time": "2020-10-24T11:08:40.070444",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "folds = train.copy()\n",
    "\n",
    "mskf = MultilabelStratifiedKFold(n_splits=7)\n",
    "\n",
    "for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n",
    "    folds.loc[v_idx, 'kfold'] = int(f)\n",
    "\n",
    "folds['kfold'] = folds['kfold'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T11:08:42.270348Z",
     "iopub.status.busy": "2020-10-24T11:08:42.269270Z",
     "iopub.status.idle": "2020-10-24T11:08:42.272700Z",
     "shell.execute_reply": "2020-10-24T11:08:42.272044Z"
    },
    "papermill": {
     "duration": 0.039455,
     "end_time": "2020-10-24T11:08:42.272817",
     "exception": false,
     "start_time": "2020-10-24T11:08:42.233362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MoADataset:\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
    "            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n",
    "        }\n",
    "        return dct\n",
    "    \n",
    "class TestDataset:\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n",
    "        }\n",
    "        return dct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T11:08:42.341379Z",
     "iopub.status.busy": "2020-10-24T11:08:42.340372Z",
     "iopub.status.idle": "2020-10-24T11:08:42.343342Z",
     "shell.execute_reply": "2020-10-24T11:08:42.342836Z"
    },
    "papermill": {
     "duration": 0.044383,
     "end_time": "2020-10-24T11:08:42.343448",
     "exception": false,
     "start_time": "2020-10-24T11:08:42.299065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    \n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "        #print(inputs.shape)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    \n",
    "    return final_loss\n",
    "\n",
    "\n",
    "def valid_fn(model, loss_fn, dataloader, device):\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    valid_preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    valid_preds = np.concatenate(valid_preds)\n",
    "    \n",
    "    return final_loss, valid_preds\n",
    "\n",
    "def inference_fn(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs = data['x'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "        \n",
    "        preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    preds = np.concatenate(preds)\n",
    "    \n",
    "    return preds\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T11:08:42.405400Z",
     "iopub.status.busy": "2020-10-24T11:08:42.399215Z",
     "iopub.status.idle": "2020-10-24T11:08:42.408450Z",
     "shell.execute_reply": "2020-10-24T11:08:42.407818Z"
    },
    "papermill": {
     "duration": 0.041854,
     "end_time": "2020-10-24T11:08:42.408554",
     "exception": false,
     "start_time": "2020-10-24T11:08:42.366700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features, num_targets, hidden_size, dropout1, dropout2, dropout3):\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dropout1 = nn.Dropout(dropout1)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(dropout2)\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout3 = nn.Dropout(dropout3)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = F.leaky_relu(self.dense1(x), negative_slope=0.01)\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = F.leaky_relu(self.dense2(x), negative_slope=0.01)\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T11:08:42.462903Z",
     "iopub.status.busy": "2020-10-24T11:08:42.461982Z",
     "iopub.status.idle": "2020-10-24T11:08:42.465193Z",
     "shell.execute_reply": "2020-10-24T11:08:42.464469Z"
    },
    "papermill": {
     "duration": 0.032758,
     "end_time": "2020-10-24T11:08:42.465302",
     "exception": false,
     "start_time": "2020-10-24T11:08:42.432544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T11:08:42.584294Z",
     "iopub.status.busy": "2020-10-24T11:08:42.580396Z",
     "iopub.status.idle": "2020-10-24T11:08:42.674728Z",
     "shell.execute_reply": "2020-10-24T11:08:42.674100Z"
    },
    "papermill": {
     "duration": 0.186381,
     "end_time": "2020-10-24T11:08:42.674843",
     "exception": false,
     "start_time": "2020-10-24T11:08:42.488462",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "906"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols = [c for c in process_data(folds).columns if c not in target_cols]\n",
    "feature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]\n",
    "len(feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T11:08:43.096788Z",
     "iopub.status.busy": "2020-10-24T11:08:43.095777Z",
     "iopub.status.idle": "2020-10-24T11:08:43.098307Z",
     "shell.execute_reply": "2020-10-24T11:08:43.098871Z"
    },
    "papermill": {
     "duration": 0.399942,
     "end_time": "2020-10-24T11:08:43.099001",
     "exception": false,
     "start_time": "2020-10-24T11:08:42.699059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HyperParameters\n",
    "\n",
    "DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 256\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "NFOLDS = 7\n",
    "EARLY_STOPPING_STEPS = 10\n",
    "EARLY_STOP = False\n",
    "\n",
    "num_features=len(feature_cols)\n",
    "num_targets=len(target_cols)\n",
    "hidden_size=1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T11:08:43.173795Z",
     "iopub.status.busy": "2020-10-24T11:08:43.166926Z",
     "iopub.status.idle": "2020-10-24T11:08:43.176825Z",
     "shell.execute_reply": "2020-10-24T11:08:43.176199Z"
    },
    "papermill": {
     "duration": 0.053238,
     "end_time": "2020-10-24T11:08:43.176940",
     "exception": false,
     "start_time": "2020-10-24T11:08:43.123702",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_training(fold,params):\n",
    "    \n",
    "    seed_everything(1903)\n",
    "    \n",
    "    train = process_data(folds)\n",
    "    test_ = process_data(test)\n",
    "    \n",
    "    trn_idx = train[train['kfold'] != fold].index\n",
    "    val_idx = train[train['kfold'] == fold].index\n",
    "    \n",
    "    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n",
    "    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n",
    "    \n",
    "    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n",
    "    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n",
    "    \n",
    "    train_dataset = MoADataset(x_train, y_train)\n",
    "    valid_dataset = MoADataset(x_valid, y_valid)\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets=num_targets,\n",
    "        hidden_size=params['hidden_size'],\n",
    "        dropout1=params['dropout1'],\n",
    "        dropout2=params['dropout2'],\n",
    "        dropout3=params['dropout3']\n",
    "    )\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    #optimizer = optim.Adagrad(model.parameters(), lr=0.1, lr_decay=0.001, weight_decay=1e-5, initial_accumulator_value=0, eps=1e-10)  \n",
    "    #scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, threshold=1e-8, eps=1e-10, verbose=True)\n",
    "    \n",
    "    lr_params = params['lr']\n",
    "    lr_decay_params = params['lr_decay']\n",
    "    weight_decay_params = params['weight_decay']\n",
    "    \n",
    "    factor_params = params['factor']\n",
    "    patience_params = params['patience']\n",
    "    \n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=lr_params, lr_decay=lr_decay_params, weight_decay=weight_decay_params, initial_accumulator_value=0, eps=1e-10)  \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=factor_params, patience=patience_params, threshold=1e-8, eps=1e-10, verbose=True)\n",
    "    \n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    early_stopping_steps = EARLY_STOPPING_STEPS\n",
    "    early_step = 0\n",
    "    \n",
    "    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        train_loss = train_fn(model, optimizer,scheduler, loss_fn, trainloader, DEVICE)\n",
    "        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n",
    "        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n",
    "        print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n",
    "        \n",
    "        scheduler.step(valid_loss)\n",
    "        #scheduler.step()\n",
    "        \n",
    "        if valid_loss < best_loss:\n",
    "            \n",
    "            best_loss = valid_loss\n",
    "            oof[val_idx] = valid_preds\n",
    "            #torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n",
    "        \n",
    "        elif(EARLY_STOP == True):\n",
    "            \n",
    "            early_step += 1\n",
    "            if (early_step >= early_stopping_steps):\n",
    "                break\n",
    "            \n",
    "    \n",
    "    return best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optuna import Trial, create_study\n",
    "\n",
    "def objective(trial: Trial) -> dict:\n",
    "    params={\n",
    "        \"hidden_size\": trial.suggest_int(\"hidden_size\", 512, 2048, step = 256, log = False),\n",
    "        \"dropout1\": trial.suggest_float(\"dropout1\", 0.1, 0.8, step = None, log = False),\n",
    "        \"dropout2\": trial.suggest_float(\"dropout2\", 0.1, 0.8, step = None, log = False),\n",
    "        \"dropout3\": trial.suggest_float(\"dropout3\", 0.1, 0.8, step = None, log = False),\n",
    "        #\"lr\": trial.suggest_loguniform(\"lr\",1e-2,1),\n",
    "        \"lr\": trial.suggest_float(\"lr\", 5e-2, 5e-1, step = None, log = False),\n",
    "        #\"lr_decay\": trial.suggest_loguniform(\"lr_decay\",1e-4,1e-2),\n",
    "        \"lr_decay\": trial.suggest_float(\"lr_decay\", 5e-4, 5e-3, step = None, log = False),\n",
    "        #\"weight_decay\": trial.suggest_loguniform(\"weight_decay\",1e-6,1e-4),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\",5e-6, 5e-5, step = None, log = False),\n",
    "        \"factor\": trial.suggest_float(\"factor\",0.1, 0.9, step = None, log = False),\n",
    "        \"patience\": trial.suggest_int(\"patience\",1,10, step = 1, log =False)\n",
    "    }\n",
    "    all_losses=[]\n",
    "    for f_ in range(7): #<-- 5folds\n",
    "        temp_loss=run_training(f_,params)\n",
    "        all_losses.append(temp_loss)\n",
    "    return np.mean(all_losses)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2020-11-11 12:25:18,174]\u001b[0m A new study created in memory with name: no-name-f3e7b05d-8bd1-4b36-9bc1-79052039146a\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 0, EPOCH: 0, train_loss: 0.04607089128143884\n",
      "FOLD: 0, EPOCH: 0, valid_loss: 0.026742163902291886\n",
      "FOLD: 0, EPOCH: 1, train_loss: 0.024663045675166556\n",
      "FOLD: 0, EPOCH: 1, valid_loss: 0.025656549976422235\n",
      "FOLD: 0, EPOCH: 2, train_loss: 0.024273096583783627\n",
      "FOLD: 0, EPOCH: 2, valid_loss: 0.0249568556363766\n",
      "FOLD: 0, EPOCH: 3, train_loss: 0.02405828222431041\n",
      "FOLD: 0, EPOCH: 3, valid_loss: 0.023822412467919864\n",
      "FOLD: 0, EPOCH: 4, train_loss: 0.02385869143983802\n",
      "FOLD: 0, EPOCH: 4, valid_loss: 0.024258543665592488\n",
      "FOLD: 0, EPOCH: 5, train_loss: 0.023625186568981892\n",
      "FOLD: 0, EPOCH: 5, valid_loss: 0.023386123231970347\n",
      "FOLD: 0, EPOCH: 6, train_loss: 0.02352020801422564\n",
      "FOLD: 0, EPOCH: 6, valid_loss: 0.02301327296747611\n",
      "FOLD: 0, EPOCH: 7, train_loss: 0.02334619394025287\n",
      "FOLD: 0, EPOCH: 7, valid_loss: 0.023302559525920793\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-ba43c2cc93c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"minimize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 self._optimize_sequential(\n\u001b[0;32m--> 339\u001b[0;31m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m                 )\n\u001b[1;32m    341\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(self, func, n_trials, timeout, catch, callbacks, gc_after_trial, time_start)\u001b[0m\n\u001b[1;32m    745\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial_and_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_progress_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial_and_callbacks\u001b[0;34m(self, func, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    774\u001b[0m     ) -> None:\n\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m         \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(self, func, catch, gc_after_trial)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Trial {} pruned. {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-2e06b68e80e2>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mall_losses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mf_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#<-- 5folds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mtemp_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mall_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-518792b0d494>\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(fold, params)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-7707b695e21b>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(model, optimizer, scheduler, loss_fn, dataloader, device)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfinal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-d8060671fb72>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         dct = {\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0;34m'x'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0;34m'y'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         }\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study=optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective,n_trials=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_226 = {'hidden_size': 1792, 'dropout1': 0.3003906575355876, 'dropout2': 0.6467618201427745, 'dropout3': 0.2573148373719613, 'lr': 0.17808542072654637, 'lr_decay': 0.004473634402592102, 'weight_decay': 5.018499734993564e-06, 'factor': 0.14966142745406366, 'patience': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_final_training(fold, params, seed):\n",
    "    \n",
    "    seed_everything(seed)\n",
    "    \n",
    "    train = process_data(folds)\n",
    "    test_ = process_data(test)\n",
    "    \n",
    "    trn_idx = train[train['kfold'] != fold].index\n",
    "    val_idx = train[train['kfold'] == fold].index\n",
    "    \n",
    "    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n",
    "    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n",
    "    \n",
    "    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n",
    "    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n",
    "    \n",
    "    train_dataset = MoADataset(x_train, y_train)\n",
    "    valid_dataset = MoADataset(x_valid, y_valid)\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets=num_targets,\n",
    "        hidden_size=params['hidden_size'],\n",
    "        dropout1=params['dropout1'],\n",
    "        dropout2=params['dropout2'],\n",
    "        dropout3=params['dropout3']\n",
    "    )\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    #optimizer = optim.Adagrad(model.parameters(), lr=0.1, lr_decay=0.001, weight_decay=1e-5, initial_accumulator_value=0, eps=1e-10)  \n",
    "    #scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, threshold=1e-8, eps=1e-10, verbose=True)\n",
    "    \n",
    "    lr_params = params['lr']\n",
    "    lr_decay_params = params['lr_decay']\n",
    "    weight_decay_params = params['weight_decay']\n",
    "    \n",
    "    factor_params = params['factor']\n",
    "    patience_params = params['patience']\n",
    "    \n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=lr_params, lr_decay=lr_decay_params, weight_decay=weight_decay_params, initial_accumulator_value=0, eps=1e-10)  \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=factor_params, patience=patience_params, threshold=1e-8, eps=1e-10, verbose=True)\n",
    "    \n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    early_stopping_steps = EARLY_STOPPING_STEPS\n",
    "    early_step = 0\n",
    "    \n",
    "    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        train_loss = train_fn(model, optimizer,scheduler, loss_fn, trainloader, DEVICE)\n",
    "        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n",
    "        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n",
    "        print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n",
    "        \n",
    "        scheduler.step(valid_loss)\n",
    "        #scheduler.step()\n",
    "        \n",
    "        if valid_loss < best_loss:\n",
    "            \n",
    "            best_loss = valid_loss\n",
    "            oof[val_idx] = valid_preds\n",
    "            torch.save(model.state_dict(), f\"moa-1867-SEED{seed}_FOLD{fold}.pth\")\n",
    "        \n",
    "        elif(EARLY_STOP == True):\n",
    "            \n",
    "            early_step += 1\n",
    "            if (early_step >= early_stopping_steps):\n",
    "                break\n",
    "            \n",
    "    #--------------------- PREDICTION---------------------\n",
    "    x_test = test_[feature_cols].values\n",
    "    testdataset = TestDataset(x_test)\n",
    "    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model.load_state_dict(torch.load(f\"moa-1867-SEED{seed}_FOLD{fold}.pth\"))\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n",
    "    predictions = inference_fn(model, testloader, DEVICE)\n",
    "    \n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_k_fold(NFOLDS, params, seed):\n",
    "    oof = np.zeros((len(train), len(target_cols)))\n",
    "    predictions = np.zeros((len(test), len(target_cols)))\n",
    "    \n",
    "    for fold in range(NFOLDS):\n",
    "        oof_, pred_ = run_final_training(fold, params, seed)\n",
    "        \n",
    "        predictions += pred_ / NFOLDS\n",
    "        oof += oof_\n",
    "        \n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T11:08:43.301627Z",
     "iopub.status.busy": "2020-10-24T11:08:43.300562Z",
     "iopub.status.idle": "2020-10-24T11:22:09.533681Z",
     "shell.execute_reply": "2020-10-24T11:22:09.532804Z"
    },
    "papermill": {
     "duration": 806.269774,
     "end_time": "2020-10-24T11:22:09.533850",
     "exception": false,
     "start_time": "2020-10-24T11:08:43.264076",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 0, EPOCH: 0, train_loss: 0.048930756521184705\n",
      "FOLD: 0, EPOCH: 0, valid_loss: 0.023167151814469926\n",
      "FOLD: 0, EPOCH: 1, train_loss: 0.022327734509835374\n",
      "FOLD: 0, EPOCH: 1, valid_loss: 0.021408247403227366\n",
      "FOLD: 0, EPOCH: 2, train_loss: 0.02156349493039621\n",
      "FOLD: 0, EPOCH: 2, valid_loss: 0.020825105934188917\n",
      "FOLD: 0, EPOCH: 3, train_loss: 0.02118993948238927\n",
      "FOLD: 0, EPOCH: 3, valid_loss: 0.02078582704640352\n",
      "FOLD: 0, EPOCH: 4, train_loss: 0.02084435457112016\n",
      "FOLD: 0, EPOCH: 4, valid_loss: 0.02042653927436242\n",
      "FOLD: 0, EPOCH: 5, train_loss: 0.020600171633870214\n",
      "FOLD: 0, EPOCH: 5, valid_loss: 0.020261978873839744\n",
      "FOLD: 0, EPOCH: 6, train_loss: 0.020425536619448983\n",
      "FOLD: 0, EPOCH: 6, valid_loss: 0.020016935725624744\n",
      "FOLD: 0, EPOCH: 7, train_loss: 0.02018555356944735\n",
      "FOLD: 0, EPOCH: 7, valid_loss: 0.01983450439113837\n",
      "FOLD: 0, EPOCH: 8, train_loss: 0.020027331177245925\n",
      "FOLD: 0, EPOCH: 8, valid_loss: 0.019710739501393758\n",
      "FOLD: 0, EPOCH: 9, train_loss: 0.019893932835878553\n",
      "FOLD: 0, EPOCH: 9, valid_loss: 0.019691523594351914\n",
      "FOLD: 0, EPOCH: 10, train_loss: 0.019755066800359135\n",
      "FOLD: 0, EPOCH: 10, valid_loss: 0.019525784999132156\n",
      "FOLD: 0, EPOCH: 11, train_loss: 0.019519166970575177\n",
      "FOLD: 0, EPOCH: 11, valid_loss: 0.019409006748061914\n",
      "FOLD: 0, EPOCH: 12, train_loss: 0.019468322597645426\n",
      "FOLD: 0, EPOCH: 12, valid_loss: 0.01943022241959205\n",
      "FOLD: 0, EPOCH: 13, train_loss: 0.019290365983505506\n",
      "FOLD: 0, EPOCH: 13, valid_loss: 0.019319572271062777\n",
      "FOLD: 0, EPOCH: 14, train_loss: 0.01919061245044341\n",
      "FOLD: 0, EPOCH: 14, valid_loss: 0.019168387955197923\n",
      "FOLD: 0, EPOCH: 15, train_loss: 0.01901883672218065\n",
      "FOLD: 0, EPOCH: 15, valid_loss: 0.019261295548998393\n",
      "FOLD: 0, EPOCH: 16, train_loss: 0.018975928979548248\n",
      "FOLD: 0, EPOCH: 16, valid_loss: 0.019322036407314815\n",
      "FOLD: 0, EPOCH: 17, train_loss: 0.01881395320634584\n",
      "FOLD: 0, EPOCH: 17, valid_loss: 0.019178117983616315\n",
      "Epoch    18: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 0, EPOCH: 18, train_loss: 0.018435645470949443\n",
      "FOLD: 0, EPOCH: 18, valid_loss: 0.019012343425017137\n",
      "FOLD: 0, EPOCH: 19, train_loss: 0.018305921524360374\n",
      "FOLD: 0, EPOCH: 19, valid_loss: 0.018959219209276713\n",
      "FOLD: 0, EPOCH: 20, train_loss: 0.018209505499013373\n",
      "FOLD: 0, EPOCH: 20, valid_loss: 0.01892811948290238\n",
      "FOLD: 0, EPOCH: 21, train_loss: 0.018143400801597414\n",
      "FOLD: 0, EPOCH: 21, valid_loss: 0.018931171498619594\n",
      "FOLD: 0, EPOCH: 22, train_loss: 0.01814952855174606\n",
      "FOLD: 0, EPOCH: 22, valid_loss: 0.018897938040586617\n",
      "FOLD: 0, EPOCH: 23, train_loss: 0.018033168077267504\n",
      "FOLD: 0, EPOCH: 23, valid_loss: 0.018922449734348517\n",
      "FOLD: 0, EPOCH: 24, train_loss: 0.018007276705592067\n",
      "FOLD: 0, EPOCH: 24, valid_loss: 0.018892410999307267\n",
      "FOLD: 0, EPOCH: 25, train_loss: 0.017956400642523896\n",
      "FOLD: 0, EPOCH: 25, valid_loss: 0.018880575895309448\n",
      "FOLD: 0, EPOCH: 26, train_loss: 0.017891692450723133\n",
      "FOLD: 0, EPOCH: 26, valid_loss: 0.018887794075103905\n",
      "FOLD: 0, EPOCH: 27, train_loss: 0.01789078796030702\n",
      "FOLD: 0, EPOCH: 27, valid_loss: 0.01890049473597453\n",
      "FOLD: 0, EPOCH: 28, train_loss: 0.017845452184209954\n",
      "FOLD: 0, EPOCH: 28, valid_loss: 0.018854057846161034\n",
      "FOLD: 0, EPOCH: 29, train_loss: 0.017819089613653517\n",
      "FOLD: 0, EPOCH: 29, valid_loss: 0.018863164031734832\n",
      "FOLD: 0, EPOCH: 30, train_loss: 0.017759668892501173\n",
      "FOLD: 0, EPOCH: 30, valid_loss: 0.018854702178102273\n",
      "FOLD: 0, EPOCH: 31, train_loss: 0.01773518810954851\n",
      "FOLD: 0, EPOCH: 31, valid_loss: 0.018852384474415045\n",
      "FOLD: 0, EPOCH: 32, train_loss: 0.017702684435691382\n",
      "FOLD: 0, EPOCH: 32, valid_loss: 0.018844535287756186\n",
      "FOLD: 0, EPOCH: 33, train_loss: 0.017670149713553285\n",
      "FOLD: 0, EPOCH: 33, valid_loss: 0.018838748049277525\n",
      "FOLD: 0, EPOCH: 34, train_loss: 0.01764804487292831\n",
      "FOLD: 0, EPOCH: 34, valid_loss: 0.018830303819133684\n",
      "FOLD: 0, EPOCH: 35, train_loss: 0.01761403037990267\n",
      "FOLD: 0, EPOCH: 35, valid_loss: 0.018826218465199836\n",
      "FOLD: 0, EPOCH: 36, train_loss: 0.017618843253601243\n",
      "FOLD: 0, EPOCH: 36, valid_loss: 0.01882520017142479\n",
      "FOLD: 0, EPOCH: 37, train_loss: 0.017580657039542456\n",
      "FOLD: 0, EPOCH: 37, valid_loss: 0.018848701308553036\n",
      "FOLD: 0, EPOCH: 38, train_loss: 0.01756915867932745\n",
      "FOLD: 0, EPOCH: 38, valid_loss: 0.018835376374996625\n",
      "FOLD: 0, EPOCH: 39, train_loss: 0.017492098419146763\n",
      "FOLD: 0, EPOCH: 39, valid_loss: 0.018806858274799127\n",
      "FOLD: 0, EPOCH: 40, train_loss: 0.01752447248814074\n",
      "FOLD: 0, EPOCH: 40, valid_loss: 0.01881840767768713\n",
      "FOLD: 0, EPOCH: 41, train_loss: 0.017489746540180734\n",
      "FOLD: 0, EPOCH: 41, valid_loss: 0.018822167355280656\n",
      "FOLD: 0, EPOCH: 42, train_loss: 0.01744362705613713\n",
      "FOLD: 0, EPOCH: 42, valid_loss: 0.018792612334856622\n",
      "FOLD: 0, EPOCH: 43, train_loss: 0.017458324270272576\n",
      "FOLD: 0, EPOCH: 43, valid_loss: 0.01880666627906836\n",
      "FOLD: 0, EPOCH: 44, train_loss: 0.01744110565129164\n",
      "FOLD: 0, EPOCH: 44, valid_loss: 0.0187940842543657\n",
      "FOLD: 0, EPOCH: 45, train_loss: 0.01739831014561492\n",
      "FOLD: 0, EPOCH: 45, valid_loss: 0.018798959369842824\n",
      "Epoch    46: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 0, EPOCH: 46, train_loss: 0.017345705497506504\n",
      "FOLD: 0, EPOCH: 46, valid_loss: 0.01880533592059062\n",
      "FOLD: 0, EPOCH: 47, train_loss: 0.01736434643484048\n",
      "FOLD: 0, EPOCH: 47, valid_loss: 0.018792444840073586\n",
      "FOLD: 0, EPOCH: 48, train_loss: 0.017379489727318287\n",
      "FOLD: 0, EPOCH: 48, valid_loss: 0.01880256373148698\n",
      "FOLD: 0, EPOCH: 49, train_loss: 0.017375985009444726\n",
      "FOLD: 0, EPOCH: 49, valid_loss: 0.01880573180432503\n",
      "FOLD: 1, EPOCH: 0, train_loss: 0.04768505803233868\n",
      "FOLD: 1, EPOCH: 0, valid_loss: 0.023662556392642167\n",
      "FOLD: 1, EPOCH: 1, train_loss: 0.022269713516170915\n",
      "FOLD: 1, EPOCH: 1, valid_loss: 0.02198641560971737\n",
      "FOLD: 1, EPOCH: 2, train_loss: 0.02153274421957699\n",
      "FOLD: 1, EPOCH: 2, valid_loss: 0.021427674362292655\n",
      "FOLD: 1, EPOCH: 3, train_loss: 0.02123825249539034\n",
      "FOLD: 1, EPOCH: 3, valid_loss: 0.021947128554949395\n",
      "FOLD: 1, EPOCH: 4, train_loss: 0.02092447572362584\n",
      "FOLD: 1, EPOCH: 4, valid_loss: 0.020895238679188948\n",
      "FOLD: 1, EPOCH: 5, train_loss: 0.02066788970920685\n",
      "FOLD: 1, EPOCH: 5, valid_loss: 0.020618645617595084\n",
      "FOLD: 1, EPOCH: 6, train_loss: 0.020443474118774\n",
      "FOLD: 1, EPOCH: 6, valid_loss: 0.020709519632733785\n",
      "FOLD: 1, EPOCH: 7, train_loss: 0.02028075877476383\n",
      "FOLD: 1, EPOCH: 7, valid_loss: 0.02033910943338504\n",
      "FOLD: 1, EPOCH: 8, train_loss: 0.02014099099245426\n",
      "FOLD: 1, EPOCH: 8, valid_loss: 0.020103031052992895\n",
      "FOLD: 1, EPOCH: 9, train_loss: 0.019992396235466003\n",
      "FOLD: 1, EPOCH: 9, valid_loss: 0.020040136117201585\n",
      "FOLD: 1, EPOCH: 10, train_loss: 0.019844587716097768\n",
      "FOLD: 1, EPOCH: 10, valid_loss: 0.019941342421449147\n",
      "FOLD: 1, EPOCH: 11, train_loss: 0.019700958242488874\n",
      "FOLD: 1, EPOCH: 11, valid_loss: 0.019895371336203355\n",
      "FOLD: 1, EPOCH: 12, train_loss: 0.019542511753939295\n",
      "FOLD: 1, EPOCH: 12, valid_loss: 0.019679720871723615\n",
      "FOLD: 1, EPOCH: 13, train_loss: 0.01939154725924537\n",
      "FOLD: 1, EPOCH: 13, valid_loss: 0.019598364400175903\n",
      "FOLD: 1, EPOCH: 14, train_loss: 0.019324193633085972\n",
      "FOLD: 1, EPOCH: 14, valid_loss: 0.019517366129618425\n",
      "FOLD: 1, EPOCH: 15, train_loss: 0.019193746099198186\n",
      "FOLD: 1, EPOCH: 15, valid_loss: 0.019538676652770776\n",
      "FOLD: 1, EPOCH: 16, train_loss: 0.01905586273484939\n",
      "FOLD: 1, EPOCH: 16, valid_loss: 0.019421097464286365\n",
      "FOLD: 1, EPOCH: 17, train_loss: 0.018962385359446745\n",
      "FOLD: 1, EPOCH: 17, valid_loss: 0.019422347872303083\n",
      "FOLD: 1, EPOCH: 18, train_loss: 0.0188816279877682\n",
      "FOLD: 1, EPOCH: 18, valid_loss: 0.019468247460631225\n",
      "FOLD: 1, EPOCH: 19, train_loss: 0.018708323732622573\n",
      "FOLD: 1, EPOCH: 19, valid_loss: 0.019313929459223382\n",
      "FOLD: 1, EPOCH: 20, train_loss: 0.01863476853012233\n",
      "FOLD: 1, EPOCH: 20, valid_loss: 0.01926039603467171\n",
      "FOLD: 1, EPOCH: 21, train_loss: 0.01852413873515419\n",
      "FOLD: 1, EPOCH: 21, valid_loss: 0.019351154565811157\n",
      "FOLD: 1, EPOCH: 22, train_loss: 0.01839489026649578\n",
      "FOLD: 1, EPOCH: 22, valid_loss: 0.01922019413457467\n",
      "FOLD: 1, EPOCH: 23, train_loss: 0.018300916219281184\n",
      "FOLD: 1, EPOCH: 23, valid_loss: 0.019175499964218874\n",
      "FOLD: 1, EPOCH: 24, train_loss: 0.018234446418245096\n",
      "FOLD: 1, EPOCH: 24, valid_loss: 0.01923475256906106\n",
      "FOLD: 1, EPOCH: 25, train_loss: 0.018074530302672774\n",
      "FOLD: 1, EPOCH: 25, valid_loss: 0.01916062316069236\n",
      "FOLD: 1, EPOCH: 26, train_loss: 0.017945054465451755\n",
      "FOLD: 1, EPOCH: 26, valid_loss: 0.019071506049770575\n",
      "FOLD: 1, EPOCH: 27, train_loss: 0.017812739403263944\n",
      "FOLD: 1, EPOCH: 27, valid_loss: 0.019105219210569676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 1, EPOCH: 28, train_loss: 0.017719675912647635\n",
      "FOLD: 1, EPOCH: 28, valid_loss: 0.01908744513415373\n",
      "FOLD: 1, EPOCH: 29, train_loss: 0.01763689940845644\n",
      "FOLD: 1, EPOCH: 29, valid_loss: 0.019049719692422792\n",
      "FOLD: 1, EPOCH: 30, train_loss: 0.017550527369855223\n",
      "FOLD: 1, EPOCH: 30, valid_loss: 0.019097520038485527\n",
      "FOLD: 1, EPOCH: 31, train_loss: 0.017463210617771018\n",
      "FOLD: 1, EPOCH: 31, valid_loss: 0.01902528737600033\n",
      "FOLD: 1, EPOCH: 32, train_loss: 0.01729562787993534\n",
      "FOLD: 1, EPOCH: 32, valid_loss: 0.01904777265512026\n",
      "FOLD: 1, EPOCH: 33, train_loss: 0.017189328105667152\n",
      "FOLD: 1, EPOCH: 33, valid_loss: 0.019006030490765206\n",
      "FOLD: 1, EPOCH: 34, train_loss: 0.017035959804480947\n",
      "FOLD: 1, EPOCH: 34, valid_loss: 0.019108981180649538\n",
      "FOLD: 1, EPOCH: 35, train_loss: 0.016963534472459876\n",
      "FOLD: 1, EPOCH: 35, valid_loss: 0.019044594266093694\n",
      "FOLD: 1, EPOCH: 36, train_loss: 0.01682031121910424\n",
      "FOLD: 1, EPOCH: 36, valid_loss: 0.019032288485994704\n",
      "Epoch    37: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 1, EPOCH: 37, train_loss: 0.01650910633238586\n",
      "FOLD: 1, EPOCH: 37, valid_loss: 0.018970913755205963\n",
      "FOLD: 1, EPOCH: 38, train_loss: 0.01644435336158888\n",
      "FOLD: 1, EPOCH: 38, valid_loss: 0.018964891250316914\n",
      "FOLD: 1, EPOCH: 39, train_loss: 0.016398789486973674\n",
      "FOLD: 1, EPOCH: 39, valid_loss: 0.01894794820019832\n",
      "FOLD: 1, EPOCH: 40, train_loss: 0.016255065433781694\n",
      "FOLD: 1, EPOCH: 40, valid_loss: 0.018930787650438454\n",
      "FOLD: 1, EPOCH: 41, train_loss: 0.016219742105317278\n",
      "FOLD: 1, EPOCH: 41, valid_loss: 0.018925995637591068\n",
      "FOLD: 1, EPOCH: 42, train_loss: 0.016186739947344805\n",
      "FOLD: 1, EPOCH: 42, valid_loss: 0.018925814244609613\n",
      "FOLD: 1, EPOCH: 43, train_loss: 0.016168959652753296\n",
      "FOLD: 1, EPOCH: 43, valid_loss: 0.01893355124271833\n",
      "FOLD: 1, EPOCH: 44, train_loss: 0.016106689411744073\n",
      "FOLD: 1, EPOCH: 44, valid_loss: 0.018930645659565926\n",
      "FOLD: 1, EPOCH: 45, train_loss: 0.01609262572349729\n",
      "FOLD: 1, EPOCH: 45, valid_loss: 0.018934795776238807\n",
      "Epoch    46: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 1, EPOCH: 46, train_loss: 0.01604304728224068\n",
      "FOLD: 1, EPOCH: 46, valid_loss: 0.018937669407862883\n",
      "FOLD: 1, EPOCH: 47, train_loss: 0.01603741221431945\n",
      "FOLD: 1, EPOCH: 47, valid_loss: 0.018914499821571205\n",
      "FOLD: 1, EPOCH: 48, train_loss: 0.01605242575090882\n",
      "FOLD: 1, EPOCH: 48, valid_loss: 0.018950714658086117\n",
      "FOLD: 1, EPOCH: 49, train_loss: 0.01600380738995768\n",
      "FOLD: 1, EPOCH: 49, valid_loss: 0.018932466610119894\n",
      "FOLD: 2, EPOCH: 0, train_loss: 0.04841459877285603\n",
      "FOLD: 2, EPOCH: 0, valid_loss: 0.02314010663674428\n",
      "FOLD: 2, EPOCH: 1, train_loss: 0.022267775257696975\n",
      "FOLD: 2, EPOCH: 1, valid_loss: 0.021983678810871564\n",
      "FOLD: 2, EPOCH: 2, train_loss: 0.021617062062629172\n",
      "FOLD: 2, EPOCH: 2, valid_loss: 0.021760327741503716\n",
      "FOLD: 2, EPOCH: 3, train_loss: 0.021249945854415763\n",
      "FOLD: 2, EPOCH: 3, valid_loss: 0.02234806106067621\n",
      "FOLD: 2, EPOCH: 4, train_loss: 0.020976143058490108\n",
      "FOLD: 2, EPOCH: 4, valid_loss: 0.02087253131545507\n",
      "FOLD: 2, EPOCH: 5, train_loss: 0.020685794939463202\n",
      "FOLD: 2, EPOCH: 5, valid_loss: 0.02212266022196183\n",
      "FOLD: 2, EPOCH: 6, train_loss: 0.020446383595667982\n",
      "FOLD: 2, EPOCH: 6, valid_loss: 0.020738747257452745\n",
      "FOLD: 2, EPOCH: 7, train_loss: 0.02028008651089024\n",
      "FOLD: 2, EPOCH: 7, valid_loss: 0.020428357932430048\n",
      "FOLD: 2, EPOCH: 8, train_loss: 0.02006115668730156\n",
      "FOLD: 2, EPOCH: 8, valid_loss: 0.02023982399931321\n",
      "FOLD: 2, EPOCH: 9, train_loss: 0.019929479404881194\n",
      "FOLD: 2, EPOCH: 9, valid_loss: 0.02028188181038086\n",
      "FOLD: 2, EPOCH: 10, train_loss: 0.019765568821615464\n",
      "FOLD: 2, EPOCH: 10, valid_loss: 0.020039433756699927\n",
      "FOLD: 2, EPOCH: 11, train_loss: 0.019620777434996656\n",
      "FOLD: 2, EPOCH: 11, valid_loss: 0.020043198162546523\n",
      "FOLD: 2, EPOCH: 12, train_loss: 0.019460830258557927\n",
      "FOLD: 2, EPOCH: 12, valid_loss: 0.01985242532996031\n",
      "FOLD: 2, EPOCH: 13, train_loss: 0.019370180353320932\n",
      "FOLD: 2, EPOCH: 13, valid_loss: 0.019756325878776036\n",
      "FOLD: 2, EPOCH: 14, train_loss: 0.019205562669683148\n",
      "FOLD: 2, EPOCH: 14, valid_loss: 0.01977228344633029\n",
      "FOLD: 2, EPOCH: 15, train_loss: 0.01906565667406933\n",
      "FOLD: 2, EPOCH: 15, valid_loss: 0.01968561686002291\n",
      "FOLD: 2, EPOCH: 16, train_loss: 0.01895411531566768\n",
      "FOLD: 2, EPOCH: 16, valid_loss: 0.01966406820485225\n",
      "FOLD: 2, EPOCH: 17, train_loss: 0.018847176075183058\n",
      "FOLD: 2, EPOCH: 17, valid_loss: 0.01954079820559575\n",
      "FOLD: 2, EPOCH: 18, train_loss: 0.018761286209966685\n",
      "FOLD: 2, EPOCH: 18, valid_loss: 0.01951719419314311\n",
      "FOLD: 2, EPOCH: 19, train_loss: 0.018633754269496816\n",
      "FOLD: 2, EPOCH: 19, valid_loss: 0.019638911176186342\n",
      "FOLD: 2, EPOCH: 20, train_loss: 0.018510641479814374\n",
      "FOLD: 2, EPOCH: 20, valid_loss: 0.01948733111986747\n",
      "FOLD: 2, EPOCH: 21, train_loss: 0.018346291966736317\n",
      "FOLD: 2, EPOCH: 21, valid_loss: 0.019463792586555846\n",
      "FOLD: 2, EPOCH: 22, train_loss: 0.018249704401839425\n",
      "FOLD: 2, EPOCH: 22, valid_loss: 0.019408134313730094\n",
      "FOLD: 2, EPOCH: 23, train_loss: 0.018159337568323355\n",
      "FOLD: 2, EPOCH: 23, valid_loss: 0.019417200094232194\n",
      "FOLD: 2, EPOCH: 24, train_loss: 0.018024648514551087\n",
      "FOLD: 2, EPOCH: 24, valid_loss: 0.01930743885728029\n",
      "FOLD: 2, EPOCH: 25, train_loss: 0.01785146485309343\n",
      "FOLD: 2, EPOCH: 25, valid_loss: 0.01927325759942715\n",
      "FOLD: 2, EPOCH: 26, train_loss: 0.017749180337665853\n",
      "FOLD: 2, EPOCH: 26, valid_loss: 0.019321411561507445\n",
      "FOLD: 2, EPOCH: 27, train_loss: 0.01767562349905839\n",
      "FOLD: 2, EPOCH: 27, valid_loss: 0.01935602094118412\n",
      "FOLD: 2, EPOCH: 28, train_loss: 0.017554992444913934\n",
      "FOLD: 2, EPOCH: 28, valid_loss: 0.01930395083931776\n",
      "Epoch    29: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 2, EPOCH: 29, train_loss: 0.017159579711890704\n",
      "FOLD: 2, EPOCH: 29, valid_loss: 0.0192160692352515\n",
      "FOLD: 2, EPOCH: 30, train_loss: 0.01709273101359203\n",
      "FOLD: 2, EPOCH: 30, valid_loss: 0.019184532933510266\n",
      "FOLD: 2, EPOCH: 31, train_loss: 0.01698542812939834\n",
      "FOLD: 2, EPOCH: 31, valid_loss: 0.019166148052765772\n",
      "FOLD: 2, EPOCH: 32, train_loss: 0.016918958416460333\n",
      "FOLD: 2, EPOCH: 32, valid_loss: 0.01917227343297922\n",
      "FOLD: 2, EPOCH: 33, train_loss: 0.01686247856028982\n",
      "FOLD: 2, EPOCH: 33, valid_loss: 0.019152741592663985\n",
      "FOLD: 2, EPOCH: 34, train_loss: 0.01681837240687093\n",
      "FOLD: 2, EPOCH: 34, valid_loss: 0.01913469356413071\n",
      "FOLD: 2, EPOCH: 35, train_loss: 0.016796871151372388\n",
      "FOLD: 2, EPOCH: 35, valid_loss: 0.01915815329322448\n",
      "FOLD: 2, EPOCH: 36, train_loss: 0.016694867920533225\n",
      "FOLD: 2, EPOCH: 36, valid_loss: 0.01915697452540581\n",
      "FOLD: 2, EPOCH: 37, train_loss: 0.016732061546094513\n",
      "FOLD: 2, EPOCH: 37, valid_loss: 0.01916574901686265\n",
      "Epoch    38: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 2, EPOCH: 38, train_loss: 0.016647929388627008\n",
      "FOLD: 2, EPOCH: 38, valid_loss: 0.019180888310074806\n",
      "FOLD: 2, EPOCH: 39, train_loss: 0.016609961884347973\n",
      "FOLD: 2, EPOCH: 39, valid_loss: 0.019157125829504087\n",
      "FOLD: 2, EPOCH: 40, train_loss: 0.016592775647704665\n",
      "FOLD: 2, EPOCH: 40, valid_loss: 0.019153983833698127\n",
      "Epoch    41: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 2, EPOCH: 41, train_loss: 0.016594836790416692\n",
      "FOLD: 2, EPOCH: 41, valid_loss: 0.0191598475839083\n",
      "FOLD: 2, EPOCH: 42, train_loss: 0.016634838529736608\n",
      "FOLD: 2, EPOCH: 42, valid_loss: 0.019142837478564337\n",
      "FOLD: 2, EPOCH: 43, train_loss: 0.016621359170892754\n",
      "FOLD: 2, EPOCH: 43, valid_loss: 0.019147638518076677\n",
      "Epoch    44: reducing learning rate of group 0 to 8.9345e-05.\n",
      "FOLD: 2, EPOCH: 44, train_loss: 0.016605928829694923\n",
      "FOLD: 2, EPOCH: 44, valid_loss: 0.01916850458544034\n",
      "FOLD: 2, EPOCH: 45, train_loss: 0.016600953175912838\n",
      "FOLD: 2, EPOCH: 45, valid_loss: 0.01916972132256398\n",
      "FOLD: 2, EPOCH: 46, train_loss: 0.016591635708873335\n",
      "FOLD: 2, EPOCH: 46, valid_loss: 0.019158429107986964\n",
      "Epoch    47: reducing learning rate of group 0 to 1.3371e-05.\n",
      "FOLD: 2, EPOCH: 47, train_loss: 0.01657093084744505\n",
      "FOLD: 2, EPOCH: 47, valid_loss: 0.01915554616313714\n",
      "FOLD: 2, EPOCH: 48, train_loss: 0.016610178190308647\n",
      "FOLD: 2, EPOCH: 48, valid_loss: 0.019158718820947867\n",
      "FOLD: 2, EPOCH: 49, train_loss: 0.016564838198089116\n",
      "FOLD: 2, EPOCH: 49, valid_loss: 0.019140994749390162\n",
      "Epoch    50: reducing learning rate of group 0 to 2.0012e-06.\n",
      "FOLD: 3, EPOCH: 0, train_loss: 0.047690809754705105\n",
      "FOLD: 3, EPOCH: 0, valid_loss: 0.02267663496044966\n",
      "FOLD: 3, EPOCH: 1, train_loss: 0.022401742863695364\n",
      "FOLD: 3, EPOCH: 1, valid_loss: 0.02149797001710305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 3, EPOCH: 2, train_loss: 0.021591766032616835\n",
      "FOLD: 3, EPOCH: 2, valid_loss: 0.021145380890140168\n",
      "FOLD: 3, EPOCH: 3, train_loss: 0.021215954946505057\n",
      "FOLD: 3, EPOCH: 3, valid_loss: 0.020855539693282202\n",
      "FOLD: 3, EPOCH: 4, train_loss: 0.02095985858122239\n",
      "FOLD: 3, EPOCH: 4, valid_loss: 0.020667840368472613\n",
      "FOLD: 3, EPOCH: 5, train_loss: 0.0206642455446559\n",
      "FOLD: 3, EPOCH: 5, valid_loss: 0.020451546718294803\n",
      "FOLD: 3, EPOCH: 6, train_loss: 0.020459633496766154\n",
      "FOLD: 3, EPOCH: 6, valid_loss: 0.0202598819652429\n",
      "FOLD: 3, EPOCH: 7, train_loss: 0.020220913143979537\n",
      "FOLD: 3, EPOCH: 7, valid_loss: 0.019956715834828522\n",
      "FOLD: 3, EPOCH: 8, train_loss: 0.020114541305480776\n",
      "FOLD: 3, EPOCH: 8, valid_loss: 0.01994357444345951\n",
      "FOLD: 3, EPOCH: 9, train_loss: 0.019977252930402756\n",
      "FOLD: 3, EPOCH: 9, valid_loss: 0.019974462831249602\n",
      "FOLD: 3, EPOCH: 10, train_loss: 0.019797446432749968\n",
      "FOLD: 3, EPOCH: 10, valid_loss: 0.019833165149276074\n",
      "FOLD: 3, EPOCH: 11, train_loss: 0.019705451873911393\n",
      "FOLD: 3, EPOCH: 11, valid_loss: 0.019803754698771697\n",
      "FOLD: 3, EPOCH: 12, train_loss: 0.01954993328734024\n",
      "FOLD: 3, EPOCH: 12, valid_loss: 0.01969282114161895\n",
      "FOLD: 3, EPOCH: 13, train_loss: 0.01941575114992825\n",
      "FOLD: 3, EPOCH: 13, valid_loss: 0.019533301918552473\n",
      "FOLD: 3, EPOCH: 14, train_loss: 0.0192559737259069\n",
      "FOLD: 3, EPOCH: 14, valid_loss: 0.01954889684342421\n",
      "FOLD: 3, EPOCH: 15, train_loss: 0.019168909865658026\n",
      "FOLD: 3, EPOCH: 15, valid_loss: 0.019486034862124003\n",
      "FOLD: 3, EPOCH: 16, train_loss: 0.018978971585228637\n",
      "FOLD: 3, EPOCH: 16, valid_loss: 0.01949449671575656\n",
      "FOLD: 3, EPOCH: 17, train_loss: 0.01893184016886595\n",
      "FOLD: 3, EPOCH: 17, valid_loss: 0.019287719749487363\n",
      "FOLD: 3, EPOCH: 18, train_loss: 0.01879930095998822\n",
      "FOLD: 3, EPOCH: 18, valid_loss: 0.0193168674237453\n",
      "FOLD: 3, EPOCH: 19, train_loss: 0.018683986285248318\n",
      "FOLD: 3, EPOCH: 19, valid_loss: 0.019304204588899247\n",
      "FOLD: 3, EPOCH: 20, train_loss: 0.018569222626251145\n",
      "FOLD: 3, EPOCH: 20, valid_loss: 0.019264239674577348\n",
      "FOLD: 3, EPOCH: 21, train_loss: 0.018442519814581483\n",
      "FOLD: 3, EPOCH: 21, valid_loss: 0.019195901373258002\n",
      "FOLD: 3, EPOCH: 22, train_loss: 0.01829735592410371\n",
      "FOLD: 3, EPOCH: 22, valid_loss: 0.019241690922241945\n",
      "FOLD: 3, EPOCH: 23, train_loss: 0.018268656413498764\n",
      "FOLD: 3, EPOCH: 23, valid_loss: 0.019159634669239704\n",
      "FOLD: 3, EPOCH: 24, train_loss: 0.018120542722376617\n",
      "FOLD: 3, EPOCH: 24, valid_loss: 0.01921348015849407\n",
      "FOLD: 3, EPOCH: 25, train_loss: 0.017969584193181346\n",
      "FOLD: 3, EPOCH: 25, valid_loss: 0.019134720930686362\n",
      "FOLD: 3, EPOCH: 26, train_loss: 0.017886522389646317\n",
      "FOLD: 3, EPOCH: 26, valid_loss: 0.01912729599728034\n",
      "FOLD: 3, EPOCH: 27, train_loss: 0.017763546164575462\n",
      "FOLD: 3, EPOCH: 27, valid_loss: 0.01913391899030942\n",
      "FOLD: 3, EPOCH: 28, train_loss: 0.017630137503147125\n",
      "FOLD: 3, EPOCH: 28, valid_loss: 0.019144551255382024\n",
      "FOLD: 3, EPOCH: 29, train_loss: 0.01750276205004067\n",
      "FOLD: 3, EPOCH: 29, valid_loss: 0.0191439027683093\n",
      "Epoch    30: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 3, EPOCH: 30, train_loss: 0.017191590439226176\n",
      "FOLD: 3, EPOCH: 30, valid_loss: 0.01905917003750801\n",
      "FOLD: 3, EPOCH: 31, train_loss: 0.017066823494796816\n",
      "FOLD: 3, EPOCH: 31, valid_loss: 0.019000968394371178\n",
      "FOLD: 3, EPOCH: 32, train_loss: 0.017012856711910385\n",
      "FOLD: 3, EPOCH: 32, valid_loss: 0.018975967827897806\n",
      "FOLD: 3, EPOCH: 33, train_loss: 0.01691228895120927\n",
      "FOLD: 3, EPOCH: 33, valid_loss: 0.01899055319909866\n",
      "FOLD: 3, EPOCH: 34, train_loss: 0.016817035002482904\n",
      "FOLD: 3, EPOCH: 34, valid_loss: 0.018956419080495834\n",
      "FOLD: 3, EPOCH: 35, train_loss: 0.01681519560264172\n",
      "FOLD: 3, EPOCH: 35, valid_loss: 0.01899194473830553\n",
      "FOLD: 3, EPOCH: 36, train_loss: 0.016760035380217676\n",
      "FOLD: 3, EPOCH: 36, valid_loss: 0.018954747428114597\n",
      "FOLD: 3, EPOCH: 37, train_loss: 0.016734321161198454\n",
      "FOLD: 3, EPOCH: 37, valid_loss: 0.018975060146588545\n",
      "FOLD: 3, EPOCH: 38, train_loss: 0.016690452532792412\n",
      "FOLD: 3, EPOCH: 38, valid_loss: 0.01898701388675433\n",
      "FOLD: 3, EPOCH: 39, train_loss: 0.016652898485394748\n",
      "FOLD: 3, EPOCH: 39, valid_loss: 0.01896008290350437\n",
      "Epoch    40: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 3, EPOCH: 40, train_loss: 0.016560774711841666\n",
      "FOLD: 3, EPOCH: 40, valid_loss: 0.018946533736128073\n",
      "FOLD: 3, EPOCH: 41, train_loss: 0.016610120045574935\n",
      "FOLD: 3, EPOCH: 41, valid_loss: 0.018955144458092175\n",
      "FOLD: 3, EPOCH: 42, train_loss: 0.016591392872804724\n",
      "FOLD: 3, EPOCH: 42, valid_loss: 0.0189624189470823\n",
      "FOLD: 3, EPOCH: 43, train_loss: 0.016614518807949247\n",
      "FOLD: 3, EPOCH: 43, valid_loss: 0.01893949995820339\n",
      "FOLD: 3, EPOCH: 44, train_loss: 0.016597549015706457\n",
      "FOLD: 3, EPOCH: 44, valid_loss: 0.018972716365869228\n",
      "FOLD: 3, EPOCH: 45, train_loss: 0.016562336502042977\n",
      "FOLD: 3, EPOCH: 45, valid_loss: 0.018958808854222298\n",
      "FOLD: 3, EPOCH: 46, train_loss: 0.016575683638252115\n",
      "FOLD: 3, EPOCH: 46, valid_loss: 0.018953355602346934\n",
      "Epoch    47: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 3, EPOCH: 47, train_loss: 0.016562668455613626\n",
      "FOLD: 3, EPOCH: 47, valid_loss: 0.018955374136567116\n",
      "FOLD: 3, EPOCH: 48, train_loss: 0.01655934131830125\n",
      "FOLD: 3, EPOCH: 48, valid_loss: 0.018964488345843095\n",
      "FOLD: 3, EPOCH: 49, train_loss: 0.016516918530435982\n",
      "FOLD: 3, EPOCH: 49, valid_loss: 0.018953190973171823\n",
      "Epoch    50: reducing learning rate of group 0 to 8.9345e-05.\n",
      "FOLD: 4, EPOCH: 0, train_loss: 0.04759222976359967\n",
      "FOLD: 4, EPOCH: 0, valid_loss: 0.023079465071742352\n",
      "FOLD: 4, EPOCH: 1, train_loss: 0.022317977018050245\n",
      "FOLD: 4, EPOCH: 1, valid_loss: 0.022089077446323175\n",
      "FOLD: 4, EPOCH: 2, train_loss: 0.02157664769706694\n",
      "FOLD: 4, EPOCH: 2, valid_loss: 0.022077282174275473\n",
      "FOLD: 4, EPOCH: 3, train_loss: 0.02122066852108047\n",
      "FOLD: 4, EPOCH: 3, valid_loss: 0.0214106931995887\n",
      "FOLD: 4, EPOCH: 4, train_loss: 0.020880451702789682\n",
      "FOLD: 4, EPOCH: 4, valid_loss: 0.020969574697888814\n",
      "FOLD: 4, EPOCH: 5, train_loss: 0.020617874616102594\n",
      "FOLD: 4, EPOCH: 5, valid_loss: 0.02089400216937065\n",
      "FOLD: 4, EPOCH: 6, train_loss: 0.02037577479574326\n",
      "FOLD: 4, EPOCH: 6, valid_loss: 0.020700831252795\n",
      "FOLD: 4, EPOCH: 7, train_loss: 0.020248533654454594\n",
      "FOLD: 4, EPOCH: 7, valid_loss: 0.020521973761228416\n",
      "FOLD: 4, EPOCH: 8, train_loss: 0.02003732504876884\n",
      "FOLD: 4, EPOCH: 8, valid_loss: 0.020386477932333946\n",
      "FOLD: 4, EPOCH: 9, train_loss: 0.019904667511582375\n",
      "FOLD: 4, EPOCH: 9, valid_loss: 0.021668612097318355\n",
      "FOLD: 4, EPOCH: 10, train_loss: 0.01980173318470652\n",
      "FOLD: 4, EPOCH: 10, valid_loss: 0.020380113560419817\n",
      "FOLD: 4, EPOCH: 11, train_loss: 0.019639556875100005\n",
      "FOLD: 4, EPOCH: 11, valid_loss: 0.02087771505690538\n",
      "FOLD: 4, EPOCH: 12, train_loss: 0.01948894381623816\n",
      "FOLD: 4, EPOCH: 12, valid_loss: 0.019890286314945955\n",
      "FOLD: 4, EPOCH: 13, train_loss: 0.019291280290564976\n",
      "FOLD: 4, EPOCH: 13, valid_loss: 0.019911842277416818\n",
      "FOLD: 4, EPOCH: 14, train_loss: 0.019169106752284476\n",
      "FOLD: 4, EPOCH: 14, valid_loss: 0.019784041178914216\n",
      "FOLD: 4, EPOCH: 15, train_loss: 0.019031327041621145\n",
      "FOLD: 4, EPOCH: 15, valid_loss: 0.019806288899137423\n",
      "FOLD: 4, EPOCH: 16, train_loss: 0.018900501617305988\n",
      "FOLD: 4, EPOCH: 16, valid_loss: 0.019682463974906847\n",
      "FOLD: 4, EPOCH: 17, train_loss: 0.018844056899684505\n",
      "FOLD: 4, EPOCH: 17, valid_loss: 0.019669183028432038\n",
      "FOLD: 4, EPOCH: 18, train_loss: 0.01872513848482757\n",
      "FOLD: 4, EPOCH: 18, valid_loss: 0.019581392264136903\n",
      "FOLD: 4, EPOCH: 19, train_loss: 0.018607149882292427\n",
      "FOLD: 4, EPOCH: 19, valid_loss: 0.01957327428345497\n",
      "FOLD: 4, EPOCH: 20, train_loss: 0.018477413954364287\n",
      "FOLD: 4, EPOCH: 20, valid_loss: 0.019594486946096785\n",
      "FOLD: 4, EPOCH: 21, train_loss: 0.018416338449193013\n",
      "FOLD: 4, EPOCH: 21, valid_loss: 0.01957949695105736\n",
      "FOLD: 4, EPOCH: 22, train_loss: 0.018220180343534495\n",
      "FOLD: 4, EPOCH: 22, valid_loss: 0.019546553349265687\n",
      "FOLD: 4, EPOCH: 23, train_loss: 0.018104647210723645\n",
      "FOLD: 4, EPOCH: 23, valid_loss: 0.019418372127872247\n",
      "FOLD: 4, EPOCH: 24, train_loss: 0.017985406759622936\n",
      "FOLD: 4, EPOCH: 24, valid_loss: 0.01944746349293452\n",
      "FOLD: 4, EPOCH: 25, train_loss: 0.017837206707210153\n",
      "FOLD: 4, EPOCH: 25, valid_loss: 0.019408749416470528\n",
      "FOLD: 4, EPOCH: 26, train_loss: 0.017780477281760524\n",
      "FOLD: 4, EPOCH: 26, valid_loss: 0.019352000349989303\n",
      "FOLD: 4, EPOCH: 27, train_loss: 0.017660209229467688\n",
      "FOLD: 4, EPOCH: 27, valid_loss: 0.01939827060470214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 4, EPOCH: 28, train_loss: 0.017533178491568244\n",
      "FOLD: 4, EPOCH: 28, valid_loss: 0.019417384639382362\n",
      "FOLD: 4, EPOCH: 29, train_loss: 0.01738074849787596\n",
      "FOLD: 4, EPOCH: 29, valid_loss: 0.019424347636791375\n",
      "Epoch    30: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 4, EPOCH: 30, train_loss: 0.017109523917472846\n",
      "FOLD: 4, EPOCH: 30, valid_loss: 0.01933650658107721\n",
      "FOLD: 4, EPOCH: 31, train_loss: 0.016929136110922775\n",
      "FOLD: 4, EPOCH: 31, valid_loss: 0.01929871651988763\n",
      "FOLD: 4, EPOCH: 32, train_loss: 0.016884078611493915\n",
      "FOLD: 4, EPOCH: 32, valid_loss: 0.019291939930273935\n",
      "FOLD: 4, EPOCH: 33, train_loss: 0.01679362214447276\n",
      "FOLD: 4, EPOCH: 33, valid_loss: 0.01930452582354729\n",
      "FOLD: 4, EPOCH: 34, train_loss: 0.01671813294996281\n",
      "FOLD: 4, EPOCH: 34, valid_loss: 0.019270314190250177\n",
      "FOLD: 4, EPOCH: 35, train_loss: 0.016703005250845407\n",
      "FOLD: 4, EPOCH: 35, valid_loss: 0.01928087481512473\n",
      "FOLD: 4, EPOCH: 36, train_loss: 0.016643797248803282\n",
      "FOLD: 4, EPOCH: 36, valid_loss: 0.019270912385903873\n",
      "FOLD: 4, EPOCH: 37, train_loss: 0.016584413000256627\n",
      "FOLD: 4, EPOCH: 37, valid_loss: 0.01925907155069021\n",
      "FOLD: 4, EPOCH: 38, train_loss: 0.01655478309839964\n",
      "FOLD: 4, EPOCH: 38, valid_loss: 0.019274366876253717\n",
      "FOLD: 4, EPOCH: 39, train_loss: 0.016532751706403656\n",
      "FOLD: 4, EPOCH: 39, valid_loss: 0.019283616198943213\n",
      "FOLD: 4, EPOCH: 40, train_loss: 0.016497128341044928\n",
      "FOLD: 4, EPOCH: 40, valid_loss: 0.019265407266525123\n",
      "Epoch    41: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 4, EPOCH: 41, train_loss: 0.016440709763382737\n",
      "FOLD: 4, EPOCH: 41, valid_loss: 0.019280936712255843\n",
      "FOLD: 4, EPOCH: 42, train_loss: 0.01640274454965382\n",
      "FOLD: 4, EPOCH: 42, valid_loss: 0.019266190580450572\n",
      "FOLD: 4, EPOCH: 43, train_loss: 0.016425326038655395\n",
      "FOLD: 4, EPOCH: 43, valid_loss: 0.019278950416124783\n",
      "Epoch    44: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 4, EPOCH: 44, train_loss: 0.016381261794752366\n",
      "FOLD: 4, EPOCH: 44, valid_loss: 0.019282902232729472\n",
      "FOLD: 4, EPOCH: 45, train_loss: 0.016398531095300976\n",
      "FOLD: 4, EPOCH: 45, valid_loss: 0.019258209719107702\n",
      "FOLD: 4, EPOCH: 46, train_loss: 0.016401259807517398\n",
      "FOLD: 4, EPOCH: 46, valid_loss: 0.019267517786759596\n",
      "FOLD: 4, EPOCH: 47, train_loss: 0.01639492087368224\n",
      "FOLD: 4, EPOCH: 47, valid_loss: 0.019270731566044\n",
      "FOLD: 4, EPOCH: 48, train_loss: 0.01642908386231677\n",
      "FOLD: 4, EPOCH: 48, valid_loss: 0.01927827886090829\n",
      "Epoch    49: reducing learning rate of group 0 to 8.9345e-05.\n",
      "FOLD: 4, EPOCH: 49, train_loss: 0.016377228790441074\n",
      "FOLD: 4, EPOCH: 49, valid_loss: 0.019268924370408058\n",
      "FOLD: 5, EPOCH: 0, train_loss: 0.04673911359261822\n",
      "FOLD: 5, EPOCH: 0, valid_loss: 0.023261512271486796\n",
      "FOLD: 5, EPOCH: 1, train_loss: 0.022318800433060608\n",
      "FOLD: 5, EPOCH: 1, valid_loss: 0.021838434900228795\n",
      "FOLD: 5, EPOCH: 2, train_loss: 0.02161245747796587\n",
      "FOLD: 5, EPOCH: 2, valid_loss: 0.02131276520398947\n",
      "FOLD: 5, EPOCH: 3, train_loss: 0.021234871265855996\n",
      "FOLD: 5, EPOCH: 3, valid_loss: 0.020995460737210054\n",
      "FOLD: 5, EPOCH: 4, train_loss: 0.02093766546631987\n",
      "FOLD: 5, EPOCH: 4, valid_loss: 0.02067902784508008\n",
      "FOLD: 5, EPOCH: 5, train_loss: 0.020680793561041355\n",
      "FOLD: 5, EPOCH: 5, valid_loss: 0.020416251741922818\n",
      "FOLD: 5, EPOCH: 6, train_loss: 0.020474307767644122\n",
      "FOLD: 5, EPOCH: 6, valid_loss: 0.02027329573264489\n",
      "FOLD: 5, EPOCH: 7, train_loss: 0.02029202740035347\n",
      "FOLD: 5, EPOCH: 7, valid_loss: 0.020065208037312213\n",
      "FOLD: 5, EPOCH: 8, train_loss: 0.020099376680681836\n",
      "FOLD: 5, EPOCH: 8, valid_loss: 0.01991664059460163\n",
      "FOLD: 5, EPOCH: 9, train_loss: 0.01996563782764448\n",
      "FOLD: 5, EPOCH: 9, valid_loss: 0.02003531802732211\n",
      "FOLD: 5, EPOCH: 10, train_loss: 0.019795260694179986\n",
      "FOLD: 5, EPOCH: 10, valid_loss: 0.019744694519501466\n",
      "FOLD: 5, EPOCH: 11, train_loss: 0.019667896819678513\n",
      "FOLD: 5, EPOCH: 11, valid_loss: 0.01985193960941755\n",
      "FOLD: 5, EPOCH: 12, train_loss: 0.01949141005909926\n",
      "FOLD: 5, EPOCH: 12, valid_loss: 0.019625891000032425\n",
      "FOLD: 5, EPOCH: 13, train_loss: 0.01943224701224952\n",
      "FOLD: 5, EPOCH: 13, valid_loss: 0.019522938017661754\n",
      "FOLD: 5, EPOCH: 14, train_loss: 0.019245106928251886\n",
      "FOLD: 5, EPOCH: 14, valid_loss: 0.019420967222406313\n",
      "FOLD: 5, EPOCH: 15, train_loss: 0.019161673489253263\n",
      "FOLD: 5, EPOCH: 15, valid_loss: 0.01947667320760397\n",
      "FOLD: 5, EPOCH: 16, train_loss: 0.019052663742489106\n",
      "FOLD: 5, EPOCH: 16, valid_loss: 0.019430364840305768\n",
      "FOLD: 5, EPOCH: 17, train_loss: 0.018939172823888226\n",
      "FOLD: 5, EPOCH: 17, valid_loss: 0.01932606975046488\n",
      "FOLD: 5, EPOCH: 18, train_loss: 0.018824962143962447\n",
      "FOLD: 5, EPOCH: 18, valid_loss: 0.019311022013425827\n",
      "FOLD: 5, EPOCH: 19, train_loss: 0.018720890999444434\n",
      "FOLD: 5, EPOCH: 19, valid_loss: 0.019205482676625252\n",
      "FOLD: 5, EPOCH: 20, train_loss: 0.018599022934968408\n",
      "FOLD: 5, EPOCH: 20, valid_loss: 0.019231355104308862\n",
      "FOLD: 5, EPOCH: 21, train_loss: 0.01847962035822707\n",
      "FOLD: 5, EPOCH: 21, valid_loss: 0.019150382910783473\n",
      "FOLD: 5, EPOCH: 22, train_loss: 0.018376509432454367\n",
      "FOLD: 5, EPOCH: 22, valid_loss: 0.019132027259239785\n",
      "FOLD: 5, EPOCH: 23, train_loss: 0.018282796231073303\n",
      "FOLD: 5, EPOCH: 23, valid_loss: 0.019141890824987337\n",
      "FOLD: 5, EPOCH: 24, train_loss: 0.018177903534190076\n",
      "FOLD: 5, EPOCH: 24, valid_loss: 0.01923504643715345\n",
      "FOLD: 5, EPOCH: 25, train_loss: 0.018035361996373615\n",
      "FOLD: 5, EPOCH: 25, valid_loss: 0.0191034499842387\n",
      "FOLD: 5, EPOCH: 26, train_loss: 0.017943095617197657\n",
      "FOLD: 5, EPOCH: 26, valid_loss: 0.019113931948175795\n",
      "FOLD: 5, EPOCH: 27, train_loss: 0.01782607946645569\n",
      "FOLD: 5, EPOCH: 27, valid_loss: 0.0190668236464262\n",
      "FOLD: 5, EPOCH: 28, train_loss: 0.017723172298959783\n",
      "FOLD: 5, EPOCH: 28, valid_loss: 0.019046450177064307\n",
      "FOLD: 5, EPOCH: 29, train_loss: 0.017608892638236284\n",
      "FOLD: 5, EPOCH: 29, valid_loss: 0.019022809341549873\n",
      "FOLD: 5, EPOCH: 30, train_loss: 0.0174669675982079\n",
      "FOLD: 5, EPOCH: 30, valid_loss: 0.01909992815210269\n",
      "FOLD: 5, EPOCH: 31, train_loss: 0.017371315433568246\n",
      "FOLD: 5, EPOCH: 31, valid_loss: 0.019001874069754895\n",
      "FOLD: 5, EPOCH: 32, train_loss: 0.01725421576584513\n",
      "FOLD: 5, EPOCH: 32, valid_loss: 0.01896889994923885\n",
      "FOLD: 5, EPOCH: 33, train_loss: 0.01709183311794658\n",
      "FOLD: 5, EPOCH: 33, valid_loss: 0.018955929204821587\n",
      "FOLD: 5, EPOCH: 34, train_loss: 0.01700511625086939\n",
      "FOLD: 5, EPOCH: 34, valid_loss: 0.01894326937886385\n",
      "FOLD: 5, EPOCH: 35, train_loss: 0.016880661979116297\n",
      "FOLD: 5, EPOCH: 35, valid_loss: 0.018976409274798173\n",
      "FOLD: 5, EPOCH: 36, train_loss: 0.016760893845678988\n",
      "FOLD: 5, EPOCH: 36, valid_loss: 0.01899993634567811\n",
      "FOLD: 5, EPOCH: 37, train_loss: 0.016668893787003047\n",
      "FOLD: 5, EPOCH: 37, valid_loss: 0.0190041082409712\n",
      "Epoch    38: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 5, EPOCH: 38, train_loss: 0.016367621405201184\n",
      "FOLD: 5, EPOCH: 38, valid_loss: 0.018917774495023947\n",
      "FOLD: 5, EPOCH: 39, train_loss: 0.01621192119814254\n",
      "FOLD: 5, EPOCH: 39, valid_loss: 0.01889112161902281\n",
      "FOLD: 5, EPOCH: 40, train_loss: 0.016226094996405614\n",
      "FOLD: 5, EPOCH: 40, valid_loss: 0.018878488873059932\n",
      "FOLD: 5, EPOCH: 41, train_loss: 0.016140646766871214\n",
      "FOLD: 5, EPOCH: 41, valid_loss: 0.01887827982696203\n",
      "FOLD: 5, EPOCH: 42, train_loss: 0.016078244340983597\n",
      "FOLD: 5, EPOCH: 42, valid_loss: 0.018879451860602085\n",
      "FOLD: 5, EPOCH: 43, train_loss: 0.01598894740832416\n",
      "FOLD: 5, EPOCH: 43, valid_loss: 0.018867778663451854\n",
      "FOLD: 5, EPOCH: 44, train_loss: 0.015975531165462895\n",
      "FOLD: 5, EPOCH: 44, valid_loss: 0.018857042376811687\n",
      "FOLD: 5, EPOCH: 45, train_loss: 0.01596506712706508\n",
      "FOLD: 5, EPOCH: 45, valid_loss: 0.01885497784958436\n",
      "FOLD: 5, EPOCH: 46, train_loss: 0.01594276708626264\n",
      "FOLD: 5, EPOCH: 46, valid_loss: 0.018878843778601058\n",
      "FOLD: 5, EPOCH: 47, train_loss: 0.015904271874476125\n",
      "FOLD: 5, EPOCH: 47, valid_loss: 0.018841921996611815\n",
      "FOLD: 5, EPOCH: 48, train_loss: 0.015877510181854706\n",
      "FOLD: 5, EPOCH: 48, valid_loss: 0.018848102539777756\n",
      "FOLD: 5, EPOCH: 49, train_loss: 0.01590068077014105\n",
      "FOLD: 5, EPOCH: 49, valid_loss: 0.01885611692873331\n",
      "FOLD: 6, EPOCH: 0, train_loss: 0.04845341763182266\n",
      "FOLD: 6, EPOCH: 0, valid_loss: 0.02283253878928148\n",
      "FOLD: 6, EPOCH: 1, train_loss: 0.022392880750467647\n",
      "FOLD: 6, EPOCH: 1, valid_loss: 0.021700554742262915\n",
      "FOLD: 6, EPOCH: 2, train_loss: 0.021614926162402372\n",
      "FOLD: 6, EPOCH: 2, valid_loss: 0.021142226858780935\n",
      "FOLD: 6, EPOCH: 3, train_loss: 0.021228195193248825\n",
      "FOLD: 6, EPOCH: 3, valid_loss: 0.020988778139536198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 6, EPOCH: 4, train_loss: 0.02090061128743597\n",
      "FOLD: 6, EPOCH: 4, valid_loss: 0.02049742194895561\n",
      "FOLD: 6, EPOCH: 5, train_loss: 0.020686356325608654\n",
      "FOLD: 6, EPOCH: 5, valid_loss: 0.020144492387771606\n",
      "FOLD: 6, EPOCH: 6, train_loss: 0.020453415559353056\n",
      "FOLD: 6, EPOCH: 6, valid_loss: 0.020037393157298748\n",
      "FOLD: 6, EPOCH: 7, train_loss: 0.020234491701263027\n",
      "FOLD: 6, EPOCH: 7, valid_loss: 0.02005587160014189\n",
      "FOLD: 6, EPOCH: 8, train_loss: 0.020073069913967234\n",
      "FOLD: 6, EPOCH: 8, valid_loss: 0.019884405227807853\n",
      "FOLD: 6, EPOCH: 9, train_loss: 0.019948121890224314\n",
      "FOLD: 6, EPOCH: 9, valid_loss: 0.019729068789344568\n",
      "FOLD: 6, EPOCH: 10, train_loss: 0.01977003609912621\n",
      "FOLD: 6, EPOCH: 10, valid_loss: 0.019606357726913232\n",
      "FOLD: 6, EPOCH: 11, train_loss: 0.019580258861989587\n",
      "FOLD: 6, EPOCH: 11, valid_loss: 0.019512857525394514\n",
      "FOLD: 6, EPOCH: 12, train_loss: 0.019443225115537643\n",
      "FOLD: 6, EPOCH: 12, valid_loss: 0.019526845417343654\n",
      "FOLD: 6, EPOCH: 13, train_loss: 0.019368126057088375\n",
      "FOLD: 6, EPOCH: 13, valid_loss: 0.01948161509174567\n",
      "FOLD: 6, EPOCH: 14, train_loss: 0.019200165831559413\n",
      "FOLD: 6, EPOCH: 14, valid_loss: 0.019318091611449536\n",
      "FOLD: 6, EPOCH: 15, train_loss: 0.019120027512513304\n",
      "FOLD: 6, EPOCH: 15, valid_loss: 0.019392835549437083\n",
      "FOLD: 6, EPOCH: 16, train_loss: 0.01900847427345611\n",
      "FOLD: 6, EPOCH: 16, valid_loss: 0.01933957478747918\n",
      "FOLD: 6, EPOCH: 17, train_loss: 0.018862271505231794\n",
      "FOLD: 6, EPOCH: 17, valid_loss: 0.019226675423292015\n",
      "FOLD: 6, EPOCH: 18, train_loss: 0.018727202572532603\n",
      "FOLD: 6, EPOCH: 18, valid_loss: 0.0192889254540205\n",
      "FOLD: 6, EPOCH: 19, train_loss: 0.018665958497975324\n",
      "FOLD: 6, EPOCH: 19, valid_loss: 0.01913780360840834\n",
      "FOLD: 6, EPOCH: 20, train_loss: 0.018534764018211816\n",
      "FOLD: 6, EPOCH: 20, valid_loss: 0.019183464634876985\n",
      "FOLD: 6, EPOCH: 21, train_loss: 0.01841902664887744\n",
      "FOLD: 6, EPOCH: 21, valid_loss: 0.01915525157864277\n",
      "FOLD: 6, EPOCH: 22, train_loss: 0.018294659367686993\n",
      "FOLD: 6, EPOCH: 22, valid_loss: 0.019023857724208098\n",
      "FOLD: 6, EPOCH: 23, train_loss: 0.01818951964378357\n",
      "FOLD: 6, EPOCH: 23, valid_loss: 0.019021551626232956\n",
      "FOLD: 6, EPOCH: 24, train_loss: 0.01807138434535748\n",
      "FOLD: 6, EPOCH: 24, valid_loss: 0.01907929076025119\n",
      "FOLD: 6, EPOCH: 25, train_loss: 0.017946665426967916\n",
      "FOLD: 6, EPOCH: 25, valid_loss: 0.01898798203239074\n",
      "FOLD: 6, EPOCH: 26, train_loss: 0.017882446154347947\n",
      "FOLD: 6, EPOCH: 26, valid_loss: 0.019165141507983208\n",
      "FOLD: 6, EPOCH: 27, train_loss: 0.017756662709084717\n",
      "FOLD: 6, EPOCH: 27, valid_loss: 0.018994501290413048\n",
      "FOLD: 6, EPOCH: 28, train_loss: 0.017639072964320313\n",
      "FOLD: 6, EPOCH: 28, valid_loss: 0.01904096153493111\n",
      "Epoch    29: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 6, EPOCH: 29, train_loss: 0.017293935742329906\n",
      "FOLD: 6, EPOCH: 29, valid_loss: 0.018898060975166466\n",
      "FOLD: 6, EPOCH: 30, train_loss: 0.017157054682438437\n",
      "FOLD: 6, EPOCH: 30, valid_loss: 0.018859429714771416\n",
      "FOLD: 6, EPOCH: 31, train_loss: 0.017098263610859175\n",
      "FOLD: 6, EPOCH: 31, valid_loss: 0.018867225028001346\n",
      "FOLD: 6, EPOCH: 32, train_loss: 0.017052657952582515\n",
      "FOLD: 6, EPOCH: 32, valid_loss: 0.018852590225063838\n",
      "FOLD: 6, EPOCH: 33, train_loss: 0.017004357462094444\n",
      "FOLD: 6, EPOCH: 33, valid_loss: 0.01885417576592702\n",
      "FOLD: 6, EPOCH: 34, train_loss: 0.016976865360865723\n",
      "FOLD: 6, EPOCH: 34, valid_loss: 0.018825984488313015\n",
      "FOLD: 6, EPOCH: 35, train_loss: 0.016884954621058865\n",
      "FOLD: 6, EPOCH: 35, valid_loss: 0.018811561453800935\n",
      "FOLD: 6, EPOCH: 36, train_loss: 0.0168374192624076\n",
      "FOLD: 6, EPOCH: 36, valid_loss: 0.018819359202797595\n",
      "FOLD: 6, EPOCH: 37, train_loss: 0.016808407154639025\n",
      "FOLD: 6, EPOCH: 37, valid_loss: 0.018835019893371142\n",
      "FOLD: 6, EPOCH: 38, train_loss: 0.0167605518615125\n",
      "FOLD: 6, EPOCH: 38, valid_loss: 0.018824923783540726\n",
      "Epoch    39: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 6, EPOCH: 39, train_loss: 0.016717331609814555\n",
      "FOLD: 6, EPOCH: 39, valid_loss: 0.018797171087219164\n",
      "FOLD: 6, EPOCH: 40, train_loss: 0.01669347841242278\n",
      "FOLD: 6, EPOCH: 40, valid_loss: 0.018798745165650662\n",
      "FOLD: 6, EPOCH: 41, train_loss: 0.016737320070230478\n",
      "FOLD: 6, EPOCH: 41, valid_loss: 0.018815654831436966\n",
      "FOLD: 6, EPOCH: 42, train_loss: 0.016737008341462224\n",
      "FOLD: 6, EPOCH: 42, valid_loss: 0.01880243893426198\n",
      "Epoch    43: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 6, EPOCH: 43, train_loss: 0.016690858929241832\n",
      "FOLD: 6, EPOCH: 43, valid_loss: 0.01880564082127351\n",
      "FOLD: 6, EPOCH: 44, train_loss: 0.01669705513159971\n",
      "FOLD: 6, EPOCH: 44, valid_loss: 0.01879737741098954\n",
      "FOLD: 6, EPOCH: 45, train_loss: 0.016691504776276445\n",
      "FOLD: 6, EPOCH: 45, valid_loss: 0.018815299639335044\n",
      "Epoch    46: reducing learning rate of group 0 to 8.9345e-05.\n",
      "FOLD: 6, EPOCH: 46, train_loss: 0.016701759116069692\n",
      "FOLD: 6, EPOCH: 46, valid_loss: 0.018816776860218782\n",
      "FOLD: 6, EPOCH: 47, train_loss: 0.016698923968785518\n",
      "FOLD: 6, EPOCH: 47, valid_loss: 0.018808011825268086\n",
      "FOLD: 6, EPOCH: 48, train_loss: 0.016712032598317474\n",
      "FOLD: 6, EPOCH: 48, valid_loss: 0.018822609518583003\n",
      "Epoch    49: reducing learning rate of group 0 to 1.3371e-05.\n",
      "FOLD: 6, EPOCH: 49, train_loss: 0.01668389602186712\n",
      "FOLD: 6, EPOCH: 49, valid_loss: 0.018822315507210217\n",
      "FOLD: 0, EPOCH: 0, train_loss: 0.0462826767281906\n",
      "FOLD: 0, EPOCH: 0, valid_loss: 0.023002044369394962\n",
      "FOLD: 0, EPOCH: 1, train_loss: 0.022242655977606773\n",
      "FOLD: 0, EPOCH: 1, valid_loss: 0.021695847837970808\n",
      "FOLD: 0, EPOCH: 2, train_loss: 0.021548794129410305\n",
      "FOLD: 0, EPOCH: 2, valid_loss: 0.020936635681069814\n",
      "FOLD: 0, EPOCH: 3, train_loss: 0.021197350116799008\n",
      "FOLD: 0, EPOCH: 3, valid_loss: 0.021332342320909865\n",
      "FOLD: 0, EPOCH: 4, train_loss: 0.020891315253401124\n",
      "FOLD: 0, EPOCH: 4, valid_loss: 0.020270083386164445\n",
      "FOLD: 0, EPOCH: 5, train_loss: 0.02069632361668187\n",
      "FOLD: 0, EPOCH: 5, valid_loss: 0.02028332664989508\n",
      "FOLD: 0, EPOCH: 6, train_loss: 0.02052591824148958\n",
      "FOLD: 0, EPOCH: 6, valid_loss: 0.02023954216677409\n",
      "FOLD: 0, EPOCH: 7, train_loss: 0.020331499494008115\n",
      "FOLD: 0, EPOCH: 7, valid_loss: 0.019812968631203357\n",
      "FOLD: 0, EPOCH: 8, train_loss: 0.020163560537873087\n",
      "FOLD: 0, EPOCH: 8, valid_loss: 0.019944652915000916\n",
      "FOLD: 0, EPOCH: 9, train_loss: 0.02002952401400418\n",
      "FOLD: 0, EPOCH: 9, valid_loss: 0.019657436471719008\n",
      "FOLD: 0, EPOCH: 10, train_loss: 0.01982972280097169\n",
      "FOLD: 0, EPOCH: 10, valid_loss: 0.019711970996398192\n",
      "FOLD: 0, EPOCH: 11, train_loss: 0.019772905931883567\n",
      "FOLD: 0, EPOCH: 11, valid_loss: 0.019501158967614174\n",
      "FOLD: 0, EPOCH: 12, train_loss: 0.01960958809768026\n",
      "FOLD: 0, EPOCH: 12, valid_loss: 0.01945257201217688\n",
      "FOLD: 0, EPOCH: 13, train_loss: 0.019485859326212794\n",
      "FOLD: 0, EPOCH: 13, valid_loss: 0.019405938541659944\n",
      "FOLD: 0, EPOCH: 14, train_loss: 0.019394820178481372\n",
      "FOLD: 0, EPOCH: 14, valid_loss: 0.019450699194119528\n",
      "FOLD: 0, EPOCH: 15, train_loss: 0.01921838360863763\n",
      "FOLD: 0, EPOCH: 15, valid_loss: 0.01929531704920989\n",
      "FOLD: 0, EPOCH: 16, train_loss: 0.019115879930354452\n",
      "FOLD: 0, EPOCH: 16, valid_loss: 0.019330932543827936\n",
      "FOLD: 0, EPOCH: 17, train_loss: 0.019019037963369408\n",
      "FOLD: 0, EPOCH: 17, valid_loss: 0.01924862927542283\n",
      "FOLD: 0, EPOCH: 18, train_loss: 0.01894276579086845\n",
      "FOLD: 0, EPOCH: 18, valid_loss: 0.0191265784490567\n",
      "FOLD: 0, EPOCH: 19, train_loss: 0.018839480430894607\n",
      "FOLD: 0, EPOCH: 19, valid_loss: 0.01916264656644601\n",
      "FOLD: 0, EPOCH: 20, train_loss: 0.018731870310934814\n",
      "FOLD: 0, EPOCH: 20, valid_loss: 0.019118716653722983\n",
      "FOLD: 0, EPOCH: 21, train_loss: 0.018596828210394125\n",
      "FOLD: 0, EPOCH: 21, valid_loss: 0.019017992684474357\n",
      "FOLD: 0, EPOCH: 22, train_loss: 0.018524577992187964\n",
      "FOLD: 0, EPOCH: 22, valid_loss: 0.01903830482982672\n",
      "FOLD: 0, EPOCH: 23, train_loss: 0.01842591679982237\n",
      "FOLD: 0, EPOCH: 23, valid_loss: 0.019020413836607568\n",
      "FOLD: 0, EPOCH: 24, train_loss: 0.018257606497688872\n",
      "FOLD: 0, EPOCH: 24, valid_loss: 0.019057508558034897\n",
      "Epoch    25: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 0, EPOCH: 25, train_loss: 0.017986120253398612\n",
      "FOLD: 0, EPOCH: 25, valid_loss: 0.018853406636760786\n",
      "FOLD: 0, EPOCH: 26, train_loss: 0.01787431928253657\n",
      "FOLD: 0, EPOCH: 26, valid_loss: 0.01884567178785801\n",
      "FOLD: 0, EPOCH: 27, train_loss: 0.017765849287546164\n",
      "FOLD: 0, EPOCH: 27, valid_loss: 0.01882914682993522\n",
      "FOLD: 0, EPOCH: 28, train_loss: 0.017704698282319145\n",
      "FOLD: 0, EPOCH: 28, valid_loss: 0.01881930189063916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 0, EPOCH: 29, train_loss: 0.017741740177813416\n",
      "FOLD: 0, EPOCH: 29, valid_loss: 0.01881130340580757\n",
      "FOLD: 0, EPOCH: 30, train_loss: 0.017616788428780193\n",
      "FOLD: 0, EPOCH: 30, valid_loss: 0.01882674344457113\n",
      "FOLD: 0, EPOCH: 31, train_loss: 0.01760217416528109\n",
      "FOLD: 0, EPOCH: 31, valid_loss: 0.01881805506463234\n",
      "FOLD: 0, EPOCH: 32, train_loss: 0.01758918835706002\n",
      "FOLD: 0, EPOCH: 32, valid_loss: 0.018792373056595143\n",
      "FOLD: 0, EPOCH: 33, train_loss: 0.01752844694498423\n",
      "FOLD: 0, EPOCH: 33, valid_loss: 0.01878967694938183\n",
      "FOLD: 0, EPOCH: 34, train_loss: 0.017507771432802483\n",
      "FOLD: 0, EPOCH: 34, valid_loss: 0.018801318481564522\n",
      "FOLD: 0, EPOCH: 35, train_loss: 0.017466841894831206\n",
      "FOLD: 0, EPOCH: 35, valid_loss: 0.01878587256830472\n",
      "FOLD: 0, EPOCH: 36, train_loss: 0.017412224180392316\n",
      "FOLD: 0, EPOCH: 36, valid_loss: 0.01878732557480152\n",
      "FOLD: 0, EPOCH: 37, train_loss: 0.017409449594246375\n",
      "FOLD: 0, EPOCH: 37, valid_loss: 0.018778633183011643\n",
      "FOLD: 0, EPOCH: 38, train_loss: 0.01738965657313128\n",
      "FOLD: 0, EPOCH: 38, valid_loss: 0.018780407280876085\n",
      "FOLD: 0, EPOCH: 39, train_loss: 0.017348528487255443\n",
      "FOLD: 0, EPOCH: 39, valid_loss: 0.018781184290464107\n",
      "FOLD: 0, EPOCH: 40, train_loss: 0.01732367418102316\n",
      "FOLD: 0, EPOCH: 40, valid_loss: 0.01875940767618326\n",
      "FOLD: 0, EPOCH: 41, train_loss: 0.017308668611017434\n",
      "FOLD: 0, EPOCH: 41, valid_loss: 0.018769647926092148\n",
      "FOLD: 0, EPOCH: 42, train_loss: 0.017286462407257105\n",
      "FOLD: 0, EPOCH: 42, valid_loss: 0.018775800099739663\n",
      "FOLD: 0, EPOCH: 43, train_loss: 0.017241954803466797\n",
      "FOLD: 0, EPOCH: 43, valid_loss: 0.01876050649354091\n",
      "Epoch    44: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 0, EPOCH: 44, train_loss: 0.01721325219684356\n",
      "FOLD: 0, EPOCH: 44, valid_loss: 0.01876986900774332\n",
      "FOLD: 0, EPOCH: 45, train_loss: 0.0172173153055278\n",
      "FOLD: 0, EPOCH: 45, valid_loss: 0.01877608049947482\n",
      "FOLD: 0, EPOCH: 46, train_loss: 0.017203155074369262\n",
      "FOLD: 0, EPOCH: 46, valid_loss: 0.018759815882031735\n",
      "Epoch    47: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 0, EPOCH: 47, train_loss: 0.017224139911500184\n",
      "FOLD: 0, EPOCH: 47, valid_loss: 0.01874739261200795\n",
      "FOLD: 0, EPOCH: 48, train_loss: 0.017192795092391\n",
      "FOLD: 0, EPOCH: 48, valid_loss: 0.018765423590174087\n",
      "FOLD: 0, EPOCH: 49, train_loss: 0.017199701528895547\n",
      "FOLD: 0, EPOCH: 49, valid_loss: 0.018760948800123654\n",
      "FOLD: 1, EPOCH: 0, train_loss: 0.04850911044490498\n",
      "FOLD: 1, EPOCH: 0, valid_loss: 0.023649385772072352\n",
      "FOLD: 1, EPOCH: 1, train_loss: 0.02244674353986173\n",
      "FOLD: 1, EPOCH: 1, valid_loss: 0.02223513934474725\n",
      "FOLD: 1, EPOCH: 2, train_loss: 0.02166632064492316\n",
      "FOLD: 1, EPOCH: 2, valid_loss: 0.021650147839234427\n",
      "FOLD: 1, EPOCH: 3, train_loss: 0.021256160569956172\n",
      "FOLD: 1, EPOCH: 3, valid_loss: 0.021231126326781053\n",
      "FOLD: 1, EPOCH: 4, train_loss: 0.02099751522512855\n",
      "FOLD: 1, EPOCH: 4, valid_loss: 0.0211203066775432\n",
      "FOLD: 1, EPOCH: 5, train_loss: 0.02069986138392139\n",
      "FOLD: 1, EPOCH: 5, valid_loss: 0.021187817248014305\n",
      "FOLD: 1, EPOCH: 6, train_loss: 0.02045485339555386\n",
      "FOLD: 1, EPOCH: 6, valid_loss: 0.020569546721302547\n",
      "FOLD: 1, EPOCH: 7, train_loss: 0.020256505951889464\n",
      "FOLD: 1, EPOCH: 7, valid_loss: 0.020372122669449218\n",
      "FOLD: 1, EPOCH: 8, train_loss: 0.020068556322036562\n",
      "FOLD: 1, EPOCH: 8, valid_loss: 0.020429277362731788\n",
      "FOLD: 1, EPOCH: 9, train_loss: 0.019899882149656076\n",
      "FOLD: 1, EPOCH: 9, valid_loss: 0.02018462336407258\n",
      "FOLD: 1, EPOCH: 10, train_loss: 0.01977251165521306\n",
      "FOLD: 1, EPOCH: 10, valid_loss: 0.01989309575695258\n",
      "FOLD: 1, EPOCH: 11, train_loss: 0.019569220952689648\n",
      "FOLD: 1, EPOCH: 11, valid_loss: 0.01976644477018943\n",
      "FOLD: 1, EPOCH: 12, train_loss: 0.019457708364604292\n",
      "FOLD: 1, EPOCH: 12, valid_loss: 0.020067721175459716\n",
      "FOLD: 1, EPOCH: 13, train_loss: 0.019327828534752935\n",
      "FOLD: 1, EPOCH: 13, valid_loss: 0.019600882266576473\n",
      "FOLD: 1, EPOCH: 14, train_loss: 0.01914840382901398\n",
      "FOLD: 1, EPOCH: 14, valid_loss: 0.01962486969736906\n",
      "FOLD: 1, EPOCH: 15, train_loss: 0.01899847645010497\n",
      "FOLD: 1, EPOCH: 15, valid_loss: 0.019485951043092288\n",
      "FOLD: 1, EPOCH: 16, train_loss: 0.01889683011717893\n",
      "FOLD: 1, EPOCH: 16, valid_loss: 0.01939826516004709\n",
      "FOLD: 1, EPOCH: 17, train_loss: 0.018724458946569544\n",
      "FOLD: 1, EPOCH: 17, valid_loss: 0.019393207505345345\n",
      "FOLD: 1, EPOCH: 18, train_loss: 0.018603060772088734\n",
      "FOLD: 1, EPOCH: 18, valid_loss: 0.019652973287380658\n",
      "FOLD: 1, EPOCH: 19, train_loss: 0.018509156297187548\n",
      "FOLD: 1, EPOCH: 19, valid_loss: 0.019445339504342813\n",
      "FOLD: 1, EPOCH: 20, train_loss: 0.01839354372507817\n",
      "FOLD: 1, EPOCH: 20, valid_loss: 0.019339756610301826\n",
      "FOLD: 1, EPOCH: 21, train_loss: 0.018260840351718502\n",
      "FOLD: 1, EPOCH: 21, valid_loss: 0.019308950322178695\n",
      "FOLD: 1, EPOCH: 22, train_loss: 0.01812004834111478\n",
      "FOLD: 1, EPOCH: 22, valid_loss: 0.019257530140189025\n",
      "FOLD: 1, EPOCH: 23, train_loss: 0.01801960137546868\n",
      "FOLD: 1, EPOCH: 23, valid_loss: 0.019257783746490113\n",
      "FOLD: 1, EPOCH: 24, train_loss: 0.017866864875965827\n",
      "FOLD: 1, EPOCH: 24, valid_loss: 0.019244408664795067\n",
      "FOLD: 1, EPOCH: 25, train_loss: 0.017737860313138447\n",
      "FOLD: 1, EPOCH: 25, valid_loss: 0.01930932915554597\n",
      "FOLD: 1, EPOCH: 26, train_loss: 0.017625298992906872\n",
      "FOLD: 1, EPOCH: 26, valid_loss: 0.019177751902204294\n",
      "FOLD: 1, EPOCH: 27, train_loss: 0.017498243562373762\n",
      "FOLD: 1, EPOCH: 27, valid_loss: 0.019170937056724843\n",
      "FOLD: 1, EPOCH: 28, train_loss: 0.017324022042590217\n",
      "FOLD: 1, EPOCH: 28, valid_loss: 0.019100787691198863\n",
      "FOLD: 1, EPOCH: 29, train_loss: 0.01720744445668282\n",
      "FOLD: 1, EPOCH: 29, valid_loss: 0.019157559395982668\n",
      "FOLD: 1, EPOCH: 30, train_loss: 0.017064231322021096\n",
      "FOLD: 1, EPOCH: 30, valid_loss: 0.01920647704257415\n",
      "FOLD: 1, EPOCH: 31, train_loss: 0.01693441511156994\n",
      "FOLD: 1, EPOCH: 31, valid_loss: 0.019145280552598145\n",
      "Epoch    32: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 1, EPOCH: 32, train_loss: 0.016572449608026323\n",
      "FOLD: 1, EPOCH: 32, valid_loss: 0.019058384287815828\n",
      "FOLD: 1, EPOCH: 33, train_loss: 0.016432631043465557\n",
      "FOLD: 1, EPOCH: 33, valid_loss: 0.019049435424116943\n",
      "FOLD: 1, EPOCH: 34, train_loss: 0.0163388877880533\n",
      "FOLD: 1, EPOCH: 34, valid_loss: 0.01901924782074415\n",
      "FOLD: 1, EPOCH: 35, train_loss: 0.01629480693136921\n",
      "FOLD: 1, EPOCH: 35, valid_loss: 0.019023984097517453\n",
      "FOLD: 1, EPOCH: 36, train_loss: 0.016248159903428844\n",
      "FOLD: 1, EPOCH: 36, valid_loss: 0.01901874003502039\n",
      "FOLD: 1, EPOCH: 37, train_loss: 0.01617771224748041\n",
      "FOLD: 1, EPOCH: 37, valid_loss: 0.01899637124286248\n",
      "FOLD: 1, EPOCH: 38, train_loss: 0.016112515728014545\n",
      "FOLD: 1, EPOCH: 38, valid_loss: 0.019018232822418213\n",
      "FOLD: 1, EPOCH: 39, train_loss: 0.016097642909232025\n",
      "FOLD: 1, EPOCH: 39, valid_loss: 0.01902893314567896\n",
      "FOLD: 1, EPOCH: 40, train_loss: 0.016061860929570487\n",
      "FOLD: 1, EPOCH: 40, valid_loss: 0.019019734114408493\n",
      "Epoch    41: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 1, EPOCH: 41, train_loss: 0.015943694897499437\n",
      "FOLD: 1, EPOCH: 41, valid_loss: 0.019015042541118767\n",
      "FOLD: 1, EPOCH: 42, train_loss: 0.015978668828973093\n",
      "FOLD: 1, EPOCH: 42, valid_loss: 0.019028388107052215\n",
      "FOLD: 1, EPOCH: 43, train_loss: 0.01597718587396918\n",
      "FOLD: 1, EPOCH: 43, valid_loss: 0.01900531853047701\n",
      "Epoch    44: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 1, EPOCH: 44, train_loss: 0.016005440992680756\n",
      "FOLD: 1, EPOCH: 44, valid_loss: 0.018997750603235684\n",
      "FOLD: 1, EPOCH: 45, train_loss: 0.015971725064052922\n",
      "FOLD: 1, EPOCH: 45, valid_loss: 0.018998555265940152\n",
      "FOLD: 1, EPOCH: 46, train_loss: 0.01592911811344124\n",
      "FOLD: 1, EPOCH: 46, valid_loss: 0.01899924143575705\n",
      "Epoch    47: reducing learning rate of group 0 to 8.9345e-05.\n",
      "FOLD: 1, EPOCH: 47, train_loss: 0.016022849639223236\n",
      "FOLD: 1, EPOCH: 47, valid_loss: 0.019022270750540953\n",
      "FOLD: 1, EPOCH: 48, train_loss: 0.0159797447960119\n",
      "FOLD: 1, EPOCH: 48, valid_loss: 0.019005719858866472\n",
      "FOLD: 1, EPOCH: 49, train_loss: 0.0159993787371629\n",
      "FOLD: 1, EPOCH: 49, valid_loss: 0.019010228749651175\n",
      "Epoch    50: reducing learning rate of group 0 to 1.3371e-05.\n",
      "FOLD: 2, EPOCH: 0, train_loss: 0.0475101153290755\n",
      "FOLD: 2, EPOCH: 0, valid_loss: 0.023433337847773846\n",
      "FOLD: 2, EPOCH: 1, train_loss: 0.022254918097845605\n",
      "FOLD: 2, EPOCH: 1, valid_loss: 0.022045910071868163\n",
      "FOLD: 2, EPOCH: 2, train_loss: 0.021549125353025424\n",
      "FOLD: 2, EPOCH: 2, valid_loss: 0.021563485121497743\n",
      "FOLD: 2, EPOCH: 3, train_loss: 0.021190183488903818\n",
      "FOLD: 2, EPOCH: 3, valid_loss: 0.02117679311105838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 2, EPOCH: 4, train_loss: 0.02084033553664749\n",
      "FOLD: 2, EPOCH: 4, valid_loss: 0.020819539634081032\n",
      "FOLD: 2, EPOCH: 5, train_loss: 0.020578328268350782\n",
      "FOLD: 2, EPOCH: 5, valid_loss: 0.020549608681064386\n",
      "FOLD: 2, EPOCH: 6, train_loss: 0.02037570670851179\n",
      "FOLD: 2, EPOCH: 6, valid_loss: 0.02056210660017454\n",
      "FOLD: 2, EPOCH: 7, train_loss: 0.02021851607069776\n",
      "FOLD: 2, EPOCH: 7, valid_loss: 0.02041457607769049\n",
      "FOLD: 2, EPOCH: 8, train_loss: 0.020025642588734627\n",
      "FOLD: 2, EPOCH: 8, valid_loss: 0.020242959547501344\n",
      "FOLD: 2, EPOCH: 9, train_loss: 0.019836497241379442\n",
      "FOLD: 2, EPOCH: 9, valid_loss: 0.020079311843101796\n",
      "FOLD: 2, EPOCH: 10, train_loss: 0.019699534099247004\n",
      "FOLD: 2, EPOCH: 10, valid_loss: 0.019927608135801095\n",
      "FOLD: 2, EPOCH: 11, train_loss: 0.01958974523822198\n",
      "FOLD: 2, EPOCH: 11, valid_loss: 0.019867317464489203\n",
      "FOLD: 2, EPOCH: 12, train_loss: 0.019423409742680756\n",
      "FOLD: 2, EPOCH: 12, valid_loss: 0.020170222251461103\n",
      "FOLD: 2, EPOCH: 13, train_loss: 0.019306778681237955\n",
      "FOLD: 2, EPOCH: 13, valid_loss: 0.019798270498330776\n",
      "FOLD: 2, EPOCH: 14, train_loss: 0.019171369136185258\n",
      "FOLD: 2, EPOCH: 14, valid_loss: 0.019640455022454262\n",
      "FOLD: 2, EPOCH: 15, train_loss: 0.019007041018355538\n",
      "FOLD: 2, EPOCH: 15, valid_loss: 0.019615554608977757\n",
      "FOLD: 2, EPOCH: 16, train_loss: 0.018930532088553585\n",
      "FOLD: 2, EPOCH: 16, valid_loss: 0.019625687685150366\n",
      "FOLD: 2, EPOCH: 17, train_loss: 0.01880807875076661\n",
      "FOLD: 2, EPOCH: 17, valid_loss: 0.020112325222446367\n",
      "FOLD: 2, EPOCH: 18, train_loss: 0.01869816262577031\n",
      "FOLD: 2, EPOCH: 18, valid_loss: 0.019542960450053215\n",
      "FOLD: 2, EPOCH: 19, train_loss: 0.018601484923950723\n",
      "FOLD: 2, EPOCH: 19, valid_loss: 0.01951840863778041\n",
      "FOLD: 2, EPOCH: 20, train_loss: 0.01843500167534158\n",
      "FOLD: 2, EPOCH: 20, valid_loss: 0.019402744248509407\n",
      "FOLD: 2, EPOCH: 21, train_loss: 0.01835553186970788\n",
      "FOLD: 2, EPOCH: 21, valid_loss: 0.01939667446108965\n",
      "FOLD: 2, EPOCH: 22, train_loss: 0.018237089461370093\n",
      "FOLD: 2, EPOCH: 22, valid_loss: 0.019233636558055878\n",
      "FOLD: 2, EPOCH: 23, train_loss: 0.018109175225568784\n",
      "FOLD: 2, EPOCH: 23, valid_loss: 0.019275981073196117\n",
      "FOLD: 2, EPOCH: 24, train_loss: 0.018037095640760822\n",
      "FOLD: 2, EPOCH: 24, valid_loss: 0.01934541962467707\n",
      "FOLD: 2, EPOCH: 25, train_loss: 0.017928588480965513\n",
      "FOLD: 2, EPOCH: 25, valid_loss: 0.019233973840108283\n",
      "Epoch    26: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 2, EPOCH: 26, train_loss: 0.01758039158743781\n",
      "FOLD: 2, EPOCH: 26, valid_loss: 0.019168759481264994\n",
      "FOLD: 2, EPOCH: 27, train_loss: 0.017484652709115197\n",
      "FOLD: 2, EPOCH: 27, valid_loss: 0.01914019939991144\n",
      "FOLD: 2, EPOCH: 28, train_loss: 0.01739798710253593\n",
      "FOLD: 2, EPOCH: 28, valid_loss: 0.01913623296870635\n",
      "FOLD: 2, EPOCH: 29, train_loss: 0.017351797001587378\n",
      "FOLD: 2, EPOCH: 29, valid_loss: 0.01913561815252671\n",
      "FOLD: 2, EPOCH: 30, train_loss: 0.017275478086761525\n",
      "FOLD: 2, EPOCH: 30, valid_loss: 0.01912200293288781\n",
      "FOLD: 2, EPOCH: 31, train_loss: 0.01721170514419272\n",
      "FOLD: 2, EPOCH: 31, valid_loss: 0.01912191639152857\n",
      "FOLD: 2, EPOCH: 32, train_loss: 0.017190775469046186\n",
      "FOLD: 2, EPOCH: 32, valid_loss: 0.019108998231016673\n",
      "FOLD: 2, EPOCH: 33, train_loss: 0.01710436279205857\n",
      "FOLD: 2, EPOCH: 33, valid_loss: 0.01910008891270711\n",
      "FOLD: 2, EPOCH: 34, train_loss: 0.01706688538999171\n",
      "FOLD: 2, EPOCH: 34, valid_loss: 0.01909768581390381\n",
      "FOLD: 2, EPOCH: 35, train_loss: 0.017095105206543528\n",
      "FOLD: 2, EPOCH: 35, valid_loss: 0.019074905950289506\n",
      "FOLD: 2, EPOCH: 36, train_loss: 0.017021873991030292\n",
      "FOLD: 2, EPOCH: 36, valid_loss: 0.019089628440829422\n",
      "FOLD: 2, EPOCH: 37, train_loss: 0.017041635724741058\n",
      "FOLD: 2, EPOCH: 37, valid_loss: 0.019099014739577588\n",
      "FOLD: 2, EPOCH: 38, train_loss: 0.016944796539138298\n",
      "FOLD: 2, EPOCH: 38, valid_loss: 0.01909374975814269\n",
      "Epoch    39: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 2, EPOCH: 39, train_loss: 0.016914160242555914\n",
      "FOLD: 2, EPOCH: 39, valid_loss: 0.019097495250977002\n",
      "FOLD: 2, EPOCH: 40, train_loss: 0.01689958824096499\n",
      "FOLD: 2, EPOCH: 40, valid_loss: 0.019110961315723565\n",
      "FOLD: 2, EPOCH: 41, train_loss: 0.016906676675520232\n",
      "FOLD: 2, EPOCH: 41, valid_loss: 0.019094605142107375\n",
      "Epoch    42: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 2, EPOCH: 42, train_loss: 0.01692121389095445\n",
      "FOLD: 2, EPOCH: 42, valid_loss: 0.019071391138892908\n",
      "FOLD: 2, EPOCH: 43, train_loss: 0.016892307500883535\n",
      "FOLD: 2, EPOCH: 43, valid_loss: 0.01909371866629674\n",
      "FOLD: 2, EPOCH: 44, train_loss: 0.01693184082270474\n",
      "FOLD: 2, EPOCH: 44, valid_loss: 0.019097109253589924\n",
      "FOLD: 2, EPOCH: 45, train_loss: 0.016942239479740728\n",
      "FOLD: 2, EPOCH: 45, valid_loss: 0.01908406185416075\n",
      "Epoch    46: reducing learning rate of group 0 to 8.9345e-05.\n",
      "FOLD: 2, EPOCH: 46, train_loss: 0.01690590993274708\n",
      "FOLD: 2, EPOCH: 46, valid_loss: 0.01907347644177767\n",
      "FOLD: 2, EPOCH: 47, train_loss: 0.0169105091685983\n",
      "FOLD: 2, EPOCH: 47, valid_loss: 0.01909241868326297\n",
      "FOLD: 2, EPOCH: 48, train_loss: 0.016931219152300746\n",
      "FOLD: 2, EPOCH: 48, valid_loss: 0.0190714391378256\n",
      "Epoch    49: reducing learning rate of group 0 to 1.3371e-05.\n",
      "FOLD: 2, EPOCH: 49, train_loss: 0.016949377425417706\n",
      "FOLD: 2, EPOCH: 49, valid_loss: 0.019078191799613144\n",
      "FOLD: 3, EPOCH: 0, train_loss: 0.04930233172568921\n",
      "FOLD: 3, EPOCH: 0, valid_loss: 0.02252023839033567\n",
      "FOLD: 3, EPOCH: 1, train_loss: 0.02245139763564677\n",
      "FOLD: 3, EPOCH: 1, valid_loss: 0.022110643008580573\n",
      "FOLD: 3, EPOCH: 2, train_loss: 0.021686202871638374\n",
      "FOLD: 3, EPOCH: 2, valid_loss: 0.021490333458551995\n",
      "FOLD: 3, EPOCH: 3, train_loss: 0.021276331808720086\n",
      "FOLD: 3, EPOCH: 3, valid_loss: 0.02124395651312975\n",
      "FOLD: 3, EPOCH: 4, train_loss: 0.021005407101600558\n",
      "FOLD: 3, EPOCH: 4, valid_loss: 0.02069626605281463\n",
      "FOLD: 3, EPOCH: 5, train_loss: 0.020711797289550304\n",
      "FOLD: 3, EPOCH: 5, valid_loss: 0.020484922023919914\n",
      "FOLD: 3, EPOCH: 6, train_loss: 0.020476440093605906\n",
      "FOLD: 3, EPOCH: 6, valid_loss: 0.020215338955704983\n",
      "FOLD: 3, EPOCH: 7, train_loss: 0.020257560032847767\n",
      "FOLD: 3, EPOCH: 7, valid_loss: 0.020107406119887646\n",
      "FOLD: 3, EPOCH: 8, train_loss: 0.020110715885420103\n",
      "FOLD: 3, EPOCH: 8, valid_loss: 0.020005141456539813\n",
      "FOLD: 3, EPOCH: 9, train_loss: 0.01995584001210896\n",
      "FOLD: 3, EPOCH: 9, valid_loss: 0.01992859748693613\n",
      "FOLD: 3, EPOCH: 10, train_loss: 0.019777061895945587\n",
      "FOLD: 3, EPOCH: 10, valid_loss: 0.01981343486561225\n",
      "FOLD: 3, EPOCH: 11, train_loss: 0.019609350434227568\n",
      "FOLD: 3, EPOCH: 11, valid_loss: 0.0196844615901892\n",
      "FOLD: 3, EPOCH: 12, train_loss: 0.01950068152635484\n",
      "FOLD: 3, EPOCH: 12, valid_loss: 0.019722016528248787\n",
      "FOLD: 3, EPOCH: 13, train_loss: 0.019396899318372882\n",
      "FOLD: 3, EPOCH: 13, valid_loss: 0.019671743878951438\n",
      "FOLD: 3, EPOCH: 14, train_loss: 0.01918641251285334\n",
      "FOLD: 3, EPOCH: 14, valid_loss: 0.019519016433220644\n",
      "FOLD: 3, EPOCH: 15, train_loss: 0.019087578522393834\n",
      "FOLD: 3, EPOCH: 15, valid_loss: 0.019413332239939615\n",
      "FOLD: 3, EPOCH: 16, train_loss: 0.01893254417321972\n",
      "FOLD: 3, EPOCH: 16, valid_loss: 0.019364031604849376\n",
      "FOLD: 3, EPOCH: 17, train_loss: 0.01883118154128661\n",
      "FOLD: 3, EPOCH: 17, valid_loss: 0.02315095654473855\n",
      "FOLD: 3, EPOCH: 18, train_loss: 0.018749518269622647\n",
      "FOLD: 3, EPOCH: 18, valid_loss: 0.019436392933130264\n",
      "FOLD: 3, EPOCH: 19, train_loss: 0.01861067657434457\n",
      "FOLD: 3, EPOCH: 19, valid_loss: 0.01925215583581191\n",
      "FOLD: 3, EPOCH: 20, train_loss: 0.01850194136637288\n",
      "FOLD: 3, EPOCH: 20, valid_loss: 0.019216239309081666\n",
      "FOLD: 3, EPOCH: 21, train_loss: 0.01832059974706656\n",
      "FOLD: 3, EPOCH: 21, valid_loss: 0.01919154808498346\n",
      "FOLD: 3, EPOCH: 22, train_loss: 0.018275665924758523\n",
      "FOLD: 3, EPOCH: 22, valid_loss: 0.01920905035848801\n",
      "FOLD: 3, EPOCH: 23, train_loss: 0.018067047100614856\n",
      "FOLD: 3, EPOCH: 23, valid_loss: 0.01923524086865095\n",
      "FOLD: 3, EPOCH: 24, train_loss: 0.017992513556335424\n",
      "FOLD: 3, EPOCH: 24, valid_loss: 0.019066733952898245\n",
      "FOLD: 3, EPOCH: 25, train_loss: 0.017947630629547545\n",
      "FOLD: 3, EPOCH: 25, valid_loss: 0.019146231074745838\n",
      "FOLD: 3, EPOCH: 26, train_loss: 0.017781057722262433\n",
      "FOLD: 3, EPOCH: 26, valid_loss: 0.0190853221485248\n",
      "FOLD: 3, EPOCH: 27, train_loss: 0.01763151341898216\n",
      "FOLD: 3, EPOCH: 27, valid_loss: 0.019166460404029258\n",
      "Epoch    28: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 3, EPOCH: 28, train_loss: 0.017257112254564826\n",
      "FOLD: 3, EPOCH: 28, valid_loss: 0.019036844229468934\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 3, EPOCH: 29, train_loss: 0.01716002673414108\n",
      "FOLD: 3, EPOCH: 29, valid_loss: 0.01898424943479208\n",
      "FOLD: 3, EPOCH: 30, train_loss: 0.01702260661467507\n",
      "FOLD: 3, EPOCH: 30, valid_loss: 0.01898503117263317\n",
      "FOLD: 3, EPOCH: 31, train_loss: 0.017006683113003098\n",
      "FOLD: 3, EPOCH: 31, valid_loss: 0.018956066467441045\n",
      "FOLD: 3, EPOCH: 32, train_loss: 0.016960639607261966\n",
      "FOLD: 3, EPOCH: 32, valid_loss: 0.018976960904323138\n",
      "FOLD: 3, EPOCH: 33, train_loss: 0.01689529960119241\n",
      "FOLD: 3, EPOCH: 33, valid_loss: 0.01895731157408311\n",
      "FOLD: 3, EPOCH: 34, train_loss: 0.016825365330520516\n",
      "FOLD: 3, EPOCH: 34, valid_loss: 0.018978025334385727\n",
      "Epoch    35: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 3, EPOCH: 35, train_loss: 0.01682449589055535\n",
      "FOLD: 3, EPOCH: 35, valid_loss: 0.018952862717784368\n",
      "FOLD: 3, EPOCH: 36, train_loss: 0.016761131949622084\n",
      "FOLD: 3, EPOCH: 36, valid_loss: 0.018953455612063408\n",
      "FOLD: 3, EPOCH: 37, train_loss: 0.016756514024392172\n",
      "FOLD: 3, EPOCH: 37, valid_loss: 0.018961741517369565\n",
      "FOLD: 3, EPOCH: 38, train_loss: 0.016742264524706313\n",
      "FOLD: 3, EPOCH: 38, valid_loss: 0.01895061206932251\n",
      "FOLD: 3, EPOCH: 39, train_loss: 0.016773217420622304\n",
      "FOLD: 3, EPOCH: 39, valid_loss: 0.018968743057205126\n",
      "FOLD: 3, EPOCH: 40, train_loss: 0.016760611894062243\n",
      "FOLD: 3, EPOCH: 40, valid_loss: 0.018935311728945144\n",
      "FOLD: 3, EPOCH: 41, train_loss: 0.01673175039625651\n",
      "FOLD: 3, EPOCH: 41, valid_loss: 0.01896025283405414\n",
      "FOLD: 3, EPOCH: 42, train_loss: 0.01675987200861847\n",
      "FOLD: 3, EPOCH: 42, valid_loss: 0.018950525384682875\n",
      "FOLD: 3, EPOCH: 43, train_loss: 0.016737557469388924\n",
      "FOLD: 3, EPOCH: 43, valid_loss: 0.018946628874311082\n",
      "Epoch    44: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 3, EPOCH: 44, train_loss: 0.016747805239582383\n",
      "FOLD: 3, EPOCH: 44, valid_loss: 0.01895382312627939\n",
      "FOLD: 3, EPOCH: 45, train_loss: 0.01673915356750021\n",
      "FOLD: 3, EPOCH: 45, valid_loss: 0.018962057164082162\n",
      "FOLD: 3, EPOCH: 46, train_loss: 0.016780718959666586\n",
      "FOLD: 3, EPOCH: 46, valid_loss: 0.01894454242518315\n",
      "Epoch    47: reducing learning rate of group 0 to 8.9345e-05.\n",
      "FOLD: 3, EPOCH: 47, train_loss: 0.016726913597636128\n",
      "FOLD: 3, EPOCH: 47, valid_loss: 0.018973740104299325\n",
      "FOLD: 3, EPOCH: 48, train_loss: 0.016741983516997582\n",
      "FOLD: 3, EPOCH: 48, valid_loss: 0.018945304963451166\n",
      "FOLD: 3, EPOCH: 49, train_loss: 0.016760827671434428\n",
      "FOLD: 3, EPOCH: 49, valid_loss: 0.018952092442374963\n",
      "Epoch    50: reducing learning rate of group 0 to 1.3371e-05.\n",
      "FOLD: 4, EPOCH: 0, train_loss: 0.046967243372991276\n",
      "FOLD: 4, EPOCH: 0, valid_loss: 0.02404937234062415\n",
      "FOLD: 4, EPOCH: 1, train_loss: 0.02218428815438135\n",
      "FOLD: 4, EPOCH: 1, valid_loss: 0.022366829503041048\n",
      "FOLD: 4, EPOCH: 2, train_loss: 0.021504021778300002\n",
      "FOLD: 4, EPOCH: 2, valid_loss: 0.0221235822313107\n",
      "FOLD: 4, EPOCH: 3, train_loss: 0.021158395058198554\n",
      "FOLD: 4, EPOCH: 3, valid_loss: 0.02135399399468532\n",
      "FOLD: 4, EPOCH: 4, train_loss: 0.02079178800655378\n",
      "FOLD: 4, EPOCH: 4, valid_loss: 0.021068357647611544\n",
      "FOLD: 4, EPOCH: 5, train_loss: 0.02059612931633318\n",
      "FOLD: 4, EPOCH: 5, valid_loss: 0.02158911468890997\n",
      "FOLD: 4, EPOCH: 6, train_loss: 0.02044682270167647\n",
      "FOLD: 4, EPOCH: 6, valid_loss: 0.020612447021099236\n",
      "FOLD: 4, EPOCH: 7, train_loss: 0.02027542018205733\n",
      "FOLD: 4, EPOCH: 7, valid_loss: 0.020497694611549377\n",
      "FOLD: 4, EPOCH: 8, train_loss: 0.02009269444120897\n",
      "FOLD: 4, EPOCH: 8, valid_loss: 0.02032696957198473\n",
      "FOLD: 4, EPOCH: 9, train_loss: 0.019965180950994428\n",
      "FOLD: 4, EPOCH: 9, valid_loss: 0.02019863002575361\n",
      "FOLD: 4, EPOCH: 10, train_loss: 0.019814984243665193\n",
      "FOLD: 4, EPOCH: 10, valid_loss: 0.020111210787525542\n",
      "FOLD: 4, EPOCH: 11, train_loss: 0.019645297346082894\n",
      "FOLD: 4, EPOCH: 11, valid_loss: 0.020173943816469267\n",
      "FOLD: 4, EPOCH: 12, train_loss: 0.01950883205879379\n",
      "FOLD: 4, EPOCH: 12, valid_loss: 0.020549593350062005\n",
      "FOLD: 4, EPOCH: 13, train_loss: 0.01943898407389989\n",
      "FOLD: 4, EPOCH: 13, valid_loss: 0.01995256428535168\n",
      "FOLD: 4, EPOCH: 14, train_loss: 0.019301310911573267\n",
      "FOLD: 4, EPOCH: 14, valid_loss: 0.01989753028521171\n",
      "FOLD: 4, EPOCH: 15, train_loss: 0.019140524336615124\n",
      "FOLD: 4, EPOCH: 15, valid_loss: 0.019672756011669453\n",
      "FOLD: 4, EPOCH: 16, train_loss: 0.01907141546945314\n",
      "FOLD: 4, EPOCH: 16, valid_loss: 0.01968686755460042\n",
      "FOLD: 4, EPOCH: 17, train_loss: 0.01891574006829713\n",
      "FOLD: 4, EPOCH: 17, valid_loss: 0.019607453535382565\n",
      "FOLD: 4, EPOCH: 18, train_loss: 0.018832334594146627\n",
      "FOLD: 4, EPOCH: 18, valid_loss: 0.019546799791546967\n",
      "FOLD: 4, EPOCH: 19, train_loss: 0.01872215694370302\n",
      "FOLD: 4, EPOCH: 19, valid_loss: 0.01954401986530194\n",
      "FOLD: 4, EPOCH: 20, train_loss: 0.018647359684109688\n",
      "FOLD: 4, EPOCH: 20, valid_loss: 0.019662392110778734\n",
      "FOLD: 4, EPOCH: 21, train_loss: 0.018498021429656324\n",
      "FOLD: 4, EPOCH: 21, valid_loss: 0.0195153710933832\n",
      "FOLD: 4, EPOCH: 22, train_loss: 0.018415217816426948\n",
      "FOLD: 4, EPOCH: 22, valid_loss: 0.019724919818914853\n",
      "FOLD: 4, EPOCH: 23, train_loss: 0.018322954306731355\n",
      "FOLD: 4, EPOCH: 23, valid_loss: 0.019453458201426726\n",
      "FOLD: 4, EPOCH: 24, train_loss: 0.018192885821131436\n",
      "FOLD: 4, EPOCH: 24, valid_loss: 0.019546046996345885\n",
      "FOLD: 4, EPOCH: 25, train_loss: 0.018097728289462423\n",
      "FOLD: 4, EPOCH: 25, valid_loss: 0.01943261190675772\n",
      "FOLD: 4, EPOCH: 26, train_loss: 0.0179783913080354\n",
      "FOLD: 4, EPOCH: 26, valid_loss: 0.019345107703254774\n",
      "FOLD: 4, EPOCH: 27, train_loss: 0.017862436688832334\n",
      "FOLD: 4, EPOCH: 27, valid_loss: 0.0194086293474986\n",
      "FOLD: 4, EPOCH: 28, train_loss: 0.01777422078255866\n",
      "FOLD: 4, EPOCH: 28, valid_loss: 0.019349289484895192\n",
      "FOLD: 4, EPOCH: 29, train_loss: 0.017655830150721846\n",
      "FOLD: 4, EPOCH: 29, valid_loss: 0.019351567356632307\n",
      "Epoch    30: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 4, EPOCH: 30, train_loss: 0.017325549852102995\n",
      "FOLD: 4, EPOCH: 30, valid_loss: 0.019280204692712195\n",
      "FOLD: 4, EPOCH: 31, train_loss: 0.017190542185326684\n",
      "FOLD: 4, EPOCH: 31, valid_loss: 0.01923591185074586\n",
      "FOLD: 4, EPOCH: 32, train_loss: 0.017175199841526715\n",
      "FOLD: 4, EPOCH: 32, valid_loss: 0.01925039176757519\n",
      "FOLD: 4, EPOCH: 33, train_loss: 0.017094275888961716\n",
      "FOLD: 4, EPOCH: 33, valid_loss: 0.01924447242457133\n",
      "FOLD: 4, EPOCH: 34, train_loss: 0.017059607418707094\n",
      "FOLD: 4, EPOCH: 34, valid_loss: 0.019228763591784697\n",
      "FOLD: 4, EPOCH: 35, train_loss: 0.016999042441917432\n",
      "FOLD: 4, EPOCH: 35, valid_loss: 0.01922235881479887\n",
      "FOLD: 4, EPOCH: 36, train_loss: 0.016954950824681972\n",
      "FOLD: 4, EPOCH: 36, valid_loss: 0.019225185020611837\n",
      "FOLD: 4, EPOCH: 37, train_loss: 0.01689899354115934\n",
      "FOLD: 4, EPOCH: 37, valid_loss: 0.019229462800117638\n",
      "FOLD: 4, EPOCH: 38, train_loss: 0.016874211324328505\n",
      "FOLD: 4, EPOCH: 38, valid_loss: 0.01923409878061368\n",
      "Epoch    39: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 4, EPOCH: 39, train_loss: 0.016814780144675356\n",
      "FOLD: 4, EPOCH: 39, valid_loss: 0.019218607017627128\n",
      "FOLD: 4, EPOCH: 40, train_loss: 0.016828273058038305\n",
      "FOLD: 4, EPOCH: 40, valid_loss: 0.019216624590066764\n",
      "FOLD: 4, EPOCH: 41, train_loss: 0.01680536224582308\n",
      "FOLD: 4, EPOCH: 41, valid_loss: 0.019224577511732396\n",
      "FOLD: 4, EPOCH: 42, train_loss: 0.01685317391781388\n",
      "FOLD: 4, EPOCH: 42, valid_loss: 0.01923083342038668\n",
      "FOLD: 4, EPOCH: 43, train_loss: 0.01680399921747881\n",
      "FOLD: 4, EPOCH: 43, valid_loss: 0.019205463763612967\n",
      "FOLD: 4, EPOCH: 44, train_loss: 0.01681328400013012\n",
      "FOLD: 4, EPOCH: 44, valid_loss: 0.019222950276273947\n",
      "FOLD: 4, EPOCH: 45, train_loss: 0.016883444682871167\n",
      "FOLD: 4, EPOCH: 45, valid_loss: 0.019217145557586964\n",
      "FOLD: 4, EPOCH: 46, train_loss: 0.016823265663776045\n",
      "FOLD: 4, EPOCH: 46, valid_loss: 0.01921548307515108\n",
      "Epoch    47: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 4, EPOCH: 47, train_loss: 0.01681727257783751\n",
      "FOLD: 4, EPOCH: 47, valid_loss: 0.019210013202749766\n",
      "FOLD: 4, EPOCH: 48, train_loss: 0.016798023411349672\n",
      "FOLD: 4, EPOCH: 48, valid_loss: 0.019199966524656\n",
      "FOLD: 4, EPOCH: 49, train_loss: 0.016790189969076497\n",
      "FOLD: 4, EPOCH: 49, valid_loss: 0.019203634932637215\n",
      "FOLD: 5, EPOCH: 0, train_loss: 0.04879130093330467\n",
      "FOLD: 5, EPOCH: 0, valid_loss: 0.022863440931989595\n",
      "FOLD: 5, EPOCH: 1, train_loss: 0.022372020509194682\n",
      "FOLD: 5, EPOCH: 1, valid_loss: 0.021895964415027544\n",
      "FOLD: 5, EPOCH: 2, train_loss: 0.02161806947677522\n",
      "FOLD: 5, EPOCH: 2, valid_loss: 0.02157274504693655\n",
      "FOLD: 5, EPOCH: 3, train_loss: 0.0212968179855395\n",
      "FOLD: 5, EPOCH: 3, valid_loss: 0.021220380297073953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 5, EPOCH: 4, train_loss: 0.020918323280843528\n",
      "FOLD: 5, EPOCH: 4, valid_loss: 0.02068101055920124\n",
      "FOLD: 5, EPOCH: 5, train_loss: 0.02076871763612773\n",
      "FOLD: 5, EPOCH: 5, valid_loss: 0.020616684682094134\n",
      "FOLD: 5, EPOCH: 6, train_loss: 0.020441792200546007\n",
      "FOLD: 5, EPOCH: 6, valid_loss: 0.02041274610047157\n",
      "FOLD: 5, EPOCH: 7, train_loss: 0.02033298505419815\n",
      "FOLD: 5, EPOCH: 7, valid_loss: 0.02030740649654315\n",
      "FOLD: 5, EPOCH: 8, train_loss: 0.020145409438457038\n",
      "FOLD: 5, EPOCH: 8, valid_loss: 0.020005515275093224\n",
      "FOLD: 5, EPOCH: 9, train_loss: 0.01997290669965583\n",
      "FOLD: 5, EPOCH: 9, valid_loss: 0.019961529053174533\n",
      "FOLD: 5, EPOCH: 10, train_loss: 0.01980783714837319\n",
      "FOLD: 5, EPOCH: 10, valid_loss: 0.019935130356596067\n",
      "FOLD: 5, EPOCH: 11, train_loss: 0.01967670939661361\n",
      "FOLD: 5, EPOCH: 11, valid_loss: 0.019631838282713525\n",
      "FOLD: 5, EPOCH: 12, train_loss: 0.019530409821183294\n",
      "FOLD: 5, EPOCH: 12, valid_loss: 0.019667098871790446\n",
      "FOLD: 5, EPOCH: 13, train_loss: 0.019412782974541187\n",
      "FOLD: 5, EPOCH: 13, valid_loss: 0.019575213726896506\n",
      "FOLD: 5, EPOCH: 14, train_loss: 0.01926237072896313\n",
      "FOLD: 5, EPOCH: 14, valid_loss: 0.019499267236544535\n",
      "FOLD: 5, EPOCH: 15, train_loss: 0.019119390991289873\n",
      "FOLD: 5, EPOCH: 15, valid_loss: 0.01941485101213822\n",
      "FOLD: 5, EPOCH: 16, train_loss: 0.019012686796486378\n",
      "FOLD: 5, EPOCH: 16, valid_loss: 0.01940596103668213\n",
      "FOLD: 5, EPOCH: 17, train_loss: 0.018911043534407746\n",
      "FOLD: 5, EPOCH: 17, valid_loss: 0.019367644133476112\n",
      "FOLD: 5, EPOCH: 18, train_loss: 0.018843234365654958\n",
      "FOLD: 5, EPOCH: 18, valid_loss: 0.019247111936028186\n",
      "FOLD: 5, EPOCH: 19, train_loss: 0.018646211464058707\n",
      "FOLD: 5, EPOCH: 19, valid_loss: 0.01924322660152729\n",
      "FOLD: 5, EPOCH: 20, train_loss: 0.018568394907020235\n",
      "FOLD: 5, EPOCH: 20, valid_loss: 0.01912165877337639\n",
      "FOLD: 5, EPOCH: 21, train_loss: 0.018362442811800016\n",
      "FOLD: 5, EPOCH: 21, valid_loss: 0.019150075431053456\n",
      "FOLD: 5, EPOCH: 22, train_loss: 0.018313208972481457\n",
      "FOLD: 5, EPOCH: 22, valid_loss: 0.019217032366074048\n",
      "FOLD: 5, EPOCH: 23, train_loss: 0.018181409485436773\n",
      "FOLD: 5, EPOCH: 23, valid_loss: 0.019110907442294635\n",
      "FOLD: 5, EPOCH: 24, train_loss: 0.01807214815572307\n",
      "FOLD: 5, EPOCH: 24, valid_loss: 0.019178845848028477\n",
      "FOLD: 5, EPOCH: 25, train_loss: 0.01793005020433181\n",
      "FOLD: 5, EPOCH: 25, valid_loss: 0.019135832213438474\n",
      "FOLD: 5, EPOCH: 26, train_loss: 0.017810491354179545\n",
      "FOLD: 5, EPOCH: 26, valid_loss: 0.019038720342975397\n",
      "FOLD: 5, EPOCH: 27, train_loss: 0.017703408551578585\n",
      "FOLD: 5, EPOCH: 27, valid_loss: 0.019055943219707563\n",
      "FOLD: 5, EPOCH: 28, train_loss: 0.017551610120446294\n",
      "FOLD: 5, EPOCH: 28, valid_loss: 0.019049637592755832\n",
      "FOLD: 5, EPOCH: 29, train_loss: 0.017458522465784807\n",
      "FOLD: 5, EPOCH: 29, valid_loss: 0.019054312258958817\n",
      "Epoch    30: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 5, EPOCH: 30, train_loss: 0.01707701903541346\n",
      "FOLD: 5, EPOCH: 30, valid_loss: 0.01897631800518586\n",
      "FOLD: 5, EPOCH: 31, train_loss: 0.0170106835587806\n",
      "FOLD: 5, EPOCH: 31, valid_loss: 0.018932750878425744\n",
      "FOLD: 5, EPOCH: 32, train_loss: 0.016917890403419733\n",
      "FOLD: 5, EPOCH: 32, valid_loss: 0.018917002356969394\n",
      "FOLD: 5, EPOCH: 33, train_loss: 0.016806766566996638\n",
      "FOLD: 5, EPOCH: 33, valid_loss: 0.018920166274675958\n",
      "FOLD: 5, EPOCH: 34, train_loss: 0.016794325154576753\n",
      "FOLD: 5, EPOCH: 34, valid_loss: 0.01889696201452842\n",
      "FOLD: 5, EPOCH: 35, train_loss: 0.016707280436782417\n",
      "FOLD: 5, EPOCH: 35, valid_loss: 0.01891791132780222\n",
      "FOLD: 5, EPOCH: 36, train_loss: 0.016648940641332318\n",
      "FOLD: 5, EPOCH: 36, valid_loss: 0.01891188509762287\n",
      "FOLD: 5, EPOCH: 37, train_loss: 0.01664171289853953\n",
      "FOLD: 5, EPOCH: 37, valid_loss: 0.018900839038766347\n",
      "Epoch    38: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 5, EPOCH: 38, train_loss: 0.016525523246240778\n",
      "FOLD: 5, EPOCH: 38, valid_loss: 0.0189051847331799\n",
      "FOLD: 5, EPOCH: 39, train_loss: 0.01654777823116731\n",
      "FOLD: 5, EPOCH: 39, valid_loss: 0.018893818872479293\n",
      "FOLD: 5, EPOCH: 40, train_loss: 0.016571661545517476\n",
      "FOLD: 5, EPOCH: 40, valid_loss: 0.018893711555462617\n",
      "FOLD: 5, EPOCH: 41, train_loss: 0.016520345923364967\n",
      "FOLD: 5, EPOCH: 41, valid_loss: 0.018901940291890733\n",
      "FOLD: 5, EPOCH: 42, train_loss: 0.01658826575589341\n",
      "FOLD: 5, EPOCH: 42, valid_loss: 0.018880992841262084\n",
      "FOLD: 5, EPOCH: 43, train_loss: 0.016510421762595307\n",
      "FOLD: 5, EPOCH: 43, valid_loss: 0.018896232430751506\n",
      "FOLD: 5, EPOCH: 44, train_loss: 0.0165232421729613\n",
      "FOLD: 5, EPOCH: 44, valid_loss: 0.018902897978058227\n",
      "FOLD: 5, EPOCH: 45, train_loss: 0.01651111602581836\n",
      "FOLD: 5, EPOCH: 45, valid_loss: 0.01889252934891444\n",
      "Epoch    46: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 5, EPOCH: 46, train_loss: 0.016484041786375078\n",
      "FOLD: 5, EPOCH: 46, valid_loss: 0.018890291452407837\n",
      "FOLD: 5, EPOCH: 47, train_loss: 0.016514846526489064\n",
      "FOLD: 5, EPOCH: 47, valid_loss: 0.018868333015304346\n",
      "FOLD: 5, EPOCH: 48, train_loss: 0.016499711200594902\n",
      "FOLD: 5, EPOCH: 48, valid_loss: 0.01890432103895224\n",
      "FOLD: 5, EPOCH: 49, train_loss: 0.016530183710258554\n",
      "FOLD: 5, EPOCH: 49, valid_loss: 0.01890338584780693\n",
      "FOLD: 6, EPOCH: 0, train_loss: 0.0475467170613843\n",
      "FOLD: 6, EPOCH: 0, valid_loss: 0.022304042170827206\n",
      "FOLD: 6, EPOCH: 1, train_loss: 0.02233942794437344\n",
      "FOLD: 6, EPOCH: 1, valid_loss: 0.021799708931491926\n",
      "FOLD: 6, EPOCH: 2, train_loss: 0.021664095086020393\n",
      "FOLD: 6, EPOCH: 2, valid_loss: 0.021344638357941922\n",
      "FOLD: 6, EPOCH: 3, train_loss: 0.021232717569816758\n",
      "FOLD: 6, EPOCH: 3, valid_loss: 0.02083175156552058\n",
      "FOLD: 6, EPOCH: 4, train_loss: 0.020949303424237547\n",
      "FOLD: 6, EPOCH: 4, valid_loss: 0.020737041647617634\n",
      "FOLD: 6, EPOCH: 5, train_loss: 0.02066378839112617\n",
      "FOLD: 6, EPOCH: 5, valid_loss: 0.02060797824882544\n",
      "FOLD: 6, EPOCH: 6, train_loss: 0.020460507077341143\n",
      "FOLD: 6, EPOCH: 6, valid_loss: 0.020184697010196172\n",
      "FOLD: 6, EPOCH: 7, train_loss: 0.020302774359446926\n",
      "FOLD: 6, EPOCH: 7, valid_loss: 0.019943097319740515\n",
      "FOLD: 6, EPOCH: 8, train_loss: 0.020123234622784564\n",
      "FOLD: 6, EPOCH: 8, valid_loss: 0.019841140852524683\n",
      "FOLD: 6, EPOCH: 9, train_loss: 0.019948873316516746\n",
      "FOLD: 6, EPOCH: 9, valid_loss: 0.019979576078745034\n",
      "FOLD: 6, EPOCH: 10, train_loss: 0.01977241728958246\n",
      "FOLD: 6, EPOCH: 10, valid_loss: 0.019649289261836272\n",
      "FOLD: 6, EPOCH: 11, train_loss: 0.01966553762857173\n",
      "FOLD: 6, EPOCH: 11, valid_loss: 0.019550024460141476\n",
      "FOLD: 6, EPOCH: 12, train_loss: 0.019522569998091942\n",
      "FOLD: 6, EPOCH: 12, valid_loss: 0.019460382799689587\n",
      "FOLD: 6, EPOCH: 13, train_loss: 0.019376691531490634\n",
      "FOLD: 6, EPOCH: 13, valid_loss: 0.019455567002296448\n",
      "FOLD: 6, EPOCH: 14, train_loss: 0.019260147687148402\n",
      "FOLD: 6, EPOCH: 14, valid_loss: 0.019386239206561677\n",
      "FOLD: 6, EPOCH: 15, train_loss: 0.019105569962915535\n",
      "FOLD: 6, EPOCH: 15, valid_loss: 0.019263617264536712\n",
      "FOLD: 6, EPOCH: 16, train_loss: 0.019000880269182695\n",
      "FOLD: 6, EPOCH: 16, valid_loss: 0.019424917892767832\n",
      "FOLD: 6, EPOCH: 17, train_loss: 0.018861390801297652\n",
      "FOLD: 6, EPOCH: 17, valid_loss: 0.019309901704008762\n",
      "FOLD: 6, EPOCH: 18, train_loss: 0.018765150116303482\n",
      "FOLD: 6, EPOCH: 18, valid_loss: 0.01925719500734256\n",
      "FOLD: 6, EPOCH: 19, train_loss: 0.01862499362008797\n",
      "FOLD: 6, EPOCH: 19, valid_loss: 0.01916983136190818\n",
      "FOLD: 6, EPOCH: 20, train_loss: 0.018512713370492328\n",
      "FOLD: 6, EPOCH: 20, valid_loss: 0.019249321319735967\n",
      "FOLD: 6, EPOCH: 21, train_loss: 0.018375361287916028\n",
      "FOLD: 6, EPOCH: 21, valid_loss: 0.019093431962224152\n",
      "FOLD: 6, EPOCH: 22, train_loss: 0.018291230464505183\n",
      "FOLD: 6, EPOCH: 22, valid_loss: 0.019089168224197168\n",
      "FOLD: 6, EPOCH: 23, train_loss: 0.018130590887488546\n",
      "FOLD: 6, EPOCH: 23, valid_loss: 0.019095698084968787\n",
      "FOLD: 6, EPOCH: 24, train_loss: 0.018023194921379153\n",
      "FOLD: 6, EPOCH: 24, valid_loss: 0.01926428036620984\n",
      "FOLD: 6, EPOCH: 25, train_loss: 0.01787978252144279\n",
      "FOLD: 6, EPOCH: 25, valid_loss: 0.019081868088016145\n",
      "FOLD: 6, EPOCH: 26, train_loss: 0.01774939403844041\n",
      "FOLD: 6, EPOCH: 26, valid_loss: 0.019118997769860122\n",
      "FOLD: 6, EPOCH: 27, train_loss: 0.017613902448903065\n",
      "FOLD: 6, EPOCH: 27, valid_loss: 0.01903188257263257\n",
      "FOLD: 6, EPOCH: 28, train_loss: 0.017480101511889213\n",
      "FOLD: 6, EPOCH: 28, valid_loss: 0.019004580206595935\n",
      "FOLD: 6, EPOCH: 29, train_loss: 0.017396222724503762\n",
      "FOLD: 6, EPOCH: 29, valid_loss: 0.0189764670167978\n",
      "FOLD: 6, EPOCH: 30, train_loss: 0.017265283552980102\n",
      "FOLD: 6, EPOCH: 30, valid_loss: 0.019084441403930005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 6, EPOCH: 31, train_loss: 0.017143455132640695\n",
      "FOLD: 6, EPOCH: 31, valid_loss: 0.018986377578515273\n",
      "FOLD: 6, EPOCH: 32, train_loss: 0.01704011858768157\n",
      "FOLD: 6, EPOCH: 32, valid_loss: 0.01900093902188998\n",
      "Epoch    33: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 6, EPOCH: 33, train_loss: 0.016643259661725245\n",
      "FOLD: 6, EPOCH: 33, valid_loss: 0.018917482203015916\n",
      "FOLD: 6, EPOCH: 34, train_loss: 0.016527983246723544\n",
      "FOLD: 6, EPOCH: 34, valid_loss: 0.018889211404782075\n",
      "FOLD: 6, EPOCH: 35, train_loss: 0.01642210387649971\n",
      "FOLD: 6, EPOCH: 35, valid_loss: 0.018898876957022227\n",
      "FOLD: 6, EPOCH: 36, train_loss: 0.016398533297753013\n",
      "FOLD: 6, EPOCH: 36, valid_loss: 0.018880289907638844\n",
      "FOLD: 6, EPOCH: 37, train_loss: 0.016322233814846824\n",
      "FOLD: 6, EPOCH: 37, valid_loss: 0.018870628367249783\n",
      "FOLD: 6, EPOCH: 38, train_loss: 0.016300997382180916\n",
      "FOLD: 6, EPOCH: 38, valid_loss: 0.018876720363130935\n",
      "FOLD: 6, EPOCH: 39, train_loss: 0.016272891992451372\n",
      "FOLD: 6, EPOCH: 39, valid_loss: 0.018878257905061428\n",
      "FOLD: 6, EPOCH: 40, train_loss: 0.0162354803996513\n",
      "FOLD: 6, EPOCH: 40, valid_loss: 0.0188826101903732\n",
      "Epoch    41: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 6, EPOCH: 41, train_loss: 0.016134809891416413\n",
      "FOLD: 6, EPOCH: 41, valid_loss: 0.01890208887366148\n",
      "FOLD: 6, EPOCH: 42, train_loss: 0.0161499760012973\n",
      "FOLD: 6, EPOCH: 42, valid_loss: 0.01887604594230652\n",
      "FOLD: 6, EPOCH: 43, train_loss: 0.016122571570244996\n",
      "FOLD: 6, EPOCH: 43, valid_loss: 0.018882702606228683\n",
      "Epoch    44: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 6, EPOCH: 44, train_loss: 0.016155973920044867\n",
      "FOLD: 6, EPOCH: 44, valid_loss: 0.018876842151467618\n",
      "FOLD: 6, EPOCH: 45, train_loss: 0.016131405202621543\n",
      "FOLD: 6, EPOCH: 45, valid_loss: 0.0188983385092937\n",
      "FOLD: 6, EPOCH: 46, train_loss: 0.016136156206296104\n",
      "FOLD: 6, EPOCH: 46, valid_loss: 0.01889143970150214\n",
      "Epoch    47: reducing learning rate of group 0 to 8.9345e-05.\n",
      "FOLD: 6, EPOCH: 47, train_loss: 0.016120795563266083\n",
      "FOLD: 6, EPOCH: 47, valid_loss: 0.018886862465968497\n",
      "FOLD: 6, EPOCH: 48, train_loss: 0.016142589480590983\n",
      "FOLD: 6, EPOCH: 48, valid_loss: 0.018909699498460844\n",
      "FOLD: 6, EPOCH: 49, train_loss: 0.016137818856215156\n",
      "FOLD: 6, EPOCH: 49, valid_loss: 0.018904602584930565\n",
      "Epoch    50: reducing learning rate of group 0 to 1.3371e-05.\n",
      "FOLD: 0, EPOCH: 0, train_loss: 0.04930045078130993\n",
      "FOLD: 0, EPOCH: 0, valid_loss: 0.022772503873476617\n",
      "FOLD: 0, EPOCH: 1, train_loss: 0.022399370709585177\n",
      "FOLD: 0, EPOCH: 1, valid_loss: 0.022679702593730047\n",
      "FOLD: 0, EPOCH: 2, train_loss: 0.021751571723536867\n",
      "FOLD: 0, EPOCH: 2, valid_loss: 0.021129455560675033\n",
      "FOLD: 0, EPOCH: 3, train_loss: 0.021401690520547533\n",
      "FOLD: 0, EPOCH: 3, valid_loss: 0.02086558651465636\n",
      "FOLD: 0, EPOCH: 4, train_loss: 0.020953530017789955\n",
      "FOLD: 0, EPOCH: 4, valid_loss: 0.02041779300914361\n",
      "FOLD: 0, EPOCH: 5, train_loss: 0.02074573838428871\n",
      "FOLD: 0, EPOCH: 5, valid_loss: 0.020431943954183504\n",
      "FOLD: 0, EPOCH: 6, train_loss: 0.020511371586975212\n",
      "FOLD: 0, EPOCH: 6, valid_loss: 0.01998736336827278\n",
      "FOLD: 0, EPOCH: 7, train_loss: 0.020290903017126227\n",
      "FOLD: 0, EPOCH: 7, valid_loss: 0.020240732540304843\n",
      "FOLD: 0, EPOCH: 8, train_loss: 0.02017817845114985\n",
      "FOLD: 0, EPOCH: 8, valid_loss: 0.01984191556962637\n",
      "FOLD: 0, EPOCH: 9, train_loss: 0.01992866595753947\n",
      "FOLD: 0, EPOCH: 9, valid_loss: 0.019743951467367318\n",
      "FOLD: 0, EPOCH: 10, train_loss: 0.01977970831196856\n",
      "FOLD: 0, EPOCH: 10, valid_loss: 0.01961715189883342\n",
      "FOLD: 0, EPOCH: 11, train_loss: 0.01965653818302058\n",
      "FOLD: 0, EPOCH: 11, valid_loss: 0.019513738843110893\n",
      "FOLD: 0, EPOCH: 12, train_loss: 0.01952081815515821\n",
      "FOLD: 0, EPOCH: 12, valid_loss: 0.019469289682232417\n",
      "FOLD: 0, EPOCH: 13, train_loss: 0.019413533922586892\n",
      "FOLD: 0, EPOCH: 13, valid_loss: 0.019384318962693214\n",
      "FOLD: 0, EPOCH: 14, train_loss: 0.01927104456400549\n",
      "FOLD: 0, EPOCH: 14, valid_loss: 0.019351335099110238\n",
      "FOLD: 0, EPOCH: 15, train_loss: 0.01909578002586558\n",
      "FOLD: 0, EPOCH: 15, valid_loss: 0.019786618220118377\n",
      "FOLD: 0, EPOCH: 16, train_loss: 0.01902889961225761\n",
      "FOLD: 0, EPOCH: 16, valid_loss: 0.019212655579814546\n",
      "FOLD: 0, EPOCH: 17, train_loss: 0.018829799836149085\n",
      "FOLD: 0, EPOCH: 17, valid_loss: 0.01924750624367824\n",
      "FOLD: 0, EPOCH: 18, train_loss: 0.01875203920880685\n",
      "FOLD: 0, EPOCH: 18, valid_loss: 0.019096563928402387\n",
      "FOLD: 0, EPOCH: 19, train_loss: 0.018600041600497993\n",
      "FOLD: 0, EPOCH: 19, valid_loss: 0.01912932928938132\n",
      "FOLD: 0, EPOCH: 20, train_loss: 0.01848261569299408\n",
      "FOLD: 0, EPOCH: 20, valid_loss: 0.019075457006692886\n",
      "FOLD: 0, EPOCH: 21, train_loss: 0.018358646162055636\n",
      "FOLD: 0, EPOCH: 21, valid_loss: 0.01909754081414296\n",
      "FOLD: 0, EPOCH: 22, train_loss: 0.01819215707380224\n",
      "FOLD: 0, EPOCH: 22, valid_loss: 0.01949428294140559\n",
      "FOLD: 0, EPOCH: 23, train_loss: 0.018111884140887775\n",
      "FOLD: 0, EPOCH: 23, valid_loss: 0.019042190737449206\n",
      "FOLD: 0, EPOCH: 24, train_loss: 0.01801070578497004\n",
      "FOLD: 0, EPOCH: 24, valid_loss: 0.019029855728149414\n",
      "FOLD: 0, EPOCH: 25, train_loss: 0.017854308256426372\n",
      "FOLD: 0, EPOCH: 25, valid_loss: 0.019079083720078834\n",
      "FOLD: 0, EPOCH: 26, train_loss: 0.01772297886074395\n",
      "FOLD: 0, EPOCH: 26, valid_loss: 0.01899786451115058\n",
      "FOLD: 0, EPOCH: 27, train_loss: 0.01763967573139313\n",
      "FOLD: 0, EPOCH: 27, valid_loss: 0.018910957786899347\n",
      "FOLD: 0, EPOCH: 28, train_loss: 0.017507076099817012\n",
      "FOLD: 0, EPOCH: 28, valid_loss: 0.018964409111784056\n",
      "FOLD: 0, EPOCH: 29, train_loss: 0.017344082453967753\n",
      "FOLD: 0, EPOCH: 29, valid_loss: 0.018923971228874646\n",
      "FOLD: 0, EPOCH: 30, train_loss: 0.0172446809104971\n",
      "FOLD: 0, EPOCH: 30, valid_loss: 0.01895369403064251\n",
      "Epoch    31: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 0, EPOCH: 31, train_loss: 0.016872828624941206\n",
      "FOLD: 0, EPOCH: 31, valid_loss: 0.01883375085890293\n",
      "FOLD: 0, EPOCH: 32, train_loss: 0.016713093513169804\n",
      "FOLD: 0, EPOCH: 32, valid_loss: 0.018819671840621874\n",
      "FOLD: 0, EPOCH: 33, train_loss: 0.016669251339359058\n",
      "FOLD: 0, EPOCH: 33, valid_loss: 0.018825646346578233\n",
      "FOLD: 0, EPOCH: 34, train_loss: 0.016574630325005668\n",
      "FOLD: 0, EPOCH: 34, valid_loss: 0.018816327676177025\n",
      "FOLD: 0, EPOCH: 35, train_loss: 0.016494279349776538\n",
      "FOLD: 0, EPOCH: 35, valid_loss: 0.018811407427375134\n",
      "FOLD: 0, EPOCH: 36, train_loss: 0.016446554051661812\n",
      "FOLD: 0, EPOCH: 36, valid_loss: 0.01884786755992816\n",
      "FOLD: 0, EPOCH: 37, train_loss: 0.016461747862096573\n",
      "FOLD: 0, EPOCH: 37, valid_loss: 0.018816579276552566\n",
      "FOLD: 0, EPOCH: 38, train_loss: 0.01637204722627192\n",
      "FOLD: 0, EPOCH: 38, valid_loss: 0.018799784664924327\n",
      "FOLD: 0, EPOCH: 39, train_loss: 0.016315926533393765\n",
      "FOLD: 0, EPOCH: 39, valid_loss: 0.018820982712965745\n",
      "FOLD: 0, EPOCH: 40, train_loss: 0.01629305292373976\n",
      "FOLD: 0, EPOCH: 40, valid_loss: 0.018813098995731428\n",
      "FOLD: 0, EPOCH: 41, train_loss: 0.016264969256479998\n",
      "FOLD: 0, EPOCH: 41, valid_loss: 0.018807509197638586\n",
      "Epoch    42: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 0, EPOCH: 42, train_loss: 0.016205682444411354\n",
      "FOLD: 0, EPOCH: 42, valid_loss: 0.018801112874196127\n",
      "FOLD: 0, EPOCH: 43, train_loss: 0.01616899827746926\n",
      "FOLD: 0, EPOCH: 43, valid_loss: 0.018809504663714997\n",
      "FOLD: 0, EPOCH: 44, train_loss: 0.016195836741275883\n",
      "FOLD: 0, EPOCH: 44, valid_loss: 0.01882206605604062\n",
      "Epoch    45: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 0, EPOCH: 45, train_loss: 0.016180255764944328\n",
      "FOLD: 0, EPOCH: 45, valid_loss: 0.018808045496161167\n",
      "FOLD: 0, EPOCH: 46, train_loss: 0.016177575594770746\n",
      "FOLD: 0, EPOCH: 46, valid_loss: 0.01880942628933833\n",
      "FOLD: 0, EPOCH: 47, train_loss: 0.016172700674852002\n",
      "FOLD: 0, EPOCH: 47, valid_loss: 0.018823163583874702\n",
      "Epoch    48: reducing learning rate of group 0 to 8.9345e-05.\n",
      "FOLD: 0, EPOCH: 48, train_loss: 0.016165988129639142\n",
      "FOLD: 0, EPOCH: 48, valid_loss: 0.018829622950691443\n",
      "FOLD: 0, EPOCH: 49, train_loss: 0.01615561976575771\n",
      "FOLD: 0, EPOCH: 49, valid_loss: 0.018826789007737085\n",
      "FOLD: 1, EPOCH: 0, train_loss: 0.05004305354747418\n",
      "FOLD: 1, EPOCH: 0, valid_loss: 0.024006430059671402\n",
      "FOLD: 1, EPOCH: 1, train_loss: 0.022376266912229964\n",
      "FOLD: 1, EPOCH: 1, valid_loss: 0.02220194127697211\n",
      "FOLD: 1, EPOCH: 2, train_loss: 0.021608391952877108\n",
      "FOLD: 1, EPOCH: 2, valid_loss: 0.021448377376565568\n",
      "FOLD: 1, EPOCH: 3, train_loss: 0.021228966227657086\n",
      "FOLD: 1, EPOCH: 3, valid_loss: 0.021582097101670045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 1, EPOCH: 4, train_loss: 0.020829186170689157\n",
      "FOLD: 1, EPOCH: 4, valid_loss: 0.020942061996230714\n",
      "FOLD: 1, EPOCH: 5, train_loss: 0.02072029047318407\n",
      "FOLD: 1, EPOCH: 5, valid_loss: 0.020622925976148017\n",
      "FOLD: 1, EPOCH: 6, train_loss: 0.020446517655776965\n",
      "FOLD: 1, EPOCH: 6, valid_loss: 0.021101596407019176\n",
      "FOLD: 1, EPOCH: 7, train_loss: 0.02026171544315042\n",
      "FOLD: 1, EPOCH: 7, valid_loss: 0.02027131846317878\n",
      "FOLD: 1, EPOCH: 8, train_loss: 0.0200813643483294\n",
      "FOLD: 1, EPOCH: 8, valid_loss: 0.020310632024820034\n",
      "FOLD: 1, EPOCH: 9, train_loss: 0.0198917221529661\n",
      "FOLD: 1, EPOCH: 9, valid_loss: 0.02002073437548601\n",
      "FOLD: 1, EPOCH: 10, train_loss: 0.019723041108935267\n",
      "FOLD: 1, EPOCH: 10, valid_loss: 0.019857927727011535\n",
      "FOLD: 1, EPOCH: 11, train_loss: 0.019587257776308705\n",
      "FOLD: 1, EPOCH: 11, valid_loss: 0.019683491438627243\n",
      "FOLD: 1, EPOCH: 12, train_loss: 0.01937985244030888\n",
      "FOLD: 1, EPOCH: 12, valid_loss: 0.019675361135831244\n",
      "FOLD: 1, EPOCH: 13, train_loss: 0.019361243935654294\n",
      "FOLD: 1, EPOCH: 13, valid_loss: 0.019869188563181803\n",
      "FOLD: 1, EPOCH: 14, train_loss: 0.019166136084980256\n",
      "FOLD: 1, EPOCH: 14, valid_loss: 0.019533128979114387\n",
      "FOLD: 1, EPOCH: 15, train_loss: 0.018987258770377248\n",
      "FOLD: 1, EPOCH: 15, valid_loss: 0.01946068827349406\n",
      "FOLD: 1, EPOCH: 16, train_loss: 0.018867449130158167\n",
      "FOLD: 1, EPOCH: 16, valid_loss: 0.019420600281311914\n",
      "FOLD: 1, EPOCH: 17, train_loss: 0.018803832775636298\n",
      "FOLD: 1, EPOCH: 17, valid_loss: 0.01938455752455271\n",
      "FOLD: 1, EPOCH: 18, train_loss: 0.01860119039947922\n",
      "FOLD: 1, EPOCH: 18, valid_loss: 0.0193158658937766\n",
      "FOLD: 1, EPOCH: 19, train_loss: 0.01850276198741552\n",
      "FOLD: 1, EPOCH: 19, valid_loss: 0.01928684960764188\n",
      "FOLD: 1, EPOCH: 20, train_loss: 0.01839701144175755\n",
      "FOLD: 1, EPOCH: 20, valid_loss: 0.01933054167490739\n",
      "FOLD: 1, EPOCH: 21, train_loss: 0.01826208288705832\n",
      "FOLD: 1, EPOCH: 21, valid_loss: 0.019175988120528367\n",
      "FOLD: 1, EPOCH: 22, train_loss: 0.01817513909190893\n",
      "FOLD: 1, EPOCH: 22, valid_loss: 0.01926999309888253\n",
      "FOLD: 1, EPOCH: 23, train_loss: 0.01799937944255165\n",
      "FOLD: 1, EPOCH: 23, valid_loss: 0.019181217854985826\n",
      "FOLD: 1, EPOCH: 24, train_loss: 0.017949605436139816\n",
      "FOLD: 1, EPOCH: 24, valid_loss: 0.020151852415158197\n",
      "Epoch    25: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 1, EPOCH: 25, train_loss: 0.017587794368532864\n",
      "FOLD: 1, EPOCH: 25, valid_loss: 0.019075262288634594\n",
      "FOLD: 1, EPOCH: 26, train_loss: 0.017389077189806347\n",
      "FOLD: 1, EPOCH: 26, valid_loss: 0.01900610986810464\n",
      "FOLD: 1, EPOCH: 27, train_loss: 0.017338681387136113\n",
      "FOLD: 1, EPOCH: 27, valid_loss: 0.018998012519799747\n",
      "FOLD: 1, EPOCH: 28, train_loss: 0.017289303570381692\n",
      "FOLD: 1, EPOCH: 28, valid_loss: 0.018962944786135968\n",
      "FOLD: 1, EPOCH: 29, train_loss: 0.017213416210300213\n",
      "FOLD: 1, EPOCH: 29, valid_loss: 0.018973813750422917\n",
      "FOLD: 1, EPOCH: 30, train_loss: 0.01713361925873402\n",
      "FOLD: 1, EPOCH: 30, valid_loss: 0.01896072494295927\n",
      "FOLD: 1, EPOCH: 31, train_loss: 0.017138097372308776\n",
      "FOLD: 1, EPOCH: 31, valid_loss: 0.01896217608681092\n",
      "FOLD: 1, EPOCH: 32, train_loss: 0.01707693246017034\n",
      "FOLD: 1, EPOCH: 32, valid_loss: 0.01893043188521495\n",
      "FOLD: 1, EPOCH: 33, train_loss: 0.01703267518078556\n",
      "FOLD: 1, EPOCH: 33, valid_loss: 0.018916935588304814\n",
      "FOLD: 1, EPOCH: 34, train_loss: 0.017001214789579045\n",
      "FOLD: 1, EPOCH: 34, valid_loss: 0.018929788699516885\n",
      "FOLD: 1, EPOCH: 35, train_loss: 0.016938888883167826\n",
      "FOLD: 1, EPOCH: 35, valid_loss: 0.01894933759019925\n",
      "FOLD: 1, EPOCH: 36, train_loss: 0.016911574651965418\n",
      "FOLD: 1, EPOCH: 36, valid_loss: 0.018926889850543097\n",
      "Epoch    37: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 1, EPOCH: 37, train_loss: 0.01686413646549792\n",
      "FOLD: 1, EPOCH: 37, valid_loss: 0.018915139998380955\n",
      "FOLD: 1, EPOCH: 38, train_loss: 0.01688637651150694\n",
      "FOLD: 1, EPOCH: 38, valid_loss: 0.018921537324786186\n",
      "FOLD: 1, EPOCH: 39, train_loss: 0.01686625956633204\n",
      "FOLD: 1, EPOCH: 39, valid_loss: 0.018918236431020957\n",
      "FOLD: 1, EPOCH: 40, train_loss: 0.016811982665613696\n",
      "FOLD: 1, EPOCH: 40, valid_loss: 0.018912420536463078\n",
      "FOLD: 1, EPOCH: 41, train_loss: 0.016814036118621763\n",
      "FOLD: 1, EPOCH: 41, valid_loss: 0.01894551902436293\n",
      "FOLD: 1, EPOCH: 42, train_loss: 0.016848134329995594\n",
      "FOLD: 1, EPOCH: 42, valid_loss: 0.018925790173503067\n",
      "FOLD: 1, EPOCH: 43, train_loss: 0.01682188904315636\n",
      "FOLD: 1, EPOCH: 43, valid_loss: 0.018919569798387013\n",
      "Epoch    44: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 1, EPOCH: 44, train_loss: 0.016852600726525526\n",
      "FOLD: 1, EPOCH: 44, valid_loss: 0.01892584519317517\n",
      "FOLD: 1, EPOCH: 45, train_loss: 0.01683018229448715\n",
      "FOLD: 1, EPOCH: 45, valid_loss: 0.01893175780200041\n",
      "FOLD: 1, EPOCH: 46, train_loss: 0.01684945127045786\n",
      "FOLD: 1, EPOCH: 46, valid_loss: 0.018913836290056888\n",
      "Epoch    47: reducing learning rate of group 0 to 8.9345e-05.\n",
      "FOLD: 1, EPOCH: 47, train_loss: 0.016835790970740287\n",
      "FOLD: 1, EPOCH: 47, valid_loss: 0.018908127999076478\n",
      "FOLD: 1, EPOCH: 48, train_loss: 0.016843732647799158\n",
      "FOLD: 1, EPOCH: 48, valid_loss: 0.01892358995974064\n",
      "FOLD: 1, EPOCH: 49, train_loss: 0.016838690215671383\n",
      "FOLD: 1, EPOCH: 49, valid_loss: 0.018908116966485977\n",
      "FOLD: 2, EPOCH: 0, train_loss: 0.0519148913539342\n",
      "FOLD: 2, EPOCH: 0, valid_loss: 0.023002354284891717\n",
      "FOLD: 2, EPOCH: 1, train_loss: 0.022342460808923114\n",
      "FOLD: 2, EPOCH: 1, valid_loss: 0.022221048434193317\n",
      "FOLD: 2, EPOCH: 2, train_loss: 0.021660041265391016\n",
      "FOLD: 2, EPOCH: 2, valid_loss: 0.021455450126757987\n",
      "FOLD: 2, EPOCH: 3, train_loss: 0.02117996899461424\n",
      "FOLD: 2, EPOCH: 3, valid_loss: 0.021456385891024884\n",
      "FOLD: 2, EPOCH: 4, train_loss: 0.02086257066174939\n",
      "FOLD: 2, EPOCH: 4, valid_loss: 0.02097935673709099\n",
      "FOLD: 2, EPOCH: 5, train_loss: 0.020670327030726382\n",
      "FOLD: 2, EPOCH: 5, valid_loss: 0.020712576663264863\n",
      "FOLD: 2, EPOCH: 6, train_loss: 0.0204398035751404\n",
      "FOLD: 2, EPOCH: 6, valid_loss: 0.020610405705296077\n",
      "FOLD: 2, EPOCH: 7, train_loss: 0.020256365297009814\n",
      "FOLD: 2, EPOCH: 7, valid_loss: 0.02041143049987463\n",
      "FOLD: 2, EPOCH: 8, train_loss: 0.020076642391850818\n",
      "FOLD: 2, EPOCH: 8, valid_loss: 0.02047563731097258\n",
      "FOLD: 2, EPOCH: 9, train_loss: 0.020006206843095856\n",
      "FOLD: 2, EPOCH: 9, valid_loss: 0.020090351311060097\n",
      "FOLD: 2, EPOCH: 10, train_loss: 0.019777295582399174\n",
      "FOLD: 2, EPOCH: 10, valid_loss: 0.02023411298600527\n",
      "FOLD: 2, EPOCH: 11, train_loss: 0.01964327715639327\n",
      "FOLD: 2, EPOCH: 11, valid_loss: 0.01981397417302315\n",
      "FOLD: 2, EPOCH: 12, train_loss: 0.01950644550694002\n",
      "FOLD: 2, EPOCH: 12, valid_loss: 0.01987972282446348\n",
      "FOLD: 2, EPOCH: 13, train_loss: 0.019390987509207147\n",
      "FOLD: 2, EPOCH: 13, valid_loss: 0.019748443021224096\n",
      "FOLD: 2, EPOCH: 14, train_loss: 0.0192683523369802\n",
      "FOLD: 2, EPOCH: 14, valid_loss: 0.019747504964470863\n",
      "FOLD: 2, EPOCH: 15, train_loss: 0.019122851962173306\n",
      "FOLD: 2, EPOCH: 15, valid_loss: 0.019703853875398636\n",
      "FOLD: 2, EPOCH: 16, train_loss: 0.018948449551857805\n",
      "FOLD: 2, EPOCH: 16, valid_loss: 0.019643379661899347\n",
      "FOLD: 2, EPOCH: 17, train_loss: 0.018875258144091914\n",
      "FOLD: 2, EPOCH: 17, valid_loss: 0.01965289333691964\n",
      "FOLD: 2, EPOCH: 18, train_loss: 0.018765262177063\n",
      "FOLD: 2, EPOCH: 18, valid_loss: 0.01957863397323168\n",
      "FOLD: 2, EPOCH: 19, train_loss: 0.018628268754361448\n",
      "FOLD: 2, EPOCH: 19, valid_loss: 0.019504243937822487\n",
      "FOLD: 2, EPOCH: 20, train_loss: 0.018525981268769986\n",
      "FOLD: 2, EPOCH: 20, valid_loss: 0.019567001181153152\n",
      "FOLD: 2, EPOCH: 21, train_loss: 0.01840404632526475\n",
      "FOLD: 2, EPOCH: 21, valid_loss: 0.019540805656176347\n",
      "FOLD: 2, EPOCH: 22, train_loss: 0.018294146385144542\n",
      "FOLD: 2, EPOCH: 22, valid_loss: 0.019452302931593016\n",
      "FOLD: 2, EPOCH: 23, train_loss: 0.018176912783166847\n",
      "FOLD: 2, EPOCH: 23, valid_loss: 0.01936401054263115\n",
      "FOLD: 2, EPOCH: 24, train_loss: 0.01799667669409836\n",
      "FOLD: 2, EPOCH: 24, valid_loss: 0.019612453304804288\n",
      "FOLD: 2, EPOCH: 25, train_loss: 0.017947501704298163\n",
      "FOLD: 2, EPOCH: 25, valid_loss: 0.01940719339136894\n",
      "FOLD: 2, EPOCH: 26, train_loss: 0.017808403995995585\n",
      "FOLD: 2, EPOCH: 26, valid_loss: 0.019346350230849706\n",
      "FOLD: 2, EPOCH: 27, train_loss: 0.017701456574974832\n",
      "FOLD: 2, EPOCH: 27, valid_loss: 0.019386585658559434\n",
      "FOLD: 2, EPOCH: 28, train_loss: 0.017569556555433852\n",
      "FOLD: 2, EPOCH: 28, valid_loss: 0.01938397867175249\n",
      "FOLD: 2, EPOCH: 29, train_loss: 0.01744588349971014\n",
      "FOLD: 2, EPOCH: 29, valid_loss: 0.019278082853326432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 2, EPOCH: 30, train_loss: 0.01737299030394973\n",
      "FOLD: 2, EPOCH: 30, valid_loss: 0.019346454109136876\n",
      "FOLD: 2, EPOCH: 31, train_loss: 0.017243675207970915\n",
      "FOLD: 2, EPOCH: 31, valid_loss: 0.019319826593765847\n",
      "FOLD: 2, EPOCH: 32, train_loss: 0.017115836220516545\n",
      "FOLD: 2, EPOCH: 32, valid_loss: 0.019298100700745217\n",
      "Epoch    33: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 2, EPOCH: 33, train_loss: 0.016785540013901284\n",
      "FOLD: 2, EPOCH: 33, valid_loss: 0.019204720711478822\n",
      "FOLD: 2, EPOCH: 34, train_loss: 0.01665512390585767\n",
      "FOLD: 2, EPOCH: 34, valid_loss: 0.01916576334490226\n",
      "FOLD: 2, EPOCH: 35, train_loss: 0.01659038516919355\n",
      "FOLD: 2, EPOCH: 35, valid_loss: 0.019175881806474466\n",
      "FOLD: 2, EPOCH: 36, train_loss: 0.016484110414780474\n",
      "FOLD: 2, EPOCH: 36, valid_loss: 0.019186590010156997\n",
      "FOLD: 2, EPOCH: 37, train_loss: 0.016475427102901646\n",
      "FOLD: 2, EPOCH: 37, valid_loss: 0.019180158009895913\n",
      "Epoch    38: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 2, EPOCH: 38, train_loss: 0.0163877928956739\n",
      "FOLD: 2, EPOCH: 38, valid_loss: 0.01915316957120712\n",
      "FOLD: 2, EPOCH: 39, train_loss: 0.016398012889800844\n",
      "FOLD: 2, EPOCH: 39, valid_loss: 0.01915505198905101\n",
      "FOLD: 2, EPOCH: 40, train_loss: 0.01640601914275337\n",
      "FOLD: 2, EPOCH: 40, valid_loss: 0.019159576640679285\n",
      "FOLD: 2, EPOCH: 41, train_loss: 0.01633767317980528\n",
      "FOLD: 2, EPOCH: 41, valid_loss: 0.019187485655912988\n",
      "Epoch    42: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 2, EPOCH: 42, train_loss: 0.016389711055199842\n",
      "FOLD: 2, EPOCH: 42, valid_loss: 0.019179827748582914\n",
      "FOLD: 2, EPOCH: 43, train_loss: 0.01638714869733195\n",
      "FOLD: 2, EPOCH: 43, valid_loss: 0.019159156255997144\n",
      "FOLD: 2, EPOCH: 44, train_loss: 0.01636835715003513\n",
      "FOLD: 2, EPOCH: 44, valid_loss: 0.01917362929536746\n",
      "Epoch    45: reducing learning rate of group 0 to 8.9345e-05.\n",
      "FOLD: 2, EPOCH: 45, train_loss: 0.01640266425454536\n",
      "FOLD: 2, EPOCH: 45, valid_loss: 0.019143187942413185\n",
      "FOLD: 2, EPOCH: 46, train_loss: 0.01634501534942034\n",
      "FOLD: 2, EPOCH: 46, valid_loss: 0.019153699278831482\n",
      "FOLD: 2, EPOCH: 47, train_loss: 0.01638684343747996\n",
      "FOLD: 2, EPOCH: 47, valid_loss: 0.019160912443812076\n",
      "FOLD: 2, EPOCH: 48, train_loss: 0.016387576816251147\n",
      "FOLD: 2, EPOCH: 48, valid_loss: 0.019162974678553067\n",
      "Epoch    49: reducing learning rate of group 0 to 1.3371e-05.\n",
      "FOLD: 2, EPOCH: 49, train_loss: 0.016335729459250294\n",
      "FOLD: 2, EPOCH: 49, valid_loss: 0.019181429909972045\n",
      "FOLD: 3, EPOCH: 0, train_loss: 0.047343790203937\n",
      "FOLD: 3, EPOCH: 0, valid_loss: 0.02282450419779007\n",
      "FOLD: 3, EPOCH: 1, train_loss: 0.022271902224904782\n",
      "FOLD: 3, EPOCH: 1, valid_loss: 0.02159987332729193\n",
      "FOLD: 3, EPOCH: 2, train_loss: 0.02160505927797105\n",
      "FOLD: 3, EPOCH: 2, valid_loss: 0.021175001103144426\n",
      "FOLD: 3, EPOCH: 3, train_loss: 0.021189140835525217\n",
      "FOLD: 3, EPOCH: 3, valid_loss: 0.020951654762029648\n",
      "FOLD: 3, EPOCH: 4, train_loss: 0.020889375962921092\n",
      "FOLD: 3, EPOCH: 4, valid_loss: 0.020486729506116647\n",
      "FOLD: 3, EPOCH: 5, train_loss: 0.020687972573009698\n",
      "FOLD: 3, EPOCH: 5, valid_loss: 0.020752269344834182\n",
      "FOLD: 3, EPOCH: 6, train_loss: 0.020468201538598216\n",
      "FOLD: 3, EPOCH: 6, valid_loss: 0.020102413084644537\n",
      "FOLD: 3, EPOCH: 7, train_loss: 0.020260783868866997\n",
      "FOLD: 3, EPOCH: 7, valid_loss: 0.019975438427466612\n",
      "FOLD: 3, EPOCH: 8, train_loss: 0.020071039580412814\n",
      "FOLD: 3, EPOCH: 8, valid_loss: 0.020045821053477433\n",
      "FOLD: 3, EPOCH: 9, train_loss: 0.019902020390774752\n",
      "FOLD: 3, EPOCH: 9, valid_loss: 0.01992971607698844\n",
      "FOLD: 3, EPOCH: 10, train_loss: 0.019805336270380666\n",
      "FOLD: 3, EPOCH: 10, valid_loss: 0.01975450822367118\n",
      "FOLD: 3, EPOCH: 11, train_loss: 0.019658160823825245\n",
      "FOLD: 3, EPOCH: 11, valid_loss: 0.019648655532644346\n",
      "FOLD: 3, EPOCH: 12, train_loss: 0.019567393219551525\n",
      "FOLD: 3, EPOCH: 12, valid_loss: 0.01956788622415983\n",
      "FOLD: 3, EPOCH: 13, train_loss: 0.019392449685649293\n",
      "FOLD: 3, EPOCH: 13, valid_loss: 0.019599080372315187\n",
      "FOLD: 3, EPOCH: 14, train_loss: 0.019302858392129075\n",
      "FOLD: 3, EPOCH: 14, valid_loss: 0.019430636070095576\n",
      "FOLD: 3, EPOCH: 15, train_loss: 0.019200623236797953\n",
      "FOLD: 3, EPOCH: 15, valid_loss: 0.020873303023668435\n",
      "FOLD: 3, EPOCH: 16, train_loss: 0.019026975373964052\n",
      "FOLD: 3, EPOCH: 16, valid_loss: 0.019407006697012827\n",
      "FOLD: 3, EPOCH: 17, train_loss: 0.018976753451735585\n",
      "FOLD: 3, EPOCH: 17, valid_loss: 0.019320329651236534\n",
      "FOLD: 3, EPOCH: 18, train_loss: 0.01883966679609305\n",
      "FOLD: 3, EPOCH: 18, valid_loss: 0.01930931654687111\n",
      "FOLD: 3, EPOCH: 19, train_loss: 0.01868987229426165\n",
      "FOLD: 3, EPOCH: 19, valid_loss: 0.019247517419549134\n",
      "FOLD: 3, EPOCH: 20, train_loss: 0.018567822672225332\n",
      "FOLD: 3, EPOCH: 20, valid_loss: 0.019219410677368823\n",
      "FOLD: 3, EPOCH: 21, train_loss: 0.018482871630505937\n",
      "FOLD: 3, EPOCH: 21, valid_loss: 0.019146403584342737\n",
      "FOLD: 3, EPOCH: 22, train_loss: 0.01838429126183729\n",
      "FOLD: 3, EPOCH: 22, valid_loss: 0.019256674899504736\n",
      "FOLD: 3, EPOCH: 23, train_loss: 0.01825622529596896\n",
      "FOLD: 3, EPOCH: 23, valid_loss: 0.019165333933555163\n",
      "FOLD: 3, EPOCH: 24, train_loss: 0.01812119094805943\n",
      "FOLD: 3, EPOCH: 24, valid_loss: 0.019132204210528962\n",
      "FOLD: 3, EPOCH: 25, train_loss: 0.01804934676132492\n",
      "FOLD: 3, EPOCH: 25, valid_loss: 0.019080237557108585\n",
      "FOLD: 3, EPOCH: 26, train_loss: 0.01789101739288182\n",
      "FOLD: 3, EPOCH: 26, valid_loss: 0.01912204921245575\n",
      "FOLD: 3, EPOCH: 27, train_loss: 0.017800773921850567\n",
      "FOLD: 3, EPOCH: 27, valid_loss: 0.019090738577338364\n",
      "FOLD: 3, EPOCH: 28, train_loss: 0.017674389647672307\n",
      "FOLD: 3, EPOCH: 28, valid_loss: 0.01913227140903473\n",
      "Epoch    29: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 3, EPOCH: 29, train_loss: 0.0173654639841737\n",
      "FOLD: 3, EPOCH: 29, valid_loss: 0.01900604381584204\n",
      "FOLD: 3, EPOCH: 30, train_loss: 0.017236838268267142\n",
      "FOLD: 3, EPOCH: 30, valid_loss: 0.018979684378092106\n",
      "FOLD: 3, EPOCH: 31, train_loss: 0.01722378496433029\n",
      "FOLD: 3, EPOCH: 31, valid_loss: 0.018981232952613097\n",
      "FOLD: 3, EPOCH: 32, train_loss: 0.01709171585940026\n",
      "FOLD: 3, EPOCH: 32, valid_loss: 0.018941422207997397\n",
      "FOLD: 3, EPOCH: 33, train_loss: 0.017016473616397864\n",
      "FOLD: 3, EPOCH: 33, valid_loss: 0.018946119799063757\n",
      "FOLD: 3, EPOCH: 34, train_loss: 0.017000603716115694\n",
      "FOLD: 3, EPOCH: 34, valid_loss: 0.01893904246389866\n",
      "FOLD: 3, EPOCH: 35, train_loss: 0.016917551313904492\n",
      "FOLD: 3, EPOCH: 35, valid_loss: 0.018931915553716514\n",
      "FOLD: 3, EPOCH: 36, train_loss: 0.01694649665591282\n",
      "FOLD: 3, EPOCH: 36, valid_loss: 0.018938107416033745\n",
      "FOLD: 3, EPOCH: 37, train_loss: 0.016915572970803525\n",
      "FOLD: 3, EPOCH: 37, valid_loss: 0.01891572128694791\n",
      "FOLD: 3, EPOCH: 38, train_loss: 0.01686935099093495\n",
      "FOLD: 3, EPOCH: 38, valid_loss: 0.018934961694937486\n",
      "FOLD: 3, EPOCH: 39, train_loss: 0.016829191505707598\n",
      "FOLD: 3, EPOCH: 39, valid_loss: 0.01892580335529951\n",
      "FOLD: 3, EPOCH: 40, train_loss: 0.01678927504533046\n",
      "FOLD: 3, EPOCH: 40, valid_loss: 0.018912528426601335\n",
      "FOLD: 3, EPOCH: 41, train_loss: 0.016786906968902896\n",
      "FOLD: 3, EPOCH: 41, valid_loss: 0.018944484539903127\n",
      "FOLD: 3, EPOCH: 42, train_loss: 0.016749278466041025\n",
      "FOLD: 3, EPOCH: 42, valid_loss: 0.01893321439050711\n",
      "FOLD: 3, EPOCH: 43, train_loss: 0.01675190737571668\n",
      "FOLD: 3, EPOCH: 43, valid_loss: 0.01890610344707966\n",
      "FOLD: 3, EPOCH: 44, train_loss: 0.016726982754630013\n",
      "FOLD: 3, EPOCH: 44, valid_loss: 0.018927668292935077\n",
      "FOLD: 3, EPOCH: 45, train_loss: 0.016613038860865542\n",
      "FOLD: 3, EPOCH: 45, valid_loss: 0.018897428822058897\n",
      "FOLD: 3, EPOCH: 46, train_loss: 0.016673039569443947\n",
      "FOLD: 3, EPOCH: 46, valid_loss: 0.018895251533159844\n",
      "FOLD: 3, EPOCH: 47, train_loss: 0.016587101313210017\n",
      "FOLD: 3, EPOCH: 47, valid_loss: 0.018898334067601424\n",
      "FOLD: 3, EPOCH: 48, train_loss: 0.016546706304054807\n",
      "FOLD: 3, EPOCH: 48, valid_loss: 0.018905015805592902\n",
      "FOLD: 3, EPOCH: 49, train_loss: 0.01655595447566058\n",
      "FOLD: 3, EPOCH: 49, valid_loss: 0.01889324216888501\n",
      "FOLD: 4, EPOCH: 0, train_loss: 0.04671475198119879\n",
      "FOLD: 4, EPOCH: 0, valid_loss: 0.023196596652269363\n",
      "FOLD: 4, EPOCH: 1, train_loss: 0.02207570130357871\n",
      "FOLD: 4, EPOCH: 1, valid_loss: 0.02190679096831725\n",
      "FOLD: 4, EPOCH: 2, train_loss: 0.021425111747875408\n",
      "FOLD: 4, EPOCH: 2, valid_loss: 0.021572416934829492\n",
      "FOLD: 4, EPOCH: 3, train_loss: 0.021088196669478674\n",
      "FOLD: 4, EPOCH: 3, valid_loss: 0.02126262953075079\n",
      "FOLD: 4, EPOCH: 4, train_loss: 0.020777526764652214\n",
      "FOLD: 4, EPOCH: 4, valid_loss: 0.021074451219577055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 4, EPOCH: 5, train_loss: 0.02062348535636792\n",
      "FOLD: 4, EPOCH: 5, valid_loss: 0.020746873405117255\n",
      "FOLD: 4, EPOCH: 6, train_loss: 0.02035991130145015\n",
      "FOLD: 4, EPOCH: 6, valid_loss: 0.020515491612828694\n",
      "FOLD: 4, EPOCH: 7, train_loss: 0.020249392937969516\n",
      "FOLD: 4, EPOCH: 7, valid_loss: 0.020489928384239856\n",
      "FOLD: 4, EPOCH: 8, train_loss: 0.020033554350202147\n",
      "FOLD: 4, EPOCH: 8, valid_loss: 0.020254490610498648\n",
      "FOLD: 4, EPOCH: 9, train_loss: 0.019948310545972875\n",
      "FOLD: 4, EPOCH: 9, valid_loss: 0.02019489212678029\n",
      "FOLD: 4, EPOCH: 10, train_loss: 0.01974604480169915\n",
      "FOLD: 4, EPOCH: 10, valid_loss: 0.020178231625602797\n",
      "FOLD: 4, EPOCH: 11, train_loss: 0.0196411648412814\n",
      "FOLD: 4, EPOCH: 11, valid_loss: 0.019981969864322588\n",
      "FOLD: 4, EPOCH: 12, train_loss: 0.019490557647234685\n",
      "FOLD: 4, EPOCH: 12, valid_loss: 0.019850651661937054\n",
      "FOLD: 4, EPOCH: 13, train_loss: 0.01941955864832208\n",
      "FOLD: 4, EPOCH: 13, valid_loss: 0.01990026221252405\n",
      "FOLD: 4, EPOCH: 14, train_loss: 0.01928970290700326\n",
      "FOLD: 4, EPOCH: 14, valid_loss: 0.019810020923614502\n",
      "FOLD: 4, EPOCH: 15, train_loss: 0.019196328178450867\n",
      "FOLD: 4, EPOCH: 15, valid_loss: 0.020070350514008448\n",
      "FOLD: 4, EPOCH: 16, train_loss: 0.01900139400685156\n",
      "FOLD: 4, EPOCH: 16, valid_loss: 0.019702775260576837\n",
      "FOLD: 4, EPOCH: 17, train_loss: 0.018935597476524277\n",
      "FOLD: 4, EPOCH: 17, valid_loss: 0.019628321035550192\n",
      "FOLD: 4, EPOCH: 18, train_loss: 0.018801385763327818\n",
      "FOLD: 4, EPOCH: 18, valid_loss: 0.01962272063470804\n",
      "FOLD: 4, EPOCH: 19, train_loss: 0.018699368713675318\n",
      "FOLD: 4, EPOCH: 19, valid_loss: 0.0195512195619253\n",
      "FOLD: 4, EPOCH: 20, train_loss: 0.018621607733940757\n",
      "FOLD: 4, EPOCH: 20, valid_loss: 0.019613454404931802\n",
      "FOLD: 4, EPOCH: 21, train_loss: 0.01856231749863238\n",
      "FOLD: 4, EPOCH: 21, valid_loss: 0.019509503331321936\n",
      "FOLD: 4, EPOCH: 22, train_loss: 0.01843384522441271\n",
      "FOLD: 4, EPOCH: 22, valid_loss: 0.019473657871668156\n",
      "FOLD: 4, EPOCH: 23, train_loss: 0.01829985393261587\n",
      "FOLD: 4, EPOCH: 23, valid_loss: 0.019474677025125578\n",
      "FOLD: 4, EPOCH: 24, train_loss: 0.018199517869868793\n",
      "FOLD: 4, EPOCH: 24, valid_loss: 0.019466818095399782\n",
      "FOLD: 4, EPOCH: 25, train_loss: 0.018098970824802243\n",
      "FOLD: 4, EPOCH: 25, valid_loss: 0.019415223541168068\n",
      "FOLD: 4, EPOCH: 26, train_loss: 0.017988265340996755\n",
      "FOLD: 4, EPOCH: 26, valid_loss: 0.019404098821374085\n",
      "FOLD: 4, EPOCH: 27, train_loss: 0.01781407929956913\n",
      "FOLD: 4, EPOCH: 27, valid_loss: 0.019387329140534766\n",
      "FOLD: 4, EPOCH: 28, train_loss: 0.017756739782320487\n",
      "FOLD: 4, EPOCH: 28, valid_loss: 0.01941857787852104\n",
      "FOLD: 4, EPOCH: 29, train_loss: 0.017627474977760703\n",
      "FOLD: 4, EPOCH: 29, valid_loss: 0.019401997614365358\n",
      "FOLD: 4, EPOCH: 30, train_loss: 0.017517554800252657\n",
      "FOLD: 4, EPOCH: 30, valid_loss: 0.019418806697313603\n",
      "Epoch    31: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 4, EPOCH: 31, train_loss: 0.017271803654226904\n",
      "FOLD: 4, EPOCH: 31, valid_loss: 0.019322252044310935\n",
      "FOLD: 4, EPOCH: 32, train_loss: 0.017104904670771713\n",
      "FOLD: 4, EPOCH: 32, valid_loss: 0.01926213030058604\n",
      "FOLD: 4, EPOCH: 33, train_loss: 0.017051985776807006\n",
      "FOLD: 4, EPOCH: 33, valid_loss: 0.019236803198089965\n",
      "FOLD: 4, EPOCH: 34, train_loss: 0.01699177862925304\n",
      "FOLD: 4, EPOCH: 34, valid_loss: 0.01921804607487642\n",
      "FOLD: 4, EPOCH: 35, train_loss: 0.016957922989653575\n",
      "FOLD: 4, EPOCH: 35, valid_loss: 0.01923371722491888\n",
      "FOLD: 4, EPOCH: 36, train_loss: 0.016923961317720445\n",
      "FOLD: 4, EPOCH: 36, valid_loss: 0.01923774455029231\n",
      "FOLD: 4, EPOCH: 37, train_loss: 0.01687128168555933\n",
      "FOLD: 4, EPOCH: 37, valid_loss: 0.01923239703934926\n",
      "Epoch    38: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 4, EPOCH: 38, train_loss: 0.016809909113657637\n",
      "FOLD: 4, EPOCH: 38, valid_loss: 0.019238845373575505\n",
      "FOLD: 4, EPOCH: 39, train_loss: 0.016833715354772034\n",
      "FOLD: 4, EPOCH: 39, valid_loss: 0.019234658147280034\n",
      "FOLD: 4, EPOCH: 40, train_loss: 0.016836762768090575\n",
      "FOLD: 4, EPOCH: 40, valid_loss: 0.019215883114016973\n",
      "FOLD: 4, EPOCH: 41, train_loss: 0.016808023575592686\n",
      "FOLD: 4, EPOCH: 41, valid_loss: 0.019239957372729596\n",
      "FOLD: 4, EPOCH: 42, train_loss: 0.016802277252380107\n",
      "FOLD: 4, EPOCH: 42, valid_loss: 0.019238483877136156\n",
      "FOLD: 4, EPOCH: 43, train_loss: 0.016818238560714432\n",
      "FOLD: 4, EPOCH: 43, valid_loss: 0.019219588201779585\n",
      "Epoch    44: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 4, EPOCH: 44, train_loss: 0.01680606183268734\n",
      "FOLD: 4, EPOCH: 44, valid_loss: 0.01922409121806805\n",
      "FOLD: 4, EPOCH: 45, train_loss: 0.016772609858496768\n",
      "FOLD: 4, EPOCH: 45, valid_loss: 0.01921840341618428\n",
      "FOLD: 4, EPOCH: 46, train_loss: 0.01679569364500207\n",
      "FOLD: 4, EPOCH: 46, valid_loss: 0.019219078983251866\n",
      "Epoch    47: reducing learning rate of group 0 to 8.9345e-05.\n",
      "FOLD: 4, EPOCH: 47, train_loss: 0.01679766144150415\n",
      "FOLD: 4, EPOCH: 47, valid_loss: 0.01922524433869582\n",
      "FOLD: 4, EPOCH: 48, train_loss: 0.016813405210504662\n",
      "FOLD: 4, EPOCH: 48, valid_loss: 0.01920745922968938\n",
      "FOLD: 4, EPOCH: 49, train_loss: 0.016841678943082288\n",
      "FOLD: 4, EPOCH: 49, valid_loss: 0.019221217586443975\n",
      "FOLD: 5, EPOCH: 0, train_loss: 0.04912884206183859\n",
      "FOLD: 5, EPOCH: 0, valid_loss: 0.022793270360964995\n",
      "FOLD: 5, EPOCH: 1, train_loss: 0.022491995647952363\n",
      "FOLD: 5, EPOCH: 1, valid_loss: 0.02200770378112793\n",
      "FOLD: 5, EPOCH: 2, train_loss: 0.021730471075185248\n",
      "FOLD: 5, EPOCH: 2, valid_loss: 0.021568533606254138\n",
      "FOLD: 5, EPOCH: 3, train_loss: 0.02130205853766686\n",
      "FOLD: 5, EPOCH: 3, valid_loss: 0.02119839735902273\n",
      "FOLD: 5, EPOCH: 4, train_loss: 0.02103696597387662\n",
      "FOLD: 5, EPOCH: 4, valid_loss: 0.02092216149545633\n",
      "FOLD: 5, EPOCH: 5, train_loss: 0.020684395715392923\n",
      "FOLD: 5, EPOCH: 5, valid_loss: 0.020604697987437248\n",
      "FOLD: 5, EPOCH: 6, train_loss: 0.020497147673489275\n",
      "FOLD: 5, EPOCH: 6, valid_loss: 0.020565424114465714\n",
      "FOLD: 5, EPOCH: 7, train_loss: 0.020323775532479223\n",
      "FOLD: 5, EPOCH: 7, valid_loss: 0.020008179144217417\n",
      "FOLD: 5, EPOCH: 8, train_loss: 0.020196157557940162\n",
      "FOLD: 5, EPOCH: 8, valid_loss: 0.020071707665920258\n",
      "FOLD: 5, EPOCH: 9, train_loss: 0.019981646331379544\n",
      "FOLD: 5, EPOCH: 9, valid_loss: 0.019933408842636988\n",
      "FOLD: 5, EPOCH: 10, train_loss: 0.01978582886324541\n",
      "FOLD: 5, EPOCH: 10, valid_loss: 0.019728697979679473\n",
      "FOLD: 5, EPOCH: 11, train_loss: 0.019699928401088394\n",
      "FOLD: 5, EPOCH: 11, valid_loss: 0.019777166298948802\n",
      "FOLD: 5, EPOCH: 12, train_loss: 0.01951533037464361\n",
      "FOLD: 5, EPOCH: 12, valid_loss: 0.019686057017399713\n",
      "FOLD: 5, EPOCH: 13, train_loss: 0.019415394856116257\n",
      "FOLD: 5, EPOCH: 13, valid_loss: 0.019805185210246306\n",
      "FOLD: 5, EPOCH: 14, train_loss: 0.019279530624280106\n",
      "FOLD: 5, EPOCH: 14, valid_loss: 0.019546880601690367\n",
      "FOLD: 5, EPOCH: 15, train_loss: 0.01917141046677087\n",
      "FOLD: 5, EPOCH: 15, valid_loss: 0.019454606880362216\n",
      "FOLD: 5, EPOCH: 16, train_loss: 0.018996914672489103\n",
      "FOLD: 5, EPOCH: 16, valid_loss: 0.019516813926971875\n",
      "FOLD: 5, EPOCH: 17, train_loss: 0.018896422223062127\n",
      "FOLD: 5, EPOCH: 17, valid_loss: 0.01927240866308029\n",
      "FOLD: 5, EPOCH: 18, train_loss: 0.018724295159650815\n",
      "FOLD: 5, EPOCH: 18, valid_loss: 0.019392080318469267\n",
      "FOLD: 5, EPOCH: 19, train_loss: 0.018674541894044425\n",
      "FOLD: 5, EPOCH: 19, valid_loss: 0.019266498633302175\n",
      "FOLD: 5, EPOCH: 20, train_loss: 0.018516414698112656\n",
      "FOLD: 5, EPOCH: 20, valid_loss: 0.019240331621124193\n",
      "FOLD: 5, EPOCH: 21, train_loss: 0.018415867401337303\n",
      "FOLD: 5, EPOCH: 21, valid_loss: 0.019214903362668477\n",
      "FOLD: 5, EPOCH: 22, train_loss: 0.018300366462082475\n",
      "FOLD: 5, EPOCH: 22, valid_loss: 0.019764495726961356\n",
      "FOLD: 5, EPOCH: 23, train_loss: 0.018132546362844674\n",
      "FOLD: 5, EPOCH: 23, valid_loss: 0.01913647955426803\n",
      "FOLD: 5, EPOCH: 24, train_loss: 0.01803628007906514\n",
      "FOLD: 5, EPOCH: 24, valid_loss: 0.019298161308352765\n",
      "FOLD: 5, EPOCH: 25, train_loss: 0.0178766086999629\n",
      "FOLD: 5, EPOCH: 25, valid_loss: 0.019152064306231644\n",
      "FOLD: 5, EPOCH: 26, train_loss: 0.017773873525092732\n",
      "FOLD: 5, EPOCH: 26, valid_loss: 0.019132424145936966\n",
      "FOLD: 5, EPOCH: 27, train_loss: 0.01765537531291311\n",
      "FOLD: 5, EPOCH: 27, valid_loss: 0.019157986944684617\n",
      "FOLD: 5, EPOCH: 28, train_loss: 0.017538356091323738\n",
      "FOLD: 5, EPOCH: 28, valid_loss: 0.019111608799833518\n",
      "FOLD: 5, EPOCH: 29, train_loss: 0.01742789340583054\n",
      "FOLD: 5, EPOCH: 29, valid_loss: 0.01908038083750468\n",
      "FOLD: 5, EPOCH: 30, train_loss: 0.01730847025189448\n",
      "FOLD: 5, EPOCH: 30, valid_loss: 0.019081926689698145\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 5, EPOCH: 31, train_loss: 0.01715001219732536\n",
      "FOLD: 5, EPOCH: 31, valid_loss: 0.019061085839684192\n",
      "FOLD: 5, EPOCH: 32, train_loss: 0.017072088286482\n",
      "FOLD: 5, EPOCH: 32, valid_loss: 0.019106663620242707\n",
      "FOLD: 5, EPOCH: 33, train_loss: 0.01688041174280885\n",
      "FOLD: 5, EPOCH: 33, valid_loss: 0.01905687253635663\n",
      "FOLD: 5, EPOCH: 34, train_loss: 0.01679219165816903\n",
      "FOLD: 5, EPOCH: 34, valid_loss: 0.019074383549965344\n",
      "FOLD: 5, EPOCH: 35, train_loss: 0.01666163291933166\n",
      "FOLD: 5, EPOCH: 35, valid_loss: 0.019142385572195053\n",
      "FOLD: 5, EPOCH: 36, train_loss: 0.016495619711743015\n",
      "FOLD: 5, EPOCH: 36, valid_loss: 0.01909096180819548\n",
      "Epoch    37: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 5, EPOCH: 37, train_loss: 0.016183237092116394\n",
      "FOLD: 5, EPOCH: 37, valid_loss: 0.019006657055937327\n",
      "FOLD: 5, EPOCH: 38, train_loss: 0.01608941607126916\n",
      "FOLD: 5, EPOCH: 38, valid_loss: 0.01900071593431326\n",
      "FOLD: 5, EPOCH: 39, train_loss: 0.016024757428346453\n",
      "FOLD: 5, EPOCH: 39, valid_loss: 0.01897880477974048\n",
      "FOLD: 5, EPOCH: 40, train_loss: 0.01594882538995227\n",
      "FOLD: 5, EPOCH: 40, valid_loss: 0.01897375428905854\n",
      "FOLD: 5, EPOCH: 41, train_loss: 0.015905694985711896\n",
      "FOLD: 5, EPOCH: 41, valid_loss: 0.018945537220973235\n",
      "FOLD: 5, EPOCH: 42, train_loss: 0.015856311983756116\n",
      "FOLD: 5, EPOCH: 42, valid_loss: 0.018964630479996022\n",
      "FOLD: 5, EPOCH: 43, train_loss: 0.015817916498997726\n",
      "FOLD: 5, EPOCH: 43, valid_loss: 0.018973150935310584\n",
      "FOLD: 5, EPOCH: 44, train_loss: 0.01576091641107121\n",
      "FOLD: 5, EPOCH: 44, valid_loss: 0.018989584480340663\n",
      "Epoch    45: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 5, EPOCH: 45, train_loss: 0.015742177521256177\n",
      "FOLD: 5, EPOCH: 45, valid_loss: 0.018977807978024848\n",
      "FOLD: 5, EPOCH: 46, train_loss: 0.015675086704258982\n",
      "FOLD: 5, EPOCH: 46, valid_loss: 0.018980834633111954\n",
      "FOLD: 5, EPOCH: 47, train_loss: 0.015673401413133014\n",
      "FOLD: 5, EPOCH: 47, valid_loss: 0.018983396629874524\n",
      "Epoch    48: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 5, EPOCH: 48, train_loss: 0.015681003623113438\n",
      "FOLD: 5, EPOCH: 48, valid_loss: 0.01897069253027439\n",
      "FOLD: 5, EPOCH: 49, train_loss: 0.015681391695161927\n",
      "FOLD: 5, EPOCH: 49, valid_loss: 0.01899555167899682\n",
      "FOLD: 6, EPOCH: 0, train_loss: 0.04953219467220274\n",
      "FOLD: 6, EPOCH: 0, valid_loss: 0.02252082096842619\n",
      "FOLD: 6, EPOCH: 1, train_loss: 0.02237554805705676\n",
      "FOLD: 6, EPOCH: 1, valid_loss: 0.025882689139017694\n",
      "FOLD: 6, EPOCH: 2, train_loss: 0.02176534540548518\n",
      "FOLD: 6, EPOCH: 2, valid_loss: 0.021437172276469376\n",
      "FOLD: 6, EPOCH: 3, train_loss: 0.021354251943931386\n",
      "FOLD: 6, EPOCH: 3, valid_loss: 0.020736992932282962\n",
      "FOLD: 6, EPOCH: 4, train_loss: 0.020970202806229528\n",
      "FOLD: 6, EPOCH: 4, valid_loss: 0.020782769442750856\n",
      "FOLD: 6, EPOCH: 5, train_loss: 0.020676458027918596\n",
      "FOLD: 6, EPOCH: 5, valid_loss: 0.020421988259141263\n",
      "FOLD: 6, EPOCH: 6, train_loss: 0.020499692676035134\n",
      "FOLD: 6, EPOCH: 6, valid_loss: 0.020134102266568404\n",
      "FOLD: 6, EPOCH: 7, train_loss: 0.02030687278287636\n",
      "FOLD: 6, EPOCH: 7, valid_loss: 0.019984365942386482\n",
      "FOLD: 6, EPOCH: 8, train_loss: 0.020144778454827296\n",
      "FOLD: 6, EPOCH: 8, valid_loss: 0.0199371689500717\n",
      "FOLD: 6, EPOCH: 9, train_loss: 0.019896086783626594\n",
      "FOLD: 6, EPOCH: 9, valid_loss: 0.019737863196776465\n",
      "FOLD: 6, EPOCH: 10, train_loss: 0.019806762326609443\n",
      "FOLD: 6, EPOCH: 10, valid_loss: 0.019578528089018967\n",
      "FOLD: 6, EPOCH: 11, train_loss: 0.019642082798118528\n",
      "FOLD: 6, EPOCH: 11, valid_loss: 0.019784813890090354\n",
      "FOLD: 6, EPOCH: 12, train_loss: 0.01952317256379772\n",
      "FOLD: 6, EPOCH: 12, valid_loss: 0.019497043238236353\n",
      "FOLD: 6, EPOCH: 13, train_loss: 0.019397940713207464\n",
      "FOLD: 6, EPOCH: 13, valid_loss: 0.02020472288131714\n",
      "FOLD: 6, EPOCH: 14, train_loss: 0.019250933357791322\n",
      "FOLD: 6, EPOCH: 14, valid_loss: 0.019461245061113283\n",
      "FOLD: 6, EPOCH: 15, train_loss: 0.01910701089513463\n",
      "FOLD: 6, EPOCH: 15, valid_loss: 0.01978115837734479\n",
      "FOLD: 6, EPOCH: 16, train_loss: 0.019020901237790648\n",
      "FOLD: 6, EPOCH: 16, valid_loss: 0.019331296189473227\n",
      "FOLD: 6, EPOCH: 17, train_loss: 0.018830740471949447\n",
      "FOLD: 6, EPOCH: 17, valid_loss: 0.019236720382021025\n",
      "FOLD: 6, EPOCH: 18, train_loss: 0.018728124833590275\n",
      "FOLD: 6, EPOCH: 18, valid_loss: 0.019141759150303327\n",
      "FOLD: 6, EPOCH: 19, train_loss: 0.018594736711600342\n",
      "FOLD: 6, EPOCH: 19, valid_loss: 0.019165372046140525\n",
      "FOLD: 6, EPOCH: 20, train_loss: 0.018471971909339364\n",
      "FOLD: 6, EPOCH: 20, valid_loss: 0.019152053273641147\n",
      "FOLD: 6, EPOCH: 21, train_loss: 0.018321320565568434\n",
      "FOLD: 6, EPOCH: 21, valid_loss: 0.019139086827635765\n",
      "FOLD: 6, EPOCH: 22, train_loss: 0.018234773916569916\n",
      "FOLD: 6, EPOCH: 22, valid_loss: 0.019068521805680715\n",
      "FOLD: 6, EPOCH: 23, train_loss: 0.018091418478335883\n",
      "FOLD: 6, EPOCH: 23, valid_loss: 0.019050604162307885\n",
      "FOLD: 6, EPOCH: 24, train_loss: 0.01801729154445835\n",
      "FOLD: 6, EPOCH: 24, valid_loss: 0.019065958089553393\n",
      "FOLD: 6, EPOCH: 25, train_loss: 0.017845897708792944\n",
      "FOLD: 6, EPOCH: 25, valid_loss: 0.019082834514287803\n",
      "FOLD: 6, EPOCH: 26, train_loss: 0.017726809012930136\n",
      "FOLD: 6, EPOCH: 26, valid_loss: 0.01902250573039055\n",
      "FOLD: 6, EPOCH: 27, train_loss: 0.017586888846110652\n",
      "FOLD: 6, EPOCH: 27, valid_loss: 0.01896996022416995\n",
      "FOLD: 6, EPOCH: 28, train_loss: 0.017488535015365562\n",
      "FOLD: 6, EPOCH: 28, valid_loss: 0.019063591527251098\n",
      "FOLD: 6, EPOCH: 29, train_loss: 0.01740087863259219\n",
      "FOLD: 6, EPOCH: 29, valid_loss: 0.019006660351386435\n",
      "FOLD: 6, EPOCH: 30, train_loss: 0.017199750259719992\n",
      "FOLD: 6, EPOCH: 30, valid_loss: 0.01905462761911062\n",
      "Epoch    31: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 6, EPOCH: 31, train_loss: 0.016838678876189766\n",
      "FOLD: 6, EPOCH: 31, valid_loss: 0.018918708826486882\n",
      "FOLD: 6, EPOCH: 32, train_loss: 0.01667520245285453\n",
      "FOLD: 6, EPOCH: 32, valid_loss: 0.018915527142011203\n",
      "FOLD: 6, EPOCH: 33, train_loss: 0.016628409974981804\n",
      "FOLD: 6, EPOCH: 33, valid_loss: 0.01889215553036103\n",
      "FOLD: 6, EPOCH: 34, train_loss: 0.01654738515011362\n",
      "FOLD: 6, EPOCH: 34, valid_loss: 0.018896901980042458\n",
      "FOLD: 6, EPOCH: 35, train_loss: 0.016510098681759997\n",
      "FOLD: 6, EPOCH: 35, valid_loss: 0.018869069763101064\n",
      "FOLD: 6, EPOCH: 36, train_loss: 0.016451746766530984\n",
      "FOLD: 6, EPOCH: 36, valid_loss: 0.0188913531601429\n",
      "FOLD: 6, EPOCH: 37, train_loss: 0.016417497328507738\n",
      "FOLD: 6, EPOCH: 37, valid_loss: 0.01887552124949602\n",
      "FOLD: 6, EPOCH: 38, train_loss: 0.01627643490713593\n",
      "FOLD: 6, EPOCH: 38, valid_loss: 0.01888623360831004\n",
      "Epoch    39: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 6, EPOCH: 39, train_loss: 0.016294584861276922\n",
      "FOLD: 6, EPOCH: 39, valid_loss: 0.01887749909208371\n",
      "FOLD: 6, EPOCH: 40, train_loss: 0.016267891356570495\n",
      "FOLD: 6, EPOCH: 40, valid_loss: 0.018869669248278324\n",
      "FOLD: 6, EPOCH: 41, train_loss: 0.016278762936692785\n",
      "FOLD: 6, EPOCH: 41, valid_loss: 0.01887450023339345\n",
      "Epoch    42: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 6, EPOCH: 42, train_loss: 0.016279728139272413\n",
      "FOLD: 6, EPOCH: 42, valid_loss: 0.0188823607392036\n",
      "FOLD: 6, EPOCH: 43, train_loss: 0.01629912166075932\n",
      "FOLD: 6, EPOCH: 43, valid_loss: 0.01888120948122098\n",
      "FOLD: 6, EPOCH: 44, train_loss: 0.016307479148176877\n",
      "FOLD: 6, EPOCH: 44, valid_loss: 0.01888601095057451\n",
      "Epoch    45: reducing learning rate of group 0 to 8.9345e-05.\n",
      "FOLD: 6, EPOCH: 45, train_loss: 0.016264409519027214\n",
      "FOLD: 6, EPOCH: 45, valid_loss: 0.01888404428385771\n",
      "FOLD: 6, EPOCH: 46, train_loss: 0.016277856784998566\n",
      "FOLD: 6, EPOCH: 46, valid_loss: 0.018894331243175726\n",
      "FOLD: 6, EPOCH: 47, train_loss: 0.016294942614999978\n",
      "FOLD: 6, EPOCH: 47, valid_loss: 0.018884006314552747\n",
      "Epoch    48: reducing learning rate of group 0 to 1.3371e-05.\n",
      "FOLD: 6, EPOCH: 48, train_loss: 0.01625927786871388\n",
      "FOLD: 6, EPOCH: 48, valid_loss: 0.01886985981120513\n",
      "FOLD: 6, EPOCH: 49, train_loss: 0.016251677873770933\n",
      "FOLD: 6, EPOCH: 49, valid_loss: 0.01890653042266002\n",
      "FOLD: 0, EPOCH: 0, train_loss: 0.047387158649193274\n",
      "FOLD: 0, EPOCH: 0, valid_loss: 0.022962280764029577\n",
      "FOLD: 0, EPOCH: 1, train_loss: 0.022368615165956923\n",
      "FOLD: 0, EPOCH: 1, valid_loss: 0.022167592094494745\n",
      "FOLD: 0, EPOCH: 2, train_loss: 0.021667079421112668\n",
      "FOLD: 0, EPOCH: 2, valid_loss: 0.021016550465272024\n",
      "FOLD: 0, EPOCH: 3, train_loss: 0.02125112697280742\n",
      "FOLD: 0, EPOCH: 3, valid_loss: 0.021091724960849836\n",
      "FOLD: 0, EPOCH: 4, train_loss: 0.020925516464017534\n",
      "FOLD: 0, EPOCH: 4, valid_loss: 0.020370169471089657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 0, EPOCH: 5, train_loss: 0.020680150369534623\n",
      "FOLD: 0, EPOCH: 5, valid_loss: 0.020513409318832252\n",
      "FOLD: 0, EPOCH: 6, train_loss: 0.02052354080149451\n",
      "FOLD: 0, EPOCH: 6, valid_loss: 0.02000392686862212\n",
      "FOLD: 0, EPOCH: 7, train_loss: 0.02026638909670952\n",
      "FOLD: 0, EPOCH: 7, valid_loss: 0.020027117803692818\n",
      "FOLD: 0, EPOCH: 8, train_loss: 0.020160728511778084\n",
      "FOLD: 0, EPOCH: 8, valid_loss: 0.019730201850716885\n",
      "FOLD: 0, EPOCH: 9, train_loss: 0.019924115968515742\n",
      "FOLD: 0, EPOCH: 9, valid_loss: 0.019755459318940457\n",
      "FOLD: 0, EPOCH: 10, train_loss: 0.01982580324181834\n",
      "FOLD: 0, EPOCH: 10, valid_loss: 0.0196626832565436\n",
      "FOLD: 0, EPOCH: 11, train_loss: 0.01964705643823018\n",
      "FOLD: 0, EPOCH: 11, valid_loss: 0.01944740775686044\n",
      "FOLD: 0, EPOCH: 12, train_loss: 0.01953865889761899\n",
      "FOLD: 0, EPOCH: 12, valid_loss: 0.019508892526993386\n",
      "FOLD: 0, EPOCH: 13, train_loss: 0.019391140875381394\n",
      "FOLD: 0, EPOCH: 13, valid_loss: 0.01937232008920266\n",
      "FOLD: 0, EPOCH: 14, train_loss: 0.019273534391981526\n",
      "FOLD: 0, EPOCH: 14, valid_loss: 0.019262596391714536\n",
      "FOLD: 0, EPOCH: 15, train_loss: 0.01911076422579385\n",
      "FOLD: 0, EPOCH: 15, valid_loss: 0.019218949457773797\n",
      "FOLD: 0, EPOCH: 16, train_loss: 0.01900457079849533\n",
      "FOLD: 0, EPOCH: 16, valid_loss: 0.019149421929166868\n",
      "FOLD: 0, EPOCH: 17, train_loss: 0.01888303506515316\n",
      "FOLD: 0, EPOCH: 17, valid_loss: 0.019160239025950432\n",
      "FOLD: 0, EPOCH: 18, train_loss: 0.01879316580960074\n",
      "FOLD: 0, EPOCH: 18, valid_loss: 0.019131569048533074\n",
      "FOLD: 0, EPOCH: 19, train_loss: 0.018705842714454676\n",
      "FOLD: 0, EPOCH: 19, valid_loss: 0.019087911798403814\n",
      "FOLD: 0, EPOCH: 20, train_loss: 0.0185579246010732\n",
      "FOLD: 0, EPOCH: 20, valid_loss: 0.019082560705450866\n",
      "FOLD: 0, EPOCH: 21, train_loss: 0.018491773589237315\n",
      "FOLD: 0, EPOCH: 21, valid_loss: 0.01900032678475747\n",
      "FOLD: 0, EPOCH: 22, train_loss: 0.018350290965188195\n",
      "FOLD: 0, EPOCH: 22, valid_loss: 0.019055800369152658\n",
      "FOLD: 0, EPOCH: 23, train_loss: 0.0182614518782577\n",
      "FOLD: 0, EPOCH: 23, valid_loss: 0.018992321995588448\n",
      "FOLD: 0, EPOCH: 24, train_loss: 0.018073900099340325\n",
      "FOLD: 0, EPOCH: 24, valid_loss: 0.018948957037467223\n",
      "FOLD: 0, EPOCH: 25, train_loss: 0.018005704079326744\n",
      "FOLD: 0, EPOCH: 25, valid_loss: 0.01894148109624019\n",
      "FOLD: 0, EPOCH: 26, train_loss: 0.017899207393261226\n",
      "FOLD: 0, EPOCH: 26, valid_loss: 0.018963700160384178\n",
      "FOLD: 0, EPOCH: 27, train_loss: 0.0177739734534879\n",
      "FOLD: 0, EPOCH: 27, valid_loss: 0.018893183567203008\n",
      "FOLD: 0, EPOCH: 28, train_loss: 0.017650560777936433\n",
      "FOLD: 0, EPOCH: 28, valid_loss: 0.018888272631626863\n",
      "FOLD: 0, EPOCH: 29, train_loss: 0.017511533913076728\n",
      "FOLD: 0, EPOCH: 29, valid_loss: 0.018842328769656327\n",
      "FOLD: 0, EPOCH: 30, train_loss: 0.01744182251796529\n",
      "FOLD: 0, EPOCH: 30, valid_loss: 0.01888023058955486\n",
      "FOLD: 0, EPOCH: 31, train_loss: 0.01728593193095278\n",
      "FOLD: 0, EPOCH: 31, valid_loss: 0.018848134348025687\n",
      "FOLD: 0, EPOCH: 32, train_loss: 0.01713972178766051\n",
      "FOLD: 0, EPOCH: 32, valid_loss: 0.018902868319016237\n",
      "Epoch    33: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 0, EPOCH: 33, train_loss: 0.016850048021690267\n",
      "FOLD: 0, EPOCH: 33, valid_loss: 0.018787287462216157\n",
      "FOLD: 0, EPOCH: 34, train_loss: 0.01669986069051398\n",
      "FOLD: 0, EPOCH: 34, valid_loss: 0.01879984584565346\n",
      "FOLD: 0, EPOCH: 35, train_loss: 0.016688134898809163\n",
      "FOLD: 0, EPOCH: 35, valid_loss: 0.01876118592917919\n",
      "FOLD: 0, EPOCH: 36, train_loss: 0.016595421975629555\n",
      "FOLD: 0, EPOCH: 36, valid_loss: 0.01875368018562977\n",
      "FOLD: 0, EPOCH: 37, train_loss: 0.016520714261443227\n",
      "FOLD: 0, EPOCH: 37, valid_loss: 0.018751821122490443\n",
      "FOLD: 0, EPOCH: 38, train_loss: 0.016485428638957644\n",
      "FOLD: 0, EPOCH: 38, valid_loss: 0.01874219482907882\n",
      "FOLD: 0, EPOCH: 39, train_loss: 0.016427939870973695\n",
      "FOLD: 0, EPOCH: 39, valid_loss: 0.01875972719146655\n",
      "FOLD: 0, EPOCH: 40, train_loss: 0.01638402570844502\n",
      "FOLD: 0, EPOCH: 40, valid_loss: 0.01873591527915918\n",
      "FOLD: 0, EPOCH: 41, train_loss: 0.01631368908125001\n",
      "FOLD: 0, EPOCH: 41, valid_loss: 0.018737739238601465\n",
      "FOLD: 0, EPOCH: 42, train_loss: 0.016291027687288618\n",
      "FOLD: 0, EPOCH: 42, valid_loss: 0.018746413863622226\n",
      "FOLD: 0, EPOCH: 43, train_loss: 0.016280014407695144\n",
      "FOLD: 0, EPOCH: 43, valid_loss: 0.018745796468395453\n",
      "Epoch    44: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 0, EPOCH: 44, train_loss: 0.016256754777419405\n",
      "FOLD: 0, EPOCH: 44, valid_loss: 0.018749879100001775\n",
      "FOLD: 0, EPOCH: 45, train_loss: 0.016205555771955767\n",
      "FOLD: 0, EPOCH: 45, valid_loss: 0.018732963416438837\n",
      "FOLD: 0, EPOCH: 46, train_loss: 0.01621006555049806\n",
      "FOLD: 0, EPOCH: 46, valid_loss: 0.018741496050587066\n",
      "FOLD: 0, EPOCH: 47, train_loss: 0.01620209646235044\n",
      "FOLD: 0, EPOCH: 47, valid_loss: 0.01875030192045065\n",
      "FOLD: 0, EPOCH: 48, train_loss: 0.016193739566448574\n",
      "FOLD: 0, EPOCH: 48, valid_loss: 0.018716875320443742\n",
      "FOLD: 0, EPOCH: 49, train_loss: 0.01624192126296662\n",
      "FOLD: 0, EPOCH: 49, valid_loss: 0.018737242342187807\n",
      "FOLD: 1, EPOCH: 0, train_loss: 0.04704192774118604\n",
      "FOLD: 1, EPOCH: 0, valid_loss: 0.02357222942205576\n",
      "FOLD: 1, EPOCH: 1, train_loss: 0.022258555944505577\n",
      "FOLD: 1, EPOCH: 1, valid_loss: 0.022814889939931724\n",
      "FOLD: 1, EPOCH: 2, train_loss: 0.021567779920391134\n",
      "FOLD: 1, EPOCH: 2, valid_loss: 0.021793412761046335\n",
      "FOLD: 1, EPOCH: 3, train_loss: 0.021150628054464184\n",
      "FOLD: 1, EPOCH: 3, valid_loss: 0.020898099415577374\n",
      "FOLD: 1, EPOCH: 4, train_loss: 0.020846546401043196\n",
      "FOLD: 1, EPOCH: 4, valid_loss: 0.021090556079378493\n",
      "FOLD: 1, EPOCH: 5, train_loss: 0.020596804550370655\n",
      "FOLD: 1, EPOCH: 5, valid_loss: 0.020555315969082024\n",
      "FOLD: 1, EPOCH: 6, train_loss: 0.020365397370344884\n",
      "FOLD: 1, EPOCH: 6, valid_loss: 0.020390650257468224\n",
      "FOLD: 1, EPOCH: 7, train_loss: 0.020243281171329924\n",
      "FOLD: 1, EPOCH: 7, valid_loss: 0.02024511004296633\n",
      "FOLD: 1, EPOCH: 8, train_loss: 0.020112346882957058\n",
      "FOLD: 1, EPOCH: 8, valid_loss: 0.020456720143556595\n",
      "FOLD: 1, EPOCH: 9, train_loss: 0.01992845336428365\n",
      "FOLD: 1, EPOCH: 9, valid_loss: 0.020098068966315344\n",
      "FOLD: 1, EPOCH: 10, train_loss: 0.019800510710558376\n",
      "FOLD: 1, EPOCH: 10, valid_loss: 0.019789976426042043\n",
      "FOLD: 1, EPOCH: 11, train_loss: 0.019633584706163085\n",
      "FOLD: 1, EPOCH: 11, valid_loss: 0.019895160714021094\n",
      "FOLD: 1, EPOCH: 12, train_loss: 0.019488261232303607\n",
      "FOLD: 1, EPOCH: 12, valid_loss: 0.019679895100685265\n",
      "FOLD: 1, EPOCH: 13, train_loss: 0.019402017565192404\n",
      "FOLD: 1, EPOCH: 13, valid_loss: 0.01962302324290459\n",
      "FOLD: 1, EPOCH: 14, train_loss: 0.01923733886734054\n",
      "FOLD: 1, EPOCH: 14, valid_loss: 0.019677663365235694\n",
      "FOLD: 1, EPOCH: 15, train_loss: 0.019152735385137634\n",
      "FOLD: 1, EPOCH: 15, valid_loss: 0.019542443207823314\n",
      "FOLD: 1, EPOCH: 16, train_loss: 0.019070089542986574\n",
      "FOLD: 1, EPOCH: 16, valid_loss: 0.019510592262332257\n",
      "FOLD: 1, EPOCH: 17, train_loss: 0.01892833464552422\n",
      "FOLD: 1, EPOCH: 17, valid_loss: 0.01940941925232227\n",
      "FOLD: 1, EPOCH: 18, train_loss: 0.018846817390137428\n",
      "FOLD: 1, EPOCH: 18, valid_loss: 0.01937789971438738\n",
      "FOLD: 1, EPOCH: 19, train_loss: 0.01869228615652065\n",
      "FOLD: 1, EPOCH: 19, valid_loss: 0.019368608983663414\n",
      "FOLD: 1, EPOCH: 20, train_loss: 0.018588011580947285\n",
      "FOLD: 1, EPOCH: 20, valid_loss: 0.019238046012245692\n",
      "FOLD: 1, EPOCH: 21, train_loss: 0.018463349951481498\n",
      "FOLD: 1, EPOCH: 21, valid_loss: 0.019269912002178338\n",
      "FOLD: 1, EPOCH: 22, train_loss: 0.018345590479470587\n",
      "FOLD: 1, EPOCH: 22, valid_loss: 0.01922157550087342\n",
      "FOLD: 1, EPOCH: 23, train_loss: 0.01829049434210803\n",
      "FOLD: 1, EPOCH: 23, valid_loss: 0.019289507172428645\n",
      "FOLD: 1, EPOCH: 24, train_loss: 0.01816679907308237\n",
      "FOLD: 1, EPOCH: 24, valid_loss: 0.019192468804808762\n",
      "FOLD: 1, EPOCH: 25, train_loss: 0.01803712861461414\n",
      "FOLD: 1, EPOCH: 25, valid_loss: 0.019121732419499986\n",
      "FOLD: 1, EPOCH: 26, train_loss: 0.01795494697383932\n",
      "FOLD: 1, EPOCH: 26, valid_loss: 0.019139989064289972\n",
      "FOLD: 1, EPOCH: 27, train_loss: 0.01787824908623824\n",
      "FOLD: 1, EPOCH: 27, valid_loss: 0.019155367349202816\n",
      "FOLD: 1, EPOCH: 28, train_loss: 0.017734879501969426\n",
      "FOLD: 1, EPOCH: 28, valid_loss: 0.019121916534808967\n",
      "Epoch    29: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 1, EPOCH: 29, train_loss: 0.017390882961351325\n",
      "FOLD: 1, EPOCH: 29, valid_loss: 0.019030978330052815\n",
      "FOLD: 1, EPOCH: 30, train_loss: 0.017318917211849947\n",
      "FOLD: 1, EPOCH: 30, valid_loss: 0.018980046161092244\n",
      "FOLD: 1, EPOCH: 31, train_loss: 0.017270343730578553\n",
      "FOLD: 1, EPOCH: 31, valid_loss: 0.01900451644681967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 1, EPOCH: 32, train_loss: 0.017171218588545516\n",
      "FOLD: 1, EPOCH: 32, valid_loss: 0.018966222898318216\n",
      "FOLD: 1, EPOCH: 33, train_loss: 0.01709592863413933\n",
      "FOLD: 1, EPOCH: 33, valid_loss: 0.018962539732456207\n",
      "FOLD: 1, EPOCH: 34, train_loss: 0.017090287487450485\n",
      "FOLD: 1, EPOCH: 34, valid_loss: 0.018969887437728736\n",
      "FOLD: 1, EPOCH: 35, train_loss: 0.017055650040305948\n",
      "FOLD: 1, EPOCH: 35, valid_loss: 0.01897859716644654\n",
      "FOLD: 1, EPOCH: 36, train_loss: 0.017009781849746768\n",
      "FOLD: 1, EPOCH: 36, valid_loss: 0.01894893253651949\n",
      "FOLD: 1, EPOCH: 37, train_loss: 0.01696570312000207\n",
      "FOLD: 1, EPOCH: 37, valid_loss: 0.01895279365663345\n",
      "FOLD: 1, EPOCH: 38, train_loss: 0.016941236281717145\n",
      "FOLD: 1, EPOCH: 38, valid_loss: 0.018958549659985762\n",
      "FOLD: 1, EPOCH: 39, train_loss: 0.01690115311459915\n",
      "FOLD: 1, EPOCH: 39, valid_loss: 0.018948504128135167\n",
      "FOLD: 1, EPOCH: 40, train_loss: 0.016891431894052674\n",
      "FOLD: 1, EPOCH: 40, valid_loss: 0.01893133383530837\n",
      "FOLD: 1, EPOCH: 41, train_loss: 0.0168382910306792\n",
      "FOLD: 1, EPOCH: 41, valid_loss: 0.018929110696682565\n",
      "FOLD: 1, EPOCH: 42, train_loss: 0.016828689195618435\n",
      "FOLD: 1, EPOCH: 42, valid_loss: 0.01893031052671946\n",
      "FOLD: 1, EPOCH: 43, train_loss: 0.016826083392810984\n",
      "FOLD: 1, EPOCH: 43, valid_loss: 0.01894655078649521\n",
      "FOLD: 1, EPOCH: 44, train_loss: 0.016799792584434554\n",
      "FOLD: 1, EPOCH: 44, valid_loss: 0.01893347716675355\n",
      "Epoch    45: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 1, EPOCH: 45, train_loss: 0.01674538489582168\n",
      "FOLD: 1, EPOCH: 45, valid_loss: 0.018936683495457355\n",
      "FOLD: 1, EPOCH: 46, train_loss: 0.016727177325535466\n",
      "FOLD: 1, EPOCH: 46, valid_loss: 0.018926295093618907\n",
      "FOLD: 1, EPOCH: 47, train_loss: 0.016693158201067836\n",
      "FOLD: 1, EPOCH: 47, valid_loss: 0.018929911634096734\n",
      "FOLD: 1, EPOCH: 48, train_loss: 0.016731819528079516\n",
      "FOLD: 1, EPOCH: 48, valid_loss: 0.018931625267634027\n",
      "FOLD: 1, EPOCH: 49, train_loss: 0.016697691187161853\n",
      "FOLD: 1, EPOCH: 49, valid_loss: 0.018937429699760217\n",
      "Epoch    50: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 2, EPOCH: 0, train_loss: 0.04776974266545998\n",
      "FOLD: 2, EPOCH: 0, valid_loss: 0.02320673417013425\n",
      "FOLD: 2, EPOCH: 1, train_loss: 0.0223387346879856\n",
      "FOLD: 2, EPOCH: 1, valid_loss: 0.02395407511637761\n",
      "FOLD: 2, EPOCH: 2, train_loss: 0.021562814360132087\n",
      "FOLD: 2, EPOCH: 2, valid_loss: 0.021824336539094265\n",
      "FOLD: 2, EPOCH: 3, train_loss: 0.021167689481297054\n",
      "FOLD: 2, EPOCH: 3, valid_loss: 0.021146210770194348\n",
      "FOLD: 2, EPOCH: 4, train_loss: 0.020862178524603713\n",
      "FOLD: 2, EPOCH: 4, valid_loss: 0.021009247606763475\n",
      "FOLD: 2, EPOCH: 5, train_loss: 0.020628927955152216\n",
      "FOLD: 2, EPOCH: 5, valid_loss: 0.020638842565508988\n",
      "FOLD: 2, EPOCH: 6, train_loss: 0.020474093336914037\n",
      "FOLD: 2, EPOCH: 6, valid_loss: 0.02053323144522997\n",
      "FOLD: 2, EPOCH: 7, train_loss: 0.02025040576385485\n",
      "FOLD: 2, EPOCH: 7, valid_loss: 0.020458398959957637\n",
      "FOLD: 2, EPOCH: 8, train_loss: 0.02012019370355316\n",
      "FOLD: 2, EPOCH: 8, valid_loss: 0.02026706446821873\n",
      "FOLD: 2, EPOCH: 9, train_loss: 0.01993950625025743\n",
      "FOLD: 2, EPOCH: 9, valid_loss: 0.020517768481603034\n",
      "FOLD: 2, EPOCH: 10, train_loss: 0.019765986230325053\n",
      "FOLD: 2, EPOCH: 10, valid_loss: 0.019975314919765178\n",
      "FOLD: 2, EPOCH: 11, train_loss: 0.019617021461394994\n",
      "FOLD: 2, EPOCH: 11, valid_loss: 0.020040890345206626\n",
      "FOLD: 2, EPOCH: 12, train_loss: 0.01952953838013314\n",
      "FOLD: 2, EPOCH: 12, valid_loss: 0.019788653518144902\n",
      "FOLD: 2, EPOCH: 13, train_loss: 0.01935749573985467\n",
      "FOLD: 2, EPOCH: 13, valid_loss: 0.01989977076076544\n",
      "FOLD: 2, EPOCH: 14, train_loss: 0.019234618902005052\n",
      "FOLD: 2, EPOCH: 14, valid_loss: 0.019783051541218392\n",
      "FOLD: 2, EPOCH: 15, train_loss: 0.019170591456664575\n",
      "FOLD: 2, EPOCH: 15, valid_loss: 0.01973887561605527\n",
      "FOLD: 2, EPOCH: 16, train_loss: 0.019038170072677975\n",
      "FOLD: 2, EPOCH: 16, valid_loss: 0.01969480184981456\n",
      "FOLD: 2, EPOCH: 17, train_loss: 0.018917981510025425\n",
      "FOLD: 2, EPOCH: 17, valid_loss: 0.019607913178893235\n",
      "FOLD: 2, EPOCH: 18, train_loss: 0.018766219350131782\n",
      "FOLD: 2, EPOCH: 18, valid_loss: 0.019554873928427696\n",
      "FOLD: 2, EPOCH: 19, train_loss: 0.018701590471775144\n",
      "FOLD: 2, EPOCH: 19, valid_loss: 0.019470455268254645\n",
      "FOLD: 2, EPOCH: 20, train_loss: 0.018615378243093554\n",
      "FOLD: 2, EPOCH: 20, valid_loss: 0.019453265202733185\n",
      "FOLD: 2, EPOCH: 21, train_loss: 0.01846155559493078\n",
      "FOLD: 2, EPOCH: 21, valid_loss: 0.01943624449463991\n",
      "FOLD: 2, EPOCH: 22, train_loss: 0.01833710280825963\n",
      "FOLD: 2, EPOCH: 22, valid_loss: 0.01944353560415598\n",
      "FOLD: 2, EPOCH: 23, train_loss: 0.01825132062406959\n",
      "FOLD: 2, EPOCH: 23, valid_loss: 0.019392139349992458\n",
      "FOLD: 2, EPOCH: 24, train_loss: 0.0181437682712803\n",
      "FOLD: 2, EPOCH: 24, valid_loss: 0.01929175337919822\n",
      "FOLD: 2, EPOCH: 25, train_loss: 0.018017412339513365\n",
      "FOLD: 2, EPOCH: 25, valid_loss: 0.01934476340046296\n",
      "FOLD: 2, EPOCH: 26, train_loss: 0.017909966459548152\n",
      "FOLD: 2, EPOCH: 26, valid_loss: 0.019358123151155617\n",
      "FOLD: 2, EPOCH: 27, train_loss: 0.01781986573257962\n",
      "FOLD: 2, EPOCH: 27, valid_loss: 0.019357577396126893\n",
      "Epoch    28: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 2, EPOCH: 28, train_loss: 0.017450515533218514\n",
      "FOLD: 2, EPOCH: 28, valid_loss: 0.0192058221078836\n",
      "FOLD: 2, EPOCH: 29, train_loss: 0.017325115969052184\n",
      "FOLD: 2, EPOCH: 29, valid_loss: 0.019163351219434004\n",
      "FOLD: 2, EPOCH: 30, train_loss: 0.017263283298627752\n",
      "FOLD: 2, EPOCH: 30, valid_loss: 0.019176928183207147\n",
      "FOLD: 2, EPOCH: 31, train_loss: 0.017219377417318726\n",
      "FOLD: 2, EPOCH: 31, valid_loss: 0.019171859066073712\n",
      "FOLD: 2, EPOCH: 32, train_loss: 0.017140624692311156\n",
      "FOLD: 2, EPOCH: 32, valid_loss: 0.019170464661258917\n",
      "Epoch    33: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 2, EPOCH: 33, train_loss: 0.017098354049832433\n",
      "FOLD: 2, EPOCH: 33, valid_loss: 0.01912988005922391\n",
      "FOLD: 2, EPOCH: 34, train_loss: 0.017072648917501036\n",
      "FOLD: 2, EPOCH: 34, valid_loss: 0.019145430997014046\n",
      "FOLD: 2, EPOCH: 35, train_loss: 0.017014776218078426\n",
      "FOLD: 2, EPOCH: 35, valid_loss: 0.019144353814996205\n",
      "FOLD: 2, EPOCH: 36, train_loss: 0.017052234163054743\n",
      "FOLD: 2, EPOCH: 36, valid_loss: 0.019137306282153495\n",
      "Epoch    37: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 2, EPOCH: 37, train_loss: 0.017059402313788195\n",
      "FOLD: 2, EPOCH: 37, valid_loss: 0.019150656146498825\n",
      "FOLD: 2, EPOCH: 38, train_loss: 0.017069469784965385\n",
      "FOLD: 2, EPOCH: 38, valid_loss: 0.01917975567854368\n",
      "FOLD: 2, EPOCH: 39, train_loss: 0.01708854958918449\n",
      "FOLD: 2, EPOCH: 39, valid_loss: 0.019136768837387744\n",
      "Epoch    40: reducing learning rate of group 0 to 8.9345e-05.\n",
      "FOLD: 2, EPOCH: 40, train_loss: 0.017055097124161752\n",
      "FOLD: 2, EPOCH: 40, valid_loss: 0.019152808074767772\n",
      "FOLD: 2, EPOCH: 41, train_loss: 0.017024598096975603\n",
      "FOLD: 2, EPOCH: 41, valid_loss: 0.019142193289903495\n",
      "FOLD: 2, EPOCH: 42, train_loss: 0.01710061525070184\n",
      "FOLD: 2, EPOCH: 42, valid_loss: 0.019136392009946015\n",
      "Epoch    43: reducing learning rate of group 0 to 1.3371e-05.\n",
      "FOLD: 2, EPOCH: 43, train_loss: 0.01708053409851886\n",
      "FOLD: 2, EPOCH: 43, valid_loss: 0.019159308419777796\n",
      "FOLD: 2, EPOCH: 44, train_loss: 0.017057667813591054\n",
      "FOLD: 2, EPOCH: 44, valid_loss: 0.019142825729571857\n",
      "FOLD: 2, EPOCH: 45, train_loss: 0.01706346685721262\n",
      "FOLD: 2, EPOCH: 45, valid_loss: 0.01914701868708317\n",
      "Epoch    46: reducing learning rate of group 0 to 2.0012e-06.\n",
      "FOLD: 2, EPOCH: 46, train_loss: 0.017036646579367085\n",
      "FOLD: 2, EPOCH: 46, valid_loss: 0.019129323701445874\n",
      "FOLD: 2, EPOCH: 47, train_loss: 0.01701327384374029\n",
      "FOLD: 2, EPOCH: 47, valid_loss: 0.01916485666655577\n",
      "FOLD: 2, EPOCH: 48, train_loss: 0.01705154911237391\n",
      "FOLD: 2, EPOCH: 48, valid_loss: 0.019155623247990243\n",
      "FOLD: 2, EPOCH: 49, train_loss: 0.017068009244630467\n",
      "FOLD: 2, EPOCH: 49, valid_loss: 0.01914650832231228\n",
      "Epoch    50: reducing learning rate of group 0 to 2.9950e-07.\n",
      "FOLD: 3, EPOCH: 0, train_loss: 0.04761509149259812\n",
      "FOLD: 3, EPOCH: 0, valid_loss: 0.022785530640528753\n",
      "FOLD: 3, EPOCH: 1, train_loss: 0.022252354896753222\n",
      "FOLD: 3, EPOCH: 1, valid_loss: 0.021867281255813744\n",
      "FOLD: 3, EPOCH: 2, train_loss: 0.021569652961113968\n",
      "FOLD: 3, EPOCH: 2, valid_loss: 0.02102471014054922\n",
      "FOLD: 3, EPOCH: 3, train_loss: 0.021183105789729068\n",
      "FOLD: 3, EPOCH: 3, valid_loss: 0.020780435261818078\n",
      "FOLD: 3, EPOCH: 4, train_loss: 0.02087218241413703\n",
      "FOLD: 3, EPOCH: 4, valid_loss: 0.02076331038887684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 3, EPOCH: 5, train_loss: 0.020666576040959037\n",
      "FOLD: 3, EPOCH: 5, valid_loss: 0.020379688590765\n",
      "FOLD: 3, EPOCH: 6, train_loss: 0.020482496937384475\n",
      "FOLD: 3, EPOCH: 6, valid_loss: 0.020229538759359948\n",
      "FOLD: 3, EPOCH: 7, train_loss: 0.020254410954343306\n",
      "FOLD: 3, EPOCH: 7, valid_loss: 0.020228037753930457\n",
      "FOLD: 3, EPOCH: 8, train_loss: 0.020121797994786018\n",
      "FOLD: 3, EPOCH: 8, valid_loss: 0.019939599701991446\n",
      "FOLD: 3, EPOCH: 9, train_loss: 0.01995792375827158\n",
      "FOLD: 3, EPOCH: 9, valid_loss: 0.019957868239054315\n",
      "FOLD: 3, EPOCH: 10, train_loss: 0.019771645852440112\n",
      "FOLD: 3, EPOCH: 10, valid_loss: 0.019731640959015258\n",
      "FOLD: 3, EPOCH: 11, train_loss: 0.019656472335997467\n",
      "FOLD: 3, EPOCH: 11, valid_loss: 0.019701170949981764\n",
      "FOLD: 3, EPOCH: 12, train_loss: 0.019518735894077533\n",
      "FOLD: 3, EPOCH: 12, valid_loss: 0.019613428471180108\n",
      "FOLD: 3, EPOCH: 13, train_loss: 0.019404902928382962\n",
      "FOLD: 3, EPOCH: 13, valid_loss: 0.019596653059124947\n",
      "FOLD: 3, EPOCH: 14, train_loss: 0.019282273822338193\n",
      "FOLD: 3, EPOCH: 14, valid_loss: 0.01974924223927351\n",
      "FOLD: 3, EPOCH: 15, train_loss: 0.01912177729143484\n",
      "FOLD: 3, EPOCH: 15, valid_loss: 0.01944306206244689\n",
      "FOLD: 3, EPOCH: 16, train_loss: 0.01901916799613753\n",
      "FOLD: 3, EPOCH: 16, valid_loss: 0.01947893460209553\n",
      "FOLD: 3, EPOCH: 17, train_loss: 0.01895574709350193\n",
      "FOLD: 3, EPOCH: 17, valid_loss: 0.019373062998056412\n",
      "FOLD: 3, EPOCH: 18, train_loss: 0.018817180289408646\n",
      "FOLD: 3, EPOCH: 18, valid_loss: 0.019298456179407928\n",
      "FOLD: 3, EPOCH: 19, train_loss: 0.018660422867617092\n",
      "FOLD: 3, EPOCH: 19, valid_loss: 0.019323620228813246\n",
      "FOLD: 3, EPOCH: 20, train_loss: 0.018603907092600257\n",
      "FOLD: 3, EPOCH: 20, valid_loss: 0.019269547926691864\n",
      "FOLD: 3, EPOCH: 21, train_loss: 0.018468472578034207\n",
      "FOLD: 3, EPOCH: 21, valid_loss: 0.019295515635838874\n",
      "FOLD: 3, EPOCH: 22, train_loss: 0.018352625111268984\n",
      "FOLD: 3, EPOCH: 22, valid_loss: 0.01920720848899621\n",
      "FOLD: 3, EPOCH: 23, train_loss: 0.018261836715848058\n",
      "FOLD: 3, EPOCH: 23, valid_loss: 0.01921375525685457\n",
      "FOLD: 3, EPOCH: 24, train_loss: 0.018116001694186312\n",
      "FOLD: 3, EPOCH: 24, valid_loss: 0.01916862393801029\n",
      "FOLD: 3, EPOCH: 25, train_loss: 0.01805434456548175\n",
      "FOLD: 3, EPOCH: 25, valid_loss: 0.019183126349861804\n",
      "FOLD: 3, EPOCH: 26, train_loss: 0.017895519091571506\n",
      "FOLD: 3, EPOCH: 26, valid_loss: 0.01916143412773426\n",
      "FOLD: 3, EPOCH: 27, train_loss: 0.017802217648037383\n",
      "FOLD: 3, EPOCH: 27, valid_loss: 0.019158729137136385\n",
      "FOLD: 3, EPOCH: 28, train_loss: 0.017727414473287156\n",
      "FOLD: 3, EPOCH: 28, valid_loss: 0.01914935516050229\n",
      "FOLD: 3, EPOCH: 29, train_loss: 0.01755395536688534\n",
      "FOLD: 3, EPOCH: 29, valid_loss: 0.019165379066879932\n",
      "FOLD: 3, EPOCH: 30, train_loss: 0.017495814358463157\n",
      "FOLD: 3, EPOCH: 30, valid_loss: 0.01913866658623402\n",
      "FOLD: 3, EPOCH: 31, train_loss: 0.017398119866344576\n",
      "FOLD: 3, EPOCH: 31, valid_loss: 0.019134130185613267\n",
      "FOLD: 3, EPOCH: 32, train_loss: 0.017206165310297464\n",
      "FOLD: 3, EPOCH: 32, valid_loss: 0.01912424383828273\n",
      "FOLD: 3, EPOCH: 33, train_loss: 0.017120928881136147\n",
      "FOLD: 3, EPOCH: 33, valid_loss: 0.01919741785297027\n",
      "FOLD: 3, EPOCH: 34, train_loss: 0.01701408001669758\n",
      "FOLD: 3, EPOCH: 34, valid_loss: 0.0191170613353069\n",
      "FOLD: 3, EPOCH: 35, train_loss: 0.016856474915166963\n",
      "FOLD: 3, EPOCH: 35, valid_loss: 0.019088261116009492\n",
      "FOLD: 3, EPOCH: 36, train_loss: 0.01675189137962219\n",
      "FOLD: 3, EPOCH: 36, valid_loss: 0.01915546477987216\n",
      "FOLD: 3, EPOCH: 37, train_loss: 0.016651012330643228\n",
      "FOLD: 3, EPOCH: 37, valid_loss: 0.019079802844386835\n",
      "FOLD: 3, EPOCH: 38, train_loss: 0.01655032319595685\n",
      "FOLD: 3, EPOCH: 38, valid_loss: 0.01911566764689409\n",
      "FOLD: 3, EPOCH: 39, train_loss: 0.016468810874062614\n",
      "FOLD: 3, EPOCH: 39, valid_loss: 0.019079088591612302\n",
      "FOLD: 3, EPOCH: 40, train_loss: 0.01635827307866232\n",
      "FOLD: 3, EPOCH: 40, valid_loss: 0.01909809659879941\n",
      "FOLD: 3, EPOCH: 41, train_loss: 0.01620464150265262\n",
      "FOLD: 3, EPOCH: 41, valid_loss: 0.019092264370276377\n",
      "FOLD: 3, EPOCH: 42, train_loss: 0.01611114537494408\n",
      "FOLD: 3, EPOCH: 42, valid_loss: 0.01910351116496783\n",
      "Epoch    43: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 3, EPOCH: 43, train_loss: 0.01574527074557704\n",
      "FOLD: 3, EPOCH: 43, valid_loss: 0.019084067585376594\n",
      "FOLD: 3, EPOCH: 44, train_loss: 0.01571832057340322\n",
      "FOLD: 3, EPOCH: 44, valid_loss: 0.019067697800122775\n",
      "FOLD: 3, EPOCH: 45, train_loss: 0.01568691326758346\n",
      "FOLD: 3, EPOCH: 45, valid_loss: 0.01904706284403801\n",
      "FOLD: 3, EPOCH: 46, train_loss: 0.015580785070621484\n",
      "FOLD: 3, EPOCH: 46, valid_loss: 0.019047379063872192\n",
      "FOLD: 3, EPOCH: 47, train_loss: 0.015538203530013561\n",
      "FOLD: 3, EPOCH: 47, valid_loss: 0.019068009721545074\n",
      "FOLD: 3, EPOCH: 48, train_loss: 0.015488129751001662\n",
      "FOLD: 3, EPOCH: 48, valid_loss: 0.019059983583597038\n",
      "Epoch    49: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 3, EPOCH: 49, train_loss: 0.015452455490122776\n",
      "FOLD: 3, EPOCH: 49, valid_loss: 0.01904109249321314\n",
      "FOLD: 4, EPOCH: 0, train_loss: 0.04681709587473322\n",
      "FOLD: 4, EPOCH: 0, valid_loss: 0.023160547591172732\n",
      "FOLD: 4, EPOCH: 1, train_loss: 0.022253846925859515\n",
      "FOLD: 4, EPOCH: 1, valid_loss: 0.02333515094449887\n",
      "FOLD: 4, EPOCH: 2, train_loss: 0.021521120458035857\n",
      "FOLD: 4, EPOCH: 2, valid_loss: 0.022495886454215415\n",
      "FOLD: 4, EPOCH: 3, train_loss: 0.021165486676870165\n",
      "FOLD: 4, EPOCH: 3, valid_loss: 0.02135217533661769\n",
      "FOLD: 4, EPOCH: 4, train_loss: 0.020864449516945594\n",
      "FOLD: 4, EPOCH: 4, valid_loss: 0.021620739967777178\n",
      "FOLD: 4, EPOCH: 5, train_loss: 0.02061522233526449\n",
      "FOLD: 4, EPOCH: 5, valid_loss: 0.020655069930049088\n",
      "FOLD: 4, EPOCH: 6, train_loss: 0.020368231863186165\n",
      "FOLD: 4, EPOCH: 6, valid_loss: 0.02066966576071886\n",
      "FOLD: 4, EPOCH: 7, train_loss: 0.020240601756282756\n",
      "FOLD: 4, EPOCH: 7, valid_loss: 0.020506518678023264\n",
      "FOLD: 4, EPOCH: 8, train_loss: 0.02013438177370542\n",
      "FOLD: 4, EPOCH: 8, valid_loss: 0.02042145597246977\n",
      "FOLD: 4, EPOCH: 9, train_loss: 0.019948929044845944\n",
      "FOLD: 4, EPOCH: 9, valid_loss: 0.02029735494691592\n",
      "FOLD: 4, EPOCH: 10, train_loss: 0.019756596283735457\n",
      "FOLD: 4, EPOCH: 10, valid_loss: 0.020152817981747482\n",
      "FOLD: 4, EPOCH: 11, train_loss: 0.019646779382349672\n",
      "FOLD: 4, EPOCH: 11, valid_loss: 0.020045859309343193\n",
      "FOLD: 4, EPOCH: 12, train_loss: 0.019491363090236444\n",
      "FOLD: 4, EPOCH: 12, valid_loss: 0.019985336953630813\n",
      "FOLD: 4, EPOCH: 13, train_loss: 0.01940680670275076\n",
      "FOLD: 4, EPOCH: 13, valid_loss: 0.019930157380608413\n",
      "FOLD: 4, EPOCH: 14, train_loss: 0.01925708642984564\n",
      "FOLD: 4, EPOCH: 14, valid_loss: 0.019831697671459272\n",
      "FOLD: 4, EPOCH: 15, train_loss: 0.01918505854602601\n",
      "FOLD: 4, EPOCH: 15, valid_loss: 0.019829713381253757\n",
      "FOLD: 4, EPOCH: 16, train_loss: 0.019036372846646887\n",
      "FOLD: 4, EPOCH: 16, valid_loss: 0.019701054176458947\n",
      "FOLD: 4, EPOCH: 17, train_loss: 0.018927874395976197\n",
      "FOLD: 4, EPOCH: 17, valid_loss: 0.01960390591277526\n",
      "FOLD: 4, EPOCH: 18, train_loss: 0.01879739693391162\n",
      "FOLD: 4, EPOCH: 18, valid_loss: 0.01957899246078271\n",
      "FOLD: 4, EPOCH: 19, train_loss: 0.018671638709870545\n",
      "FOLD: 4, EPOCH: 19, valid_loss: 0.019684589109741725\n",
      "FOLD: 4, EPOCH: 20, train_loss: 0.01857174391782767\n",
      "FOLD: 4, EPOCH: 20, valid_loss: 0.019496395754126403\n",
      "FOLD: 4, EPOCH: 21, train_loss: 0.018454852463627183\n",
      "FOLD: 4, EPOCH: 21, valid_loss: 0.019607792966640912\n",
      "FOLD: 4, EPOCH: 22, train_loss: 0.018363778152175853\n",
      "FOLD: 4, EPOCH: 22, valid_loss: 0.019500186953407068\n",
      "FOLD: 4, EPOCH: 23, train_loss: 0.018262930566797387\n",
      "FOLD: 4, EPOCH: 23, valid_loss: 0.01947670100400081\n",
      "FOLD: 4, EPOCH: 24, train_loss: 0.018087614453523547\n",
      "FOLD: 4, EPOCH: 24, valid_loss: 0.019437208055303648\n",
      "FOLD: 4, EPOCH: 25, train_loss: 0.01802453104205228\n",
      "FOLD: 4, EPOCH: 25, valid_loss: 0.019392978256711595\n",
      "FOLD: 4, EPOCH: 26, train_loss: 0.01784536793727327\n",
      "FOLD: 4, EPOCH: 26, valid_loss: 0.019416591582389977\n",
      "FOLD: 4, EPOCH: 27, train_loss: 0.01775517438009784\n",
      "FOLD: 4, EPOCH: 27, valid_loss: 0.01949119481902856\n",
      "FOLD: 4, EPOCH: 28, train_loss: 0.01768005231546389\n",
      "FOLD: 4, EPOCH: 28, valid_loss: 0.019367763342765663\n",
      "FOLD: 4, EPOCH: 29, train_loss: 0.01756225770490395\n",
      "FOLD: 4, EPOCH: 29, valid_loss: 0.0193535927683115\n",
      "FOLD: 4, EPOCH: 30, train_loss: 0.017392785691127583\n",
      "FOLD: 4, EPOCH: 30, valid_loss: 0.01935757753940729\n",
      "FOLD: 4, EPOCH: 31, train_loss: 0.017360927927232272\n",
      "FOLD: 4, EPOCH: 31, valid_loss: 0.019374818326188967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 4, EPOCH: 32, train_loss: 0.01719733012991177\n",
      "FOLD: 4, EPOCH: 32, valid_loss: 0.019390923902392387\n",
      "Epoch    33: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 4, EPOCH: 33, train_loss: 0.016861141432781477\n",
      "FOLD: 4, EPOCH: 33, valid_loss: 0.019265757013971988\n",
      "FOLD: 4, EPOCH: 34, train_loss: 0.016744091691499628\n",
      "FOLD: 4, EPOCH: 34, valid_loss: 0.019235592478742965\n",
      "FOLD: 4, EPOCH: 35, train_loss: 0.016674773251587473\n",
      "FOLD: 4, EPOCH: 35, valid_loss: 0.0192318342339534\n",
      "FOLD: 4, EPOCH: 36, train_loss: 0.016607894637697452\n",
      "FOLD: 4, EPOCH: 36, valid_loss: 0.019206367433071136\n",
      "FOLD: 4, EPOCH: 37, train_loss: 0.016660930085423832\n",
      "FOLD: 4, EPOCH: 37, valid_loss: 0.019216391616142713\n",
      "FOLD: 4, EPOCH: 38, train_loss: 0.016518996336270828\n",
      "FOLD: 4, EPOCH: 38, valid_loss: 0.019230737422521297\n",
      "FOLD: 4, EPOCH: 39, train_loss: 0.01648408340642581\n",
      "FOLD: 4, EPOCH: 39, valid_loss: 0.019197406963660166\n",
      "FOLD: 4, EPOCH: 40, train_loss: 0.016476731973926764\n",
      "FOLD: 4, EPOCH: 40, valid_loss: 0.019217339129402086\n",
      "FOLD: 4, EPOCH: 41, train_loss: 0.01642595383816877\n",
      "FOLD: 4, EPOCH: 41, valid_loss: 0.019208833288687926\n",
      "FOLD: 4, EPOCH: 42, train_loss: 0.01635967680832019\n",
      "FOLD: 4, EPOCH: 42, valid_loss: 0.01921075410567797\n",
      "Epoch    43: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 4, EPOCH: 43, train_loss: 0.016383877665911976\n",
      "FOLD: 4, EPOCH: 43, valid_loss: 0.01919997684084452\n",
      "FOLD: 4, EPOCH: 44, train_loss: 0.01629953514281157\n",
      "FOLD: 4, EPOCH: 44, valid_loss: 0.019206940697935913\n",
      "FOLD: 4, EPOCH: 45, train_loss: 0.016378520711048228\n",
      "FOLD: 4, EPOCH: 45, valid_loss: 0.019198132535585992\n",
      "Epoch    46: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 4, EPOCH: 46, train_loss: 0.01632282803640575\n",
      "FOLD: 4, EPOCH: 46, valid_loss: 0.01919318076509696\n",
      "FOLD: 4, EPOCH: 47, train_loss: 0.016317441996589705\n",
      "FOLD: 4, EPOCH: 47, valid_loss: 0.019200859734645255\n",
      "FOLD: 4, EPOCH: 48, train_loss: 0.01627591113887123\n",
      "FOLD: 4, EPOCH: 48, valid_loss: 0.019205843886503808\n",
      "FOLD: 4, EPOCH: 49, train_loss: 0.016286930635672162\n",
      "FOLD: 4, EPOCH: 49, valid_loss: 0.019210725449598752\n",
      "Epoch    50: reducing learning rate of group 0 to 8.9345e-05.\n",
      "FOLD: 5, EPOCH: 0, train_loss: 0.04662906464088608\n",
      "FOLD: 5, EPOCH: 0, valid_loss: 0.022993673068972733\n",
      "FOLD: 5, EPOCH: 1, train_loss: 0.02237537266636217\n",
      "FOLD: 5, EPOCH: 1, valid_loss: 0.021946251535644896\n",
      "FOLD: 5, EPOCH: 2, train_loss: 0.021591180205546522\n",
      "FOLD: 5, EPOCH: 2, valid_loss: 0.021272218714539822\n",
      "FOLD: 5, EPOCH: 3, train_loss: 0.02131747842036389\n",
      "FOLD: 5, EPOCH: 3, valid_loss: 0.020879197006042186\n",
      "FOLD: 5, EPOCH: 4, train_loss: 0.020961742143373232\n",
      "FOLD: 5, EPOCH: 4, valid_loss: 0.020781563308376532\n",
      "FOLD: 5, EPOCH: 5, train_loss: 0.020727213547640556\n",
      "FOLD: 5, EPOCH: 5, valid_loss: 0.020374602136703637\n",
      "FOLD: 5, EPOCH: 6, train_loss: 0.020484581161793823\n",
      "FOLD: 5, EPOCH: 6, valid_loss: 0.020246326493529174\n",
      "FOLD: 5, EPOCH: 7, train_loss: 0.020295956146878166\n",
      "FOLD: 5, EPOCH: 7, valid_loss: 0.020104864039100133\n",
      "FOLD: 5, EPOCH: 8, train_loss: 0.020090567350790307\n",
      "FOLD: 5, EPOCH: 8, valid_loss: 0.019957168171038993\n",
      "FOLD: 5, EPOCH: 9, train_loss: 0.019949647295918013\n",
      "FOLD: 5, EPOCH: 9, valid_loss: 0.01988991478887888\n",
      "FOLD: 5, EPOCH: 10, train_loss: 0.019824837535821104\n",
      "FOLD: 5, EPOCH: 10, valid_loss: 0.01974249717134696\n",
      "FOLD: 5, EPOCH: 11, train_loss: 0.01963988510338036\n",
      "FOLD: 5, EPOCH: 11, valid_loss: 0.0196153512940957\n",
      "FOLD: 5, EPOCH: 12, train_loss: 0.019483748622037268\n",
      "FOLD: 5, EPOCH: 12, valid_loss: 0.019639810977073815\n",
      "FOLD: 5, EPOCH: 13, train_loss: 0.019368115183268045\n",
      "FOLD: 5, EPOCH: 13, valid_loss: 0.01973497337446763\n",
      "FOLD: 5, EPOCH: 14, train_loss: 0.01925527978990529\n",
      "FOLD: 5, EPOCH: 14, valid_loss: 0.019395855756906364\n",
      "FOLD: 5, EPOCH: 15, train_loss: 0.01913382951170206\n",
      "FOLD: 5, EPOCH: 15, valid_loss: 0.019794967168798812\n",
      "FOLD: 5, EPOCH: 16, train_loss: 0.019046851761035016\n",
      "FOLD: 5, EPOCH: 16, valid_loss: 0.0193500485844337\n",
      "FOLD: 5, EPOCH: 17, train_loss: 0.018879867536393372\n",
      "FOLD: 5, EPOCH: 17, valid_loss: 0.019298598170280457\n",
      "FOLD: 5, EPOCH: 18, train_loss: 0.018778786063194275\n",
      "FOLD: 5, EPOCH: 18, valid_loss: 0.01923014696400899\n",
      "FOLD: 5, EPOCH: 19, train_loss: 0.018680132749314245\n",
      "FOLD: 5, EPOCH: 19, valid_loss: 0.01923683686898305\n",
      "FOLD: 5, EPOCH: 20, train_loss: 0.01853201475397155\n",
      "FOLD: 5, EPOCH: 20, valid_loss: 0.019213337021378372\n",
      "FOLD: 5, EPOCH: 21, train_loss: 0.018409141163165506\n",
      "FOLD: 5, EPOCH: 21, valid_loss: 0.01923489713898072\n",
      "FOLD: 5, EPOCH: 22, train_loss: 0.018316103246163676\n",
      "FOLD: 5, EPOCH: 22, valid_loss: 0.01915997166473132\n",
      "FOLD: 5, EPOCH: 23, train_loss: 0.018162983796886495\n",
      "FOLD: 5, EPOCH: 23, valid_loss: 0.019207408365148764\n",
      "FOLD: 5, EPOCH: 24, train_loss: 0.018027659455263936\n",
      "FOLD: 5, EPOCH: 24, valid_loss: 0.01911609820448435\n",
      "FOLD: 5, EPOCH: 25, train_loss: 0.017940650593388726\n",
      "FOLD: 5, EPOCH: 25, valid_loss: 0.019099375089773767\n",
      "FOLD: 5, EPOCH: 26, train_loss: 0.017843787080130062\n",
      "FOLD: 5, EPOCH: 26, valid_loss: 0.019088280458862964\n",
      "FOLD: 5, EPOCH: 27, train_loss: 0.017703906809156005\n",
      "FOLD: 5, EPOCH: 27, valid_loss: 0.019044126885441635\n",
      "FOLD: 5, EPOCH: 28, train_loss: 0.01753624468236356\n",
      "FOLD: 5, EPOCH: 28, valid_loss: 0.01902486799428096\n",
      "FOLD: 5, EPOCH: 29, train_loss: 0.01744285422201092\n",
      "FOLD: 5, EPOCH: 29, valid_loss: 0.019058017776562616\n",
      "FOLD: 5, EPOCH: 30, train_loss: 0.017338841071201337\n",
      "FOLD: 5, EPOCH: 30, valid_loss: 0.01909511908888817\n",
      "FOLD: 5, EPOCH: 31, train_loss: 0.01722315643486139\n",
      "FOLD: 5, EPOCH: 31, valid_loss: 0.019037706634173028\n",
      "Epoch    32: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 5, EPOCH: 32, train_loss: 0.016808836167124478\n",
      "FOLD: 5, EPOCH: 32, valid_loss: 0.018939729063556746\n",
      "FOLD: 5, EPOCH: 33, train_loss: 0.016716305946780217\n",
      "FOLD: 5, EPOCH: 33, valid_loss: 0.01890818516795452\n",
      "FOLD: 5, EPOCH: 34, train_loss: 0.016618860772231948\n",
      "FOLD: 5, EPOCH: 34, valid_loss: 0.01892053737090184\n",
      "FOLD: 5, EPOCH: 35, train_loss: 0.016545990909877663\n",
      "FOLD: 5, EPOCH: 35, valid_loss: 0.018878948659851\n",
      "FOLD: 5, EPOCH: 36, train_loss: 0.016510756837355124\n",
      "FOLD: 5, EPOCH: 36, valid_loss: 0.01889890289077392\n",
      "FOLD: 5, EPOCH: 37, train_loss: 0.016457131384192285\n",
      "FOLD: 5, EPOCH: 37, valid_loss: 0.01891374201155626\n",
      "FOLD: 5, EPOCH: 38, train_loss: 0.01640047023475573\n",
      "FOLD: 5, EPOCH: 38, valid_loss: 0.018895348390707604\n",
      "Epoch    39: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 5, EPOCH: 39, train_loss: 0.016313789966138633\n",
      "FOLD: 5, EPOCH: 39, valid_loss: 0.018913275920427762\n",
      "FOLD: 5, EPOCH: 40, train_loss: 0.01639056018226453\n",
      "FOLD: 5, EPOCH: 40, valid_loss: 0.0188774737314536\n",
      "FOLD: 5, EPOCH: 41, train_loss: 0.016360362941348874\n",
      "FOLD: 5, EPOCH: 41, valid_loss: 0.018893441615196373\n",
      "FOLD: 5, EPOCH: 42, train_loss: 0.01628142858326838\n",
      "FOLD: 5, EPOCH: 42, valid_loss: 0.018892662026561223\n",
      "FOLD: 5, EPOCH: 43, train_loss: 0.016354203148669488\n",
      "FOLD: 5, EPOCH: 43, valid_loss: 0.018888867245270655\n",
      "Epoch    44: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 5, EPOCH: 44, train_loss: 0.016272492517994064\n",
      "FOLD: 5, EPOCH: 44, valid_loss: 0.018882915950738467\n",
      "FOLD: 5, EPOCH: 45, train_loss: 0.01627069670153228\n",
      "FOLD: 5, EPOCH: 45, valid_loss: 0.018882345981322803\n",
      "FOLD: 5, EPOCH: 46, train_loss: 0.016269302020805912\n",
      "FOLD: 5, EPOCH: 46, valid_loss: 0.018898308993532106\n",
      "Epoch    47: reducing learning rate of group 0 to 8.9345e-05.\n",
      "FOLD: 5, EPOCH: 47, train_loss: 0.01631882060570894\n",
      "FOLD: 5, EPOCH: 47, valid_loss: 0.018915903539611742\n",
      "FOLD: 5, EPOCH: 48, train_loss: 0.016296126867166243\n",
      "FOLD: 5, EPOCH: 48, valid_loss: 0.018869195563288834\n",
      "FOLD: 5, EPOCH: 49, train_loss: 0.016343682500961666\n",
      "FOLD: 5, EPOCH: 49, valid_loss: 0.018886373879817817\n",
      "FOLD: 6, EPOCH: 0, train_loss: 0.048860169846463845\n",
      "FOLD: 6, EPOCH: 0, valid_loss: 0.023824346609986745\n",
      "FOLD: 6, EPOCH: 1, train_loss: 0.022434662196885894\n",
      "FOLD: 6, EPOCH: 1, valid_loss: 0.02186295246848693\n",
      "FOLD: 6, EPOCH: 2, train_loss: 0.02167934033314924\n",
      "FOLD: 6, EPOCH: 2, valid_loss: 0.021566334252174083\n",
      "FOLD: 6, EPOCH: 3, train_loss: 0.02126677830174968\n",
      "FOLD: 6, EPOCH: 3, valid_loss: 0.02071491700525467\n",
      "FOLD: 6, EPOCH: 4, train_loss: 0.020947331738834445\n",
      "FOLD: 6, EPOCH: 4, valid_loss: 0.020520914202699296\n",
      "FOLD: 6, EPOCH: 5, train_loss: 0.020625226451335726\n",
      "FOLD: 6, EPOCH: 5, valid_loss: 0.020265899025476895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 6, EPOCH: 6, train_loss: 0.020428666442229942\n",
      "FOLD: 6, EPOCH: 6, valid_loss: 0.02015683499093239\n",
      "FOLD: 6, EPOCH: 7, train_loss: 0.020179159310017084\n",
      "FOLD: 6, EPOCH: 7, valid_loss: 0.020028877430237256\n",
      "FOLD: 6, EPOCH: 8, train_loss: 0.020028105282501596\n",
      "FOLD: 6, EPOCH: 8, valid_loss: 0.01994160505441519\n",
      "FOLD: 6, EPOCH: 9, train_loss: 0.019835044000599836\n",
      "FOLD: 6, EPOCH: 9, valid_loss: 0.019768357563477296\n",
      "FOLD: 6, EPOCH: 10, train_loss: 0.019726154520302207\n",
      "FOLD: 6, EPOCH: 10, valid_loss: 0.019572397264150474\n",
      "FOLD: 6, EPOCH: 11, train_loss: 0.019548926502466202\n",
      "FOLD: 6, EPOCH: 11, valid_loss: 0.01952944380732683\n",
      "FOLD: 6, EPOCH: 12, train_loss: 0.019450718763510923\n",
      "FOLD: 6, EPOCH: 12, valid_loss: 0.01944322970051032\n",
      "FOLD: 6, EPOCH: 13, train_loss: 0.01932521182036883\n",
      "FOLD: 6, EPOCH: 13, valid_loss: 0.019435399283583347\n",
      "FOLD: 6, EPOCH: 14, train_loss: 0.01915197165028469\n",
      "FOLD: 6, EPOCH: 14, valid_loss: 0.019298624533873338\n",
      "FOLD: 6, EPOCH: 15, train_loss: 0.019035545656004467\n",
      "FOLD: 6, EPOCH: 15, valid_loss: 0.019234434200020935\n",
      "FOLD: 6, EPOCH: 16, train_loss: 0.018941315167860406\n",
      "FOLD: 6, EPOCH: 16, valid_loss: 0.019274760467501786\n",
      "FOLD: 6, EPOCH: 17, train_loss: 0.018823387856419023\n",
      "FOLD: 6, EPOCH: 17, valid_loss: 0.019161491583173092\n",
      "FOLD: 6, EPOCH: 18, train_loss: 0.018674521480460424\n",
      "FOLD: 6, EPOCH: 18, valid_loss: 0.019204354630066797\n",
      "FOLD: 6, EPOCH: 19, train_loss: 0.018550292865650075\n",
      "FOLD: 6, EPOCH: 19, valid_loss: 0.01910327632839863\n",
      "FOLD: 6, EPOCH: 20, train_loss: 0.01843865729264311\n",
      "FOLD: 6, EPOCH: 20, valid_loss: 0.01910145881657417\n",
      "FOLD: 6, EPOCH: 21, train_loss: 0.018329419573215214\n",
      "FOLD: 6, EPOCH: 21, valid_loss: 0.019074149429798126\n",
      "FOLD: 6, EPOCH: 22, train_loss: 0.018196447299340286\n",
      "FOLD: 6, EPOCH: 22, valid_loss: 0.019022673798295166\n",
      "FOLD: 6, EPOCH: 23, train_loss: 0.018117725949835132\n",
      "FOLD: 6, EPOCH: 23, valid_loss: 0.019078614906622812\n",
      "FOLD: 6, EPOCH: 24, train_loss: 0.017986994136024167\n",
      "FOLD: 6, EPOCH: 24, valid_loss: 0.018972943751857832\n",
      "FOLD: 6, EPOCH: 25, train_loss: 0.01791073427208372\n",
      "FOLD: 6, EPOCH: 25, valid_loss: 0.018957608594344214\n",
      "FOLD: 6, EPOCH: 26, train_loss: 0.017744247147158998\n",
      "FOLD: 6, EPOCH: 26, valid_loss: 0.018977009333097018\n",
      "FOLD: 6, EPOCH: 27, train_loss: 0.017630111426115036\n",
      "FOLD: 6, EPOCH: 27, valid_loss: 0.01896760555414053\n",
      "FOLD: 6, EPOCH: 28, train_loss: 0.017529370414244162\n",
      "FOLD: 6, EPOCH: 28, valid_loss: 0.019007356980672248\n",
      "Epoch    29: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 6, EPOCH: 29, train_loss: 0.017149031892217493\n",
      "FOLD: 6, EPOCH: 29, valid_loss: 0.0188771950510832\n",
      "FOLD: 6, EPOCH: 30, train_loss: 0.017064174335147883\n",
      "FOLD: 6, EPOCH: 30, valid_loss: 0.018849062231870797\n",
      "FOLD: 6, EPOCH: 31, train_loss: 0.01693324402377412\n",
      "FOLD: 6, EPOCH: 31, valid_loss: 0.018853230115312796\n",
      "FOLD: 6, EPOCH: 32, train_loss: 0.016919881168350175\n",
      "FOLD: 6, EPOCH: 32, valid_loss: 0.01883070070583087\n",
      "FOLD: 6, EPOCH: 33, train_loss: 0.016873918826112878\n",
      "FOLD: 6, EPOCH: 33, valid_loss: 0.01883878157689021\n",
      "FOLD: 6, EPOCH: 34, train_loss: 0.01681880941111091\n",
      "FOLD: 6, EPOCH: 34, valid_loss: 0.018832427091323413\n",
      "FOLD: 6, EPOCH: 35, train_loss: 0.016779033341319173\n",
      "FOLD: 6, EPOCH: 35, valid_loss: 0.018805332481861115\n",
      "FOLD: 6, EPOCH: 36, train_loss: 0.016734198125935084\n",
      "FOLD: 6, EPOCH: 36, valid_loss: 0.01881199086514803\n",
      "FOLD: 6, EPOCH: 37, train_loss: 0.016669344962448686\n",
      "FOLD: 6, EPOCH: 37, valid_loss: 0.018816543743014336\n",
      "FOLD: 6, EPOCH: 38, train_loss: 0.016665796019338274\n",
      "FOLD: 6, EPOCH: 38, valid_loss: 0.01881392113864422\n",
      "Epoch    39: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 6, EPOCH: 39, train_loss: 0.016589233815367962\n",
      "FOLD: 6, EPOCH: 39, valid_loss: 0.018830053365001313\n",
      "FOLD: 6, EPOCH: 40, train_loss: 0.016585413857388334\n",
      "FOLD: 6, EPOCH: 40, valid_loss: 0.018817829684569284\n",
      "FOLD: 6, EPOCH: 41, train_loss: 0.016575342006477957\n",
      "FOLD: 6, EPOCH: 41, valid_loss: 0.018836454989818428\n",
      "Epoch    42: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 6, EPOCH: 42, train_loss: 0.016532616438092413\n",
      "FOLD: 6, EPOCH: 42, valid_loss: 0.018818534050996486\n",
      "FOLD: 6, EPOCH: 43, train_loss: 0.016537322889308672\n",
      "FOLD: 6, EPOCH: 43, valid_loss: 0.018812836362765387\n",
      "FOLD: 6, EPOCH: 44, train_loss: 0.016532276316571073\n",
      "FOLD: 6, EPOCH: 44, valid_loss: 0.01882046231856713\n",
      "Epoch    45: reducing learning rate of group 0 to 8.9345e-05.\n",
      "FOLD: 6, EPOCH: 45, train_loss: 0.016533248843876896\n",
      "FOLD: 6, EPOCH: 45, valid_loss: 0.018806838788665257\n",
      "FOLD: 6, EPOCH: 46, train_loss: 0.01655499287251685\n",
      "FOLD: 6, EPOCH: 46, valid_loss: 0.01881701355943313\n",
      "FOLD: 6, EPOCH: 47, train_loss: 0.0165623978434785\n",
      "FOLD: 6, EPOCH: 47, valid_loss: 0.018836477484840613\n",
      "Epoch    48: reducing learning rate of group 0 to 1.3371e-05.\n",
      "FOLD: 6, EPOCH: 48, train_loss: 0.016626915667910833\n",
      "FOLD: 6, EPOCH: 48, valid_loss: 0.018817097665025637\n",
      "FOLD: 6, EPOCH: 49, train_loss: 0.01657410989188262\n",
      "FOLD: 6, EPOCH: 49, valid_loss: 0.018821080000354692\n",
      "FOLD: 0, EPOCH: 0, train_loss: 0.05002762941089836\n",
      "FOLD: 0, EPOCH: 0, valid_loss: 0.02301239981674231\n",
      "FOLD: 0, EPOCH: 1, train_loss: 0.02247449740566112\n",
      "FOLD: 0, EPOCH: 1, valid_loss: 0.022725651757075235\n",
      "FOLD: 0, EPOCH: 2, train_loss: 0.02173267272175164\n",
      "FOLD: 0, EPOCH: 2, valid_loss: 0.021363731330403916\n",
      "FOLD: 0, EPOCH: 3, train_loss: 0.021296858384802535\n",
      "FOLD: 0, EPOCH: 3, valid_loss: 0.02073740643950609\n",
      "FOLD: 0, EPOCH: 4, train_loss: 0.020975106345439278\n",
      "FOLD: 0, EPOCH: 4, valid_loss: 0.020740800752089575\n",
      "FOLD: 0, EPOCH: 5, train_loss: 0.020738158551220957\n",
      "FOLD: 0, EPOCH: 5, valid_loss: 0.020725504423563298\n",
      "FOLD: 0, EPOCH: 6, train_loss: 0.020622359689425777\n",
      "FOLD: 0, EPOCH: 6, valid_loss: 0.020275630056858063\n",
      "FOLD: 0, EPOCH: 7, train_loss: 0.020265693725967728\n",
      "FOLD: 0, EPOCH: 7, valid_loss: 0.019946516419832524\n",
      "FOLD: 0, EPOCH: 8, train_loss: 0.020083667332860263\n",
      "FOLD: 0, EPOCH: 8, valid_loss: 0.01995270885527134\n",
      "FOLD: 0, EPOCH: 9, train_loss: 0.01993441362739415\n",
      "FOLD: 0, EPOCH: 9, valid_loss: 0.019885081654557817\n",
      "FOLD: 0, EPOCH: 10, train_loss: 0.019739131493544258\n",
      "FOLD: 0, EPOCH: 10, valid_loss: 0.01951726611990195\n",
      "FOLD: 0, EPOCH: 11, train_loss: 0.019621133602954244\n",
      "FOLD: 0, EPOCH: 11, valid_loss: 0.020792269649413917\n",
      "FOLD: 0, EPOCH: 12, train_loss: 0.019459064697494376\n",
      "FOLD: 0, EPOCH: 12, valid_loss: 0.019502442330121994\n",
      "FOLD: 0, EPOCH: 13, train_loss: 0.019332294025131175\n",
      "FOLD: 0, EPOCH: 13, valid_loss: 0.019250570294948723\n",
      "FOLD: 0, EPOCH: 14, train_loss: 0.01917017815080849\n",
      "FOLD: 0, EPOCH: 14, valid_loss: 0.01930413982616021\n",
      "FOLD: 0, EPOCH: 15, train_loss: 0.01907990940821332\n",
      "FOLD: 0, EPOCH: 15, valid_loss: 0.01924243440421728\n",
      "FOLD: 0, EPOCH: 16, train_loss: 0.018867143027081684\n",
      "FOLD: 0, EPOCH: 16, valid_loss: 0.019254276958795693\n",
      "FOLD: 0, EPOCH: 17, train_loss: 0.0188020715187933\n",
      "FOLD: 0, EPOCH: 17, valid_loss: 0.01917521397654827\n",
      "FOLD: 0, EPOCH: 18, train_loss: 0.018668676600665658\n",
      "FOLD: 0, EPOCH: 18, valid_loss: 0.01911575261216897\n",
      "FOLD: 0, EPOCH: 19, train_loss: 0.018563832408068952\n",
      "FOLD: 0, EPOCH: 19, valid_loss: 0.01922557445672842\n",
      "FOLD: 0, EPOCH: 20, train_loss: 0.018463523152309494\n",
      "FOLD: 0, EPOCH: 20, valid_loss: 0.019123528725825824\n",
      "FOLD: 0, EPOCH: 21, train_loss: 0.018311921632974536\n",
      "FOLD: 0, EPOCH: 21, valid_loss: 0.01904236038143818\n",
      "FOLD: 0, EPOCH: 22, train_loss: 0.018149837901866115\n",
      "FOLD: 0, EPOCH: 22, valid_loss: 0.018950190824957993\n",
      "FOLD: 0, EPOCH: 23, train_loss: 0.01803385796075737\n",
      "FOLD: 0, EPOCH: 23, valid_loss: 0.01905333107480636\n",
      "FOLD: 0, EPOCH: 24, train_loss: 0.01789430102232743\n",
      "FOLD: 0, EPOCH: 24, valid_loss: 0.01894541600575814\n",
      "FOLD: 0, EPOCH: 25, train_loss: 0.017811060052465747\n",
      "FOLD: 0, EPOCH: 25, valid_loss: 0.01896051661326335\n",
      "FOLD: 0, EPOCH: 26, train_loss: 0.017708124970463483\n",
      "FOLD: 0, EPOCH: 26, valid_loss: 0.018909531430556223\n",
      "FOLD: 0, EPOCH: 27, train_loss: 0.017558027964991493\n",
      "FOLD: 0, EPOCH: 27, valid_loss: 0.018839465597501166\n",
      "FOLD: 0, EPOCH: 28, train_loss: 0.017454161963148696\n",
      "FOLD: 0, EPOCH: 28, valid_loss: 0.01888530156933344\n",
      "FOLD: 0, EPOCH: 29, train_loss: 0.01731886130732459\n",
      "FOLD: 0, EPOCH: 29, valid_loss: 0.018899335597570125\n",
      "FOLD: 0, EPOCH: 30, train_loss: 0.01711497117287001\n",
      "FOLD: 0, EPOCH: 30, valid_loss: 0.01893047343652982\n",
      "Epoch    31: reducing learning rate of group 0 to 2.6653e-02.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 0, EPOCH: 31, train_loss: 0.016843158965678635\n",
      "FOLD: 0, EPOCH: 31, valid_loss: 0.018825422112758342\n",
      "FOLD: 0, EPOCH: 32, train_loss: 0.016688668320106494\n",
      "FOLD: 0, EPOCH: 32, valid_loss: 0.018791536872203533\n",
      "FOLD: 0, EPOCH: 33, train_loss: 0.016620172250613168\n",
      "FOLD: 0, EPOCH: 33, valid_loss: 0.01879467012790533\n",
      "FOLD: 0, EPOCH: 34, train_loss: 0.016537196279780286\n",
      "FOLD: 0, EPOCH: 34, valid_loss: 0.018777829523269948\n",
      "FOLD: 0, EPOCH: 35, train_loss: 0.01648849678049619\n",
      "FOLD: 0, EPOCH: 35, valid_loss: 0.018756215245677874\n",
      "FOLD: 0, EPOCH: 36, train_loss: 0.016450771243890393\n",
      "FOLD: 0, EPOCH: 36, valid_loss: 0.018785175365897324\n",
      "FOLD: 0, EPOCH: 37, train_loss: 0.016415572700065537\n",
      "FOLD: 0, EPOCH: 37, valid_loss: 0.018754229379388\n",
      "FOLD: 0, EPOCH: 38, train_loss: 0.016361312525397218\n",
      "FOLD: 0, EPOCH: 38, valid_loss: 0.018759317696094513\n",
      "FOLD: 0, EPOCH: 39, train_loss: 0.016239438356982695\n",
      "FOLD: 0, EPOCH: 39, valid_loss: 0.018761681822630074\n",
      "FOLD: 0, EPOCH: 40, train_loss: 0.01630676499995831\n",
      "FOLD: 0, EPOCH: 40, valid_loss: 0.018757726567295883\n",
      "Epoch    41: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 0, EPOCH: 41, train_loss: 0.01623913667139572\n",
      "FOLD: 0, EPOCH: 41, valid_loss: 0.01876325962635187\n",
      "FOLD: 0, EPOCH: 42, train_loss: 0.01623497555988866\n",
      "FOLD: 0, EPOCH: 42, valid_loss: 0.018770768378789607\n",
      "FOLD: 0, EPOCH: 43, train_loss: 0.016176687427670568\n",
      "FOLD: 0, EPOCH: 43, valid_loss: 0.018752614179482825\n",
      "FOLD: 0, EPOCH: 44, train_loss: 0.01623080694745924\n",
      "FOLD: 0, EPOCH: 44, valid_loss: 0.01875366112933709\n",
      "FOLD: 0, EPOCH: 45, train_loss: 0.016180885200564925\n",
      "FOLD: 0, EPOCH: 45, valid_loss: 0.018735073650112517\n",
      "FOLD: 0, EPOCH: 46, train_loss: 0.016187066312980006\n",
      "FOLD: 0, EPOCH: 46, valid_loss: 0.018761570207201518\n",
      "FOLD: 0, EPOCH: 47, train_loss: 0.01618776413788264\n",
      "FOLD: 0, EPOCH: 47, valid_loss: 0.018763302753751095\n",
      "FOLD: 0, EPOCH: 48, train_loss: 0.016186383565434732\n",
      "FOLD: 0, EPOCH: 48, valid_loss: 0.018752305553509638\n",
      "Epoch    49: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 0, EPOCH: 49, train_loss: 0.016156262038527307\n",
      "FOLD: 0, EPOCH: 49, valid_loss: 0.018749089911580086\n",
      "FOLD: 1, EPOCH: 0, train_loss: 0.04888093670377055\n",
      "FOLD: 1, EPOCH: 0, valid_loss: 0.02290188377866378\n",
      "FOLD: 1, EPOCH: 1, train_loss: 0.022321174726695626\n",
      "FOLD: 1, EPOCH: 1, valid_loss: 0.022296277089760855\n",
      "FOLD: 1, EPOCH: 2, train_loss: 0.021615791084194504\n",
      "FOLD: 1, EPOCH: 2, valid_loss: 0.023366430344489906\n",
      "FOLD: 1, EPOCH: 3, train_loss: 0.021186400783827174\n",
      "FOLD: 1, EPOCH: 3, valid_loss: 0.021513059592017762\n",
      "FOLD: 1, EPOCH: 4, train_loss: 0.020854362513165216\n",
      "FOLD: 1, EPOCH: 4, valid_loss: 0.020752166756070577\n",
      "FOLD: 1, EPOCH: 5, train_loss: 0.020561257201070722\n",
      "FOLD: 1, EPOCH: 5, valid_loss: 0.020530252072673578\n",
      "FOLD: 1, EPOCH: 6, train_loss: 0.02039382511095421\n",
      "FOLD: 1, EPOCH: 6, valid_loss: 0.020626148352256186\n",
      "FOLD: 1, EPOCH: 7, train_loss: 0.020231397344252548\n",
      "FOLD: 1, EPOCH: 7, valid_loss: 0.02016723772081045\n",
      "FOLD: 1, EPOCH: 8, train_loss: 0.02004459246086913\n",
      "FOLD: 1, EPOCH: 8, valid_loss: 0.020037774712993547\n",
      "FOLD: 1, EPOCH: 9, train_loss: 0.019867350649390672\n",
      "FOLD: 1, EPOCH: 9, valid_loss: 0.019983131151932936\n",
      "FOLD: 1, EPOCH: 10, train_loss: 0.01972402216916954\n",
      "FOLD: 1, EPOCH: 10, valid_loss: 0.019752300129486963\n",
      "FOLD: 1, EPOCH: 11, train_loss: 0.01950836174089361\n",
      "FOLD: 1, EPOCH: 11, valid_loss: 0.01973412286203641\n",
      "FOLD: 1, EPOCH: 12, train_loss: 0.019388933187803707\n",
      "FOLD: 1, EPOCH: 12, valid_loss: 0.019793048071173523\n",
      "FOLD: 1, EPOCH: 13, train_loss: 0.019278217258082854\n",
      "FOLD: 1, EPOCH: 13, valid_loss: 0.019564783343902\n",
      "FOLD: 1, EPOCH: 14, train_loss: 0.019117782144127664\n",
      "FOLD: 1, EPOCH: 14, valid_loss: 0.019555449915619996\n",
      "FOLD: 1, EPOCH: 15, train_loss: 0.01896520462390539\n",
      "FOLD: 1, EPOCH: 15, valid_loss: 0.019474160069456466\n",
      "FOLD: 1, EPOCH: 16, train_loss: 0.01882242295588996\n",
      "FOLD: 1, EPOCH: 16, valid_loss: 0.019361469608086806\n",
      "FOLD: 1, EPOCH: 17, train_loss: 0.018771429571348267\n",
      "FOLD: 1, EPOCH: 17, valid_loss: 0.0193404290252007\n",
      "FOLD: 1, EPOCH: 18, train_loss: 0.01859610781979722\n",
      "FOLD: 1, EPOCH: 18, valid_loss: 0.019286350848583076\n",
      "FOLD: 1, EPOCH: 19, train_loss: 0.018467759864555823\n",
      "FOLD: 1, EPOCH: 19, valid_loss: 0.019287670174470313\n",
      "FOLD: 1, EPOCH: 20, train_loss: 0.018339147212336194\n",
      "FOLD: 1, EPOCH: 20, valid_loss: 0.019258857203217652\n",
      "FOLD: 1, EPOCH: 21, train_loss: 0.018259000209336344\n",
      "FOLD: 1, EPOCH: 21, valid_loss: 0.01936869638470503\n",
      "FOLD: 1, EPOCH: 22, train_loss: 0.018127349356340396\n",
      "FOLD: 1, EPOCH: 22, valid_loss: 0.019237384343376525\n",
      "FOLD: 1, EPOCH: 23, train_loss: 0.01804806473287376\n",
      "FOLD: 1, EPOCH: 23, valid_loss: 0.019199364460431613\n",
      "FOLD: 1, EPOCH: 24, train_loss: 0.017940345950223303\n",
      "FOLD: 1, EPOCH: 24, valid_loss: 0.019167165773419235\n",
      "FOLD: 1, EPOCH: 25, train_loss: 0.017776106559746974\n",
      "FOLD: 1, EPOCH: 25, valid_loss: 0.01906721666455269\n",
      "FOLD: 1, EPOCH: 26, train_loss: 0.017633645958896424\n",
      "FOLD: 1, EPOCH: 26, valid_loss: 0.01911227605663813\n",
      "FOLD: 1, EPOCH: 27, train_loss: 0.0175081979281999\n",
      "FOLD: 1, EPOCH: 27, valid_loss: 0.01909449897133387\n",
      "FOLD: 1, EPOCH: 28, train_loss: 0.017385620951048425\n",
      "FOLD: 1, EPOCH: 28, valid_loss: 0.01903126474756461\n",
      "FOLD: 1, EPOCH: 29, train_loss: 0.01730283641734639\n",
      "FOLD: 1, EPOCH: 29, valid_loss: 0.01907112807608568\n",
      "FOLD: 1, EPOCH: 30, train_loss: 0.017186613677925355\n",
      "FOLD: 1, EPOCH: 30, valid_loss: 0.019006692589475557\n",
      "FOLD: 1, EPOCH: 31, train_loss: 0.017041049356496817\n",
      "FOLD: 1, EPOCH: 31, valid_loss: 0.01901751670699853\n",
      "FOLD: 1, EPOCH: 32, train_loss: 0.016933623701334\n",
      "FOLD: 1, EPOCH: 32, valid_loss: 0.019054778923208896\n",
      "FOLD: 1, EPOCH: 33, train_loss: 0.016816226501219177\n",
      "FOLD: 1, EPOCH: 33, valid_loss: 0.019018280391509716\n",
      "Epoch    34: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 1, EPOCH: 34, train_loss: 0.016465382461712974\n",
      "FOLD: 1, EPOCH: 34, valid_loss: 0.018949617846654013\n",
      "FOLD: 1, EPOCH: 35, train_loss: 0.016305696710038023\n",
      "FOLD: 1, EPOCH: 35, valid_loss: 0.01894289183502014\n",
      "FOLD: 1, EPOCH: 36, train_loss: 0.0163000494342398\n",
      "FOLD: 1, EPOCH: 36, valid_loss: 0.018926149377456077\n",
      "FOLD: 1, EPOCH: 37, train_loss: 0.01619122946332838\n",
      "FOLD: 1, EPOCH: 37, valid_loss: 0.01892260232797036\n",
      "FOLD: 1, EPOCH: 38, train_loss: 0.01615284662693739\n",
      "FOLD: 1, EPOCH: 38, valid_loss: 0.018892601132392883\n",
      "FOLD: 1, EPOCH: 39, train_loss: 0.016046538269398985\n",
      "FOLD: 1, EPOCH: 39, valid_loss: 0.018913843454076693\n",
      "FOLD: 1, EPOCH: 40, train_loss: 0.016115214914788265\n",
      "FOLD: 1, EPOCH: 40, valid_loss: 0.01889379981618661\n",
      "FOLD: 1, EPOCH: 41, train_loss: 0.016010384893397223\n",
      "FOLD: 1, EPOCH: 41, valid_loss: 0.018886106518598702\n",
      "FOLD: 1, EPOCH: 42, train_loss: 0.01598391737285498\n",
      "FOLD: 1, EPOCH: 42, valid_loss: 0.01890301417845946\n",
      "FOLD: 1, EPOCH: 43, train_loss: 0.015935303290953506\n",
      "FOLD: 1, EPOCH: 43, valid_loss: 0.018885404731218632\n",
      "FOLD: 1, EPOCH: 44, train_loss: 0.015894286120562134\n",
      "FOLD: 1, EPOCH: 44, valid_loss: 0.018878497183322906\n",
      "FOLD: 1, EPOCH: 45, train_loss: 0.015886697616126086\n",
      "FOLD: 1, EPOCH: 45, valid_loss: 0.01888342001117193\n",
      "FOLD: 1, EPOCH: 46, train_loss: 0.015857422573340905\n",
      "FOLD: 1, EPOCH: 46, valid_loss: 0.01887690476500071\n",
      "FOLD: 1, EPOCH: 47, train_loss: 0.01583112347115939\n",
      "FOLD: 1, EPOCH: 47, valid_loss: 0.018894491144097768\n",
      "FOLD: 1, EPOCH: 48, train_loss: 0.015787048856853635\n",
      "FOLD: 1, EPOCH: 48, valid_loss: 0.01890142935399826\n",
      "FOLD: 1, EPOCH: 49, train_loss: 0.01572727121261729\n",
      "FOLD: 1, EPOCH: 49, valid_loss: 0.018887526427324\n",
      "Epoch    50: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 2, EPOCH: 0, train_loss: 0.047199314107766024\n",
      "FOLD: 2, EPOCH: 0, valid_loss: 0.02346805282510244\n",
      "FOLD: 2, EPOCH: 1, train_loss: 0.022242518192207492\n",
      "FOLD: 2, EPOCH: 1, valid_loss: 0.022808519406960562\n",
      "FOLD: 2, EPOCH: 2, train_loss: 0.021561851271906414\n",
      "FOLD: 2, EPOCH: 2, valid_loss: 0.021797371455110036\n",
      "FOLD: 2, EPOCH: 3, train_loss: 0.02116890883425603\n",
      "FOLD: 2, EPOCH: 3, valid_loss: 0.02123168812921414\n",
      "FOLD: 2, EPOCH: 4, train_loss: 0.020834476938722906\n",
      "FOLD: 2, EPOCH: 4, valid_loss: 0.020935913261312705\n",
      "FOLD: 2, EPOCH: 5, train_loss: 0.020626867794104525\n",
      "FOLD: 2, EPOCH: 5, valid_loss: 0.02058682433114602\n",
      "FOLD: 2, EPOCH: 6, train_loss: 0.02041719501485696\n",
      "FOLD: 2, EPOCH: 6, valid_loss: 0.020726731333595056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 2, EPOCH: 7, train_loss: 0.020298814703081106\n",
      "FOLD: 2, EPOCH: 7, valid_loss: 0.020345458760857582\n",
      "FOLD: 2, EPOCH: 8, train_loss: 0.020076358992908452\n",
      "FOLD: 2, EPOCH: 8, valid_loss: 0.020972111620582067\n",
      "FOLD: 2, EPOCH: 9, train_loss: 0.019934533314930426\n",
      "FOLD: 2, EPOCH: 9, valid_loss: 0.020285348479564373\n",
      "FOLD: 2, EPOCH: 10, train_loss: 0.01977114225863605\n",
      "FOLD: 2, EPOCH: 10, valid_loss: 0.02011576696084096\n",
      "FOLD: 2, EPOCH: 11, train_loss: 0.019621609080884908\n",
      "FOLD: 2, EPOCH: 11, valid_loss: 0.01995074677352722\n",
      "FOLD: 2, EPOCH: 12, train_loss: 0.0195021724479424\n",
      "FOLD: 2, EPOCH: 12, valid_loss: 0.01983467460824893\n",
      "FOLD: 2, EPOCH: 13, train_loss: 0.0194129443450554\n",
      "FOLD: 2, EPOCH: 13, valid_loss: 0.019799100378384955\n",
      "FOLD: 2, EPOCH: 14, train_loss: 0.019251911120640265\n",
      "FOLD: 2, EPOCH: 14, valid_loss: 0.01973421570773308\n",
      "FOLD: 2, EPOCH: 15, train_loss: 0.019152759675037215\n",
      "FOLD: 2, EPOCH: 15, valid_loss: 0.01968668229304827\n",
      "FOLD: 2, EPOCH: 16, train_loss: 0.019057743100298417\n",
      "FOLD: 2, EPOCH: 16, valid_loss: 0.019589972610657033\n",
      "FOLD: 2, EPOCH: 17, train_loss: 0.018893571041927144\n",
      "FOLD: 2, EPOCH: 17, valid_loss: 0.01955282401580077\n",
      "FOLD: 2, EPOCH: 18, train_loss: 0.018788267807984673\n",
      "FOLD: 2, EPOCH: 18, valid_loss: 0.019521670845838692\n",
      "FOLD: 2, EPOCH: 19, train_loss: 0.01870795080085864\n",
      "FOLD: 2, EPOCH: 19, valid_loss: 0.019611094433527727\n",
      "FOLD: 2, EPOCH: 20, train_loss: 0.0186087654500797\n",
      "FOLD: 2, EPOCH: 20, valid_loss: 0.0194816543505742\n",
      "FOLD: 2, EPOCH: 21, train_loss: 0.01849984662053553\n",
      "FOLD: 2, EPOCH: 21, valid_loss: 0.01946405006142763\n",
      "FOLD: 2, EPOCH: 22, train_loss: 0.018394466791604017\n",
      "FOLD: 2, EPOCH: 22, valid_loss: 0.01941307490834823\n",
      "FOLD: 2, EPOCH: 23, train_loss: 0.01830604818423052\n",
      "FOLD: 2, EPOCH: 23, valid_loss: 0.019415475141543608\n",
      "FOLD: 2, EPOCH: 24, train_loss: 0.018158658885875263\n",
      "FOLD: 2, EPOCH: 24, valid_loss: 0.019309595943643496\n",
      "FOLD: 2, EPOCH: 25, train_loss: 0.018026585690677166\n",
      "FOLD: 2, EPOCH: 25, valid_loss: 0.019343027271903478\n",
      "FOLD: 2, EPOCH: 26, train_loss: 0.017906603743196338\n",
      "FOLD: 2, EPOCH: 26, valid_loss: 0.019256794538635474\n",
      "FOLD: 2, EPOCH: 27, train_loss: 0.017830179425308835\n",
      "FOLD: 2, EPOCH: 27, valid_loss: 0.019242298860962573\n",
      "FOLD: 2, EPOCH: 28, train_loss: 0.017674772471592232\n",
      "FOLD: 2, EPOCH: 28, valid_loss: 0.01925590605689929\n",
      "FOLD: 2, EPOCH: 29, train_loss: 0.017594996965616137\n",
      "FOLD: 2, EPOCH: 29, valid_loss: 0.01918967655644967\n",
      "FOLD: 2, EPOCH: 30, train_loss: 0.01749087094857886\n",
      "FOLD: 2, EPOCH: 30, valid_loss: 0.01922656509738702\n",
      "FOLD: 2, EPOCH: 31, train_loss: 0.017433485444131737\n",
      "FOLD: 2, EPOCH: 31, valid_loss: 0.0192385153988233\n",
      "FOLD: 2, EPOCH: 32, train_loss: 0.01729002654099384\n",
      "FOLD: 2, EPOCH: 32, valid_loss: 0.01919989101588726\n",
      "Epoch    33: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 2, EPOCH: 33, train_loss: 0.016962174099643488\n",
      "FOLD: 2, EPOCH: 33, valid_loss: 0.019157659262418747\n",
      "FOLD: 2, EPOCH: 34, train_loss: 0.016860263170422735\n",
      "FOLD: 2, EPOCH: 34, valid_loss: 0.019128400832414627\n",
      "FOLD: 2, EPOCH: 35, train_loss: 0.016763046799177252\n",
      "FOLD: 2, EPOCH: 35, valid_loss: 0.019124072045087814\n",
      "FOLD: 2, EPOCH: 36, train_loss: 0.01672448732613309\n",
      "FOLD: 2, EPOCH: 36, valid_loss: 0.019139543605538514\n",
      "FOLD: 2, EPOCH: 37, train_loss: 0.01666791783645749\n",
      "FOLD: 2, EPOCH: 37, valid_loss: 0.019107395353225563\n",
      "FOLD: 2, EPOCH: 38, train_loss: 0.016617364501832304\n",
      "FOLD: 2, EPOCH: 38, valid_loss: 0.01911159003010163\n",
      "FOLD: 2, EPOCH: 39, train_loss: 0.016559251231720317\n",
      "FOLD: 2, EPOCH: 39, valid_loss: 0.019126594066619873\n",
      "FOLD: 2, EPOCH: 40, train_loss: 0.01655021749084463\n",
      "FOLD: 2, EPOCH: 40, valid_loss: 0.01909724436700344\n",
      "FOLD: 2, EPOCH: 41, train_loss: 0.01645826199721243\n",
      "FOLD: 2, EPOCH: 41, valid_loss: 0.019082488062290046\n",
      "FOLD: 2, EPOCH: 42, train_loss: 0.01647253441498489\n",
      "FOLD: 2, EPOCH: 42, valid_loss: 0.019090972410944793\n",
      "FOLD: 2, EPOCH: 43, train_loss: 0.016401959860042947\n",
      "FOLD: 2, EPOCH: 43, valid_loss: 0.019095404216876395\n",
      "FOLD: 2, EPOCH: 44, train_loss: 0.01638628591506465\n",
      "FOLD: 2, EPOCH: 44, valid_loss: 0.019082819040005024\n",
      "Epoch    45: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 2, EPOCH: 45, train_loss: 0.016359138101137972\n",
      "FOLD: 2, EPOCH: 45, valid_loss: 0.019073886796832085\n",
      "FOLD: 2, EPOCH: 46, train_loss: 0.01636161091359886\n",
      "FOLD: 2, EPOCH: 46, valid_loss: 0.019078579946206167\n",
      "FOLD: 2, EPOCH: 47, train_loss: 0.016388386487960815\n",
      "FOLD: 2, EPOCH: 47, valid_loss: 0.01908941251727251\n",
      "FOLD: 2, EPOCH: 48, train_loss: 0.016320458512652566\n",
      "FOLD: 2, EPOCH: 48, valid_loss: 0.01910753404864898\n",
      "Epoch    49: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 2, EPOCH: 49, train_loss: 0.016342448699917342\n",
      "FOLD: 2, EPOCH: 49, valid_loss: 0.019094376896436397\n",
      "FOLD: 3, EPOCH: 0, train_loss: 0.0470021286274533\n",
      "FOLD: 3, EPOCH: 0, valid_loss: 0.022283391024057683\n",
      "FOLD: 3, EPOCH: 1, train_loss: 0.02214409614837653\n",
      "FOLD: 3, EPOCH: 1, valid_loss: 0.02170075189608794\n",
      "FOLD: 3, EPOCH: 2, train_loss: 0.02152183828120296\n",
      "FOLD: 3, EPOCH: 2, valid_loss: 0.021684210317639206\n",
      "FOLD: 3, EPOCH: 3, train_loss: 0.021178872148330148\n",
      "FOLD: 3, EPOCH: 3, valid_loss: 0.020888337435630653\n",
      "FOLD: 3, EPOCH: 4, train_loss: 0.020830642608170573\n",
      "FOLD: 3, EPOCH: 4, valid_loss: 0.020334816322876856\n",
      "FOLD: 3, EPOCH: 5, train_loss: 0.020645606356698112\n",
      "FOLD: 3, EPOCH: 5, valid_loss: 0.0202666472357053\n",
      "FOLD: 3, EPOCH: 6, train_loss: 0.02040544698467931\n",
      "FOLD: 3, EPOCH: 6, valid_loss: 0.020159977559859935\n",
      "FOLD: 3, EPOCH: 7, train_loss: 0.02028109704622546\n",
      "FOLD: 3, EPOCH: 7, valid_loss: 0.019996076105878904\n",
      "FOLD: 3, EPOCH: 8, train_loss: 0.0200985371814789\n",
      "FOLD: 3, EPOCH: 8, valid_loss: 0.019896546951853313\n",
      "FOLD: 3, EPOCH: 9, train_loss: 0.019941218121833092\n",
      "FOLD: 3, EPOCH: 9, valid_loss: 0.01983872915689762\n",
      "FOLD: 3, EPOCH: 10, train_loss: 0.019829226003305334\n",
      "FOLD: 3, EPOCH: 10, valid_loss: 0.019664707235418834\n",
      "FOLD: 3, EPOCH: 11, train_loss: 0.019674123944462957\n",
      "FOLD: 3, EPOCH: 11, valid_loss: 0.019699782133102417\n",
      "FOLD: 3, EPOCH: 12, train_loss: 0.01955617783037392\n",
      "FOLD: 3, EPOCH: 12, valid_loss: 0.019477884786633346\n",
      "FOLD: 3, EPOCH: 13, train_loss: 0.01943524879076191\n",
      "FOLD: 3, EPOCH: 13, valid_loss: 0.019548975934202854\n",
      "FOLD: 3, EPOCH: 14, train_loss: 0.019316514626749465\n",
      "FOLD: 3, EPOCH: 14, valid_loss: 0.01946397885107077\n",
      "FOLD: 3, EPOCH: 15, train_loss: 0.019199173192720156\n",
      "FOLD: 3, EPOCH: 15, valid_loss: 0.019475513352797583\n",
      "FOLD: 3, EPOCH: 16, train_loss: 0.01903706174847242\n",
      "FOLD: 3, EPOCH: 16, valid_loss: 0.019405672469964393\n",
      "FOLD: 3, EPOCH: 17, train_loss: 0.018975169146181765\n",
      "FOLD: 3, EPOCH: 17, valid_loss: 0.019283751742197916\n",
      "FOLD: 3, EPOCH: 18, train_loss: 0.01880981622112764\n",
      "FOLD: 3, EPOCH: 18, valid_loss: 0.019275301207716648\n",
      "FOLD: 3, EPOCH: 19, train_loss: 0.018749979878398212\n",
      "FOLD: 3, EPOCH: 19, valid_loss: 0.01929187201536619\n",
      "FOLD: 3, EPOCH: 20, train_loss: 0.018605665706500813\n",
      "FOLD: 3, EPOCH: 20, valid_loss: 0.019197037156957846\n",
      "FOLD: 3, EPOCH: 21, train_loss: 0.018561379984021187\n",
      "FOLD: 3, EPOCH: 21, valid_loss: 0.019182566123513076\n",
      "FOLD: 3, EPOCH: 22, train_loss: 0.018430949188768864\n",
      "FOLD: 3, EPOCH: 22, valid_loss: 0.01923550020616788\n",
      "FOLD: 3, EPOCH: 23, train_loss: 0.018295413776728754\n",
      "FOLD: 3, EPOCH: 23, valid_loss: 0.019121300572386153\n",
      "FOLD: 3, EPOCH: 24, train_loss: 0.01820834376219962\n",
      "FOLD: 3, EPOCH: 24, valid_loss: 0.019420183908480864\n",
      "FOLD: 3, EPOCH: 25, train_loss: 0.018086351504599727\n",
      "FOLD: 3, EPOCH: 25, valid_loss: 0.01918656020783461\n",
      "FOLD: 3, EPOCH: 26, train_loss: 0.018008322429818077\n",
      "FOLD: 3, EPOCH: 26, valid_loss: 0.019140639270727452\n",
      "Epoch    27: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 3, EPOCH: 27, train_loss: 0.01768166878940286\n",
      "FOLD: 3, EPOCH: 27, valid_loss: 0.01901709345670847\n",
      "FOLD: 3, EPOCH: 28, train_loss: 0.01755178484411256\n",
      "FOLD: 3, EPOCH: 28, valid_loss: 0.018989792236915\n",
      "FOLD: 3, EPOCH: 29, train_loss: 0.01746093884513185\n",
      "FOLD: 3, EPOCH: 29, valid_loss: 0.018982060970022127\n",
      "FOLD: 3, EPOCH: 30, train_loss: 0.01742582004265608\n",
      "FOLD: 3, EPOCH: 30, valid_loss: 0.018943039557108514\n",
      "FOLD: 3, EPOCH: 31, train_loss: 0.01735730019926622\n",
      "FOLD: 3, EPOCH: 31, valid_loss: 0.018944027188878793\n",
      "FOLD: 3, EPOCH: 32, train_loss: 0.01730424563425618\n",
      "FOLD: 3, EPOCH: 32, valid_loss: 0.018922239255446654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 3, EPOCH: 33, train_loss: 0.017277713361624127\n",
      "FOLD: 3, EPOCH: 33, valid_loss: 0.018965000859819926\n",
      "FOLD: 3, EPOCH: 34, train_loss: 0.017235017054387042\n",
      "FOLD: 3, EPOCH: 34, valid_loss: 0.018948414004766025\n",
      "FOLD: 3, EPOCH: 35, train_loss: 0.017166056569564988\n",
      "FOLD: 3, EPOCH: 35, valid_loss: 0.01893024891614914\n",
      "Epoch    36: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 3, EPOCH: 36, train_loss: 0.017196549794863205\n",
      "FOLD: 3, EPOCH: 36, valid_loss: 0.018936362117528915\n",
      "FOLD: 3, EPOCH: 37, train_loss: 0.01715491140714368\n",
      "FOLD: 3, EPOCH: 37, valid_loss: 0.01893664667239556\n",
      "FOLD: 3, EPOCH: 38, train_loss: 0.017113044493001054\n",
      "FOLD: 3, EPOCH: 38, valid_loss: 0.018937543607675113\n",
      "Epoch    39: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 3, EPOCH: 39, train_loss: 0.017104537540895713\n",
      "FOLD: 3, EPOCH: 39, valid_loss: 0.01893820656606784\n",
      "FOLD: 3, EPOCH: 40, train_loss: 0.017164890188723803\n",
      "FOLD: 3, EPOCH: 40, valid_loss: 0.018924595931401618\n",
      "FOLD: 3, EPOCH: 41, train_loss: 0.01715529229290582\n",
      "FOLD: 3, EPOCH: 41, valid_loss: 0.01893699226471094\n",
      "Epoch    42: reducing learning rate of group 0 to 8.9345e-05.\n",
      "FOLD: 3, EPOCH: 42, train_loss: 0.01713040630912056\n",
      "FOLD: 3, EPOCH: 42, valid_loss: 0.018931754363270905\n",
      "FOLD: 3, EPOCH: 43, train_loss: 0.01713220014966823\n",
      "FOLD: 3, EPOCH: 43, valid_loss: 0.018941439401644927\n",
      "FOLD: 3, EPOCH: 44, train_loss: 0.017094575988783223\n",
      "FOLD: 3, EPOCH: 44, valid_loss: 0.018930706553734265\n",
      "Epoch    45: reducing learning rate of group 0 to 1.3371e-05.\n",
      "FOLD: 3, EPOCH: 45, train_loss: 0.017181883062663918\n",
      "FOLD: 3, EPOCH: 45, valid_loss: 0.01891796477138996\n",
      "FOLD: 3, EPOCH: 46, train_loss: 0.017120408750063664\n",
      "FOLD: 3, EPOCH: 46, valid_loss: 0.01892219311915911\n",
      "FOLD: 3, EPOCH: 47, train_loss: 0.01716475419045703\n",
      "FOLD: 3, EPOCH: 47, valid_loss: 0.018933994695544243\n",
      "FOLD: 3, EPOCH: 48, train_loss: 0.017148162112445443\n",
      "FOLD: 3, EPOCH: 48, valid_loss: 0.01895094061127076\n",
      "Epoch    49: reducing learning rate of group 0 to 2.0012e-06.\n",
      "FOLD: 3, EPOCH: 49, train_loss: 0.01711229670390084\n",
      "FOLD: 3, EPOCH: 49, valid_loss: 0.018922873557760164\n",
      "FOLD: 4, EPOCH: 0, train_loss: 0.04633949752393607\n",
      "FOLD: 4, EPOCH: 0, valid_loss: 0.023202413263229225\n",
      "FOLD: 4, EPOCH: 1, train_loss: 0.022060734345703513\n",
      "FOLD: 4, EPOCH: 1, valid_loss: 0.021905290965850536\n",
      "FOLD: 4, EPOCH: 2, train_loss: 0.02143079480408011\n",
      "FOLD: 4, EPOCH: 2, valid_loss: 0.02158512447315913\n",
      "FOLD: 4, EPOCH: 3, train_loss: 0.021157634066971572\n",
      "FOLD: 4, EPOCH: 3, valid_loss: 0.021145805573234193\n",
      "FOLD: 4, EPOCH: 4, train_loss: 0.020818163968018583\n",
      "FOLD: 4, EPOCH: 4, valid_loss: 0.02093010352781186\n",
      "FOLD: 4, EPOCH: 5, train_loss: 0.020611996485574824\n",
      "FOLD: 4, EPOCH: 5, valid_loss: 0.02085790582574331\n",
      "FOLD: 4, EPOCH: 6, train_loss: 0.020384380820433836\n",
      "FOLD: 4, EPOCH: 6, valid_loss: 0.020598228734273177\n",
      "FOLD: 4, EPOCH: 7, train_loss: 0.02023067101332787\n",
      "FOLD: 4, EPOCH: 7, valid_loss: 0.020380683816396274\n",
      "FOLD: 4, EPOCH: 8, train_loss: 0.0200708755166144\n",
      "FOLD: 4, EPOCH: 8, valid_loss: 0.02021074782197292\n",
      "FOLD: 4, EPOCH: 9, train_loss: 0.01993169164838823\n",
      "FOLD: 4, EPOCH: 9, valid_loss: 0.020166868343949318\n",
      "FOLD: 4, EPOCH: 10, train_loss: 0.01981646884736177\n",
      "FOLD: 4, EPOCH: 10, valid_loss: 0.02026444014448386\n",
      "FOLD: 4, EPOCH: 11, train_loss: 0.019670609510629565\n",
      "FOLD: 4, EPOCH: 11, valid_loss: 0.01993051113990637\n",
      "FOLD: 4, EPOCH: 12, train_loss: 0.01953689708701662\n",
      "FOLD: 4, EPOCH: 12, valid_loss: 0.01991087241241565\n",
      "FOLD: 4, EPOCH: 13, train_loss: 0.01935937424265855\n",
      "FOLD: 4, EPOCH: 13, valid_loss: 0.01976867034458197\n",
      "FOLD: 4, EPOCH: 14, train_loss: 0.01921743798900295\n",
      "FOLD: 4, EPOCH: 14, valid_loss: 0.019754804814091094\n",
      "FOLD: 4, EPOCH: 15, train_loss: 0.01918925787951495\n",
      "FOLD: 4, EPOCH: 15, valid_loss: 0.019724326924635813\n",
      "FOLD: 4, EPOCH: 16, train_loss: 0.019028950507777767\n",
      "FOLD: 4, EPOCH: 16, valid_loss: 0.01972135528922081\n",
      "FOLD: 4, EPOCH: 17, train_loss: 0.01893215618926931\n",
      "FOLD: 4, EPOCH: 17, valid_loss: 0.019583031821709413\n",
      "FOLD: 4, EPOCH: 18, train_loss: 0.018826666992862482\n",
      "FOLD: 4, EPOCH: 18, valid_loss: 0.019536863152797405\n",
      "FOLD: 4, EPOCH: 19, train_loss: 0.018699175300630363\n",
      "FOLD: 4, EPOCH: 19, valid_loss: 0.019569726231006477\n",
      "FOLD: 4, EPOCH: 20, train_loss: 0.018631397872357756\n",
      "FOLD: 4, EPOCH: 20, valid_loss: 0.01957567150776203\n",
      "FOLD: 4, EPOCH: 21, train_loss: 0.01850875119703847\n",
      "FOLD: 4, EPOCH: 21, valid_loss: 0.019495818477410536\n",
      "FOLD: 4, EPOCH: 22, train_loss: 0.018387864142454958\n",
      "FOLD: 4, EPOCH: 22, valid_loss: 0.019482401987681024\n",
      "FOLD: 4, EPOCH: 23, train_loss: 0.018279225565493107\n",
      "FOLD: 4, EPOCH: 23, valid_loss: 0.01940963059090651\n",
      "FOLD: 4, EPOCH: 24, train_loss: 0.018134852720273507\n",
      "FOLD: 4, EPOCH: 24, valid_loss: 0.019700733085091297\n",
      "FOLD: 4, EPOCH: 25, train_loss: 0.018067925438486242\n",
      "FOLD: 4, EPOCH: 25, valid_loss: 0.01937716998733007\n",
      "FOLD: 4, EPOCH: 26, train_loss: 0.017908454709057067\n",
      "FOLD: 4, EPOCH: 26, valid_loss: 0.01936960406601429\n",
      "FOLD: 4, EPOCH: 27, train_loss: 0.01784816389349667\n",
      "FOLD: 4, EPOCH: 27, valid_loss: 0.019358621050532047\n",
      "FOLD: 4, EPOCH: 28, train_loss: 0.017727651834689283\n",
      "FOLD: 4, EPOCH: 28, valid_loss: 0.019363664663754977\n",
      "FOLD: 4, EPOCH: 29, train_loss: 0.017614723208385544\n",
      "FOLD: 4, EPOCH: 29, valid_loss: 0.019390556674737196\n",
      "FOLD: 4, EPOCH: 30, train_loss: 0.01749782347296541\n",
      "FOLD: 4, EPOCH: 30, valid_loss: 0.019368001331503574\n",
      "Epoch    31: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 4, EPOCH: 31, train_loss: 0.01720417197793722\n",
      "FOLD: 4, EPOCH: 31, valid_loss: 0.01924191931119332\n",
      "FOLD: 4, EPOCH: 32, train_loss: 0.017072093698221283\n",
      "FOLD: 4, EPOCH: 32, valid_loss: 0.01922526439795127\n",
      "FOLD: 4, EPOCH: 33, train_loss: 0.01701828187986\n",
      "FOLD: 4, EPOCH: 33, valid_loss: 0.019241266812269505\n",
      "FOLD: 4, EPOCH: 34, train_loss: 0.016956477161698246\n",
      "FOLD: 4, EPOCH: 34, valid_loss: 0.019241162504141148\n",
      "FOLD: 4, EPOCH: 35, train_loss: 0.016921521869261522\n",
      "FOLD: 4, EPOCH: 35, valid_loss: 0.019221480649251204\n",
      "FOLD: 4, EPOCH: 36, train_loss: 0.01689348284255814\n",
      "FOLD: 4, EPOCH: 36, valid_loss: 0.0192135377572133\n",
      "FOLD: 4, EPOCH: 37, train_loss: 0.016797555522439448\n",
      "FOLD: 4, EPOCH: 37, valid_loss: 0.01920382406276006\n",
      "FOLD: 4, EPOCH: 38, train_loss: 0.016768925886198476\n",
      "FOLD: 4, EPOCH: 38, valid_loss: 0.019241038280037735\n",
      "FOLD: 4, EPOCH: 39, train_loss: 0.016733837087412138\n",
      "FOLD: 4, EPOCH: 39, valid_loss: 0.019216954994660158\n",
      "FOLD: 4, EPOCH: 40, train_loss: 0.016720859195432952\n",
      "FOLD: 4, EPOCH: 40, valid_loss: 0.019211292839967288\n",
      "Epoch    41: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 4, EPOCH: 41, train_loss: 0.016711949760950095\n",
      "FOLD: 4, EPOCH: 41, valid_loss: 0.019212380051612854\n",
      "FOLD: 4, EPOCH: 42, train_loss: 0.016634799477115676\n",
      "FOLD: 4, EPOCH: 42, valid_loss: 0.019224651587697174\n",
      "FOLD: 4, EPOCH: 43, train_loss: 0.016628422069589834\n",
      "FOLD: 4, EPOCH: 43, valid_loss: 0.019210765424829263\n",
      "Epoch    44: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 4, EPOCH: 44, train_loss: 0.01662538295007638\n",
      "FOLD: 4, EPOCH: 44, valid_loss: 0.01921785479554763\n",
      "FOLD: 4, EPOCH: 45, train_loss: 0.01668806906437149\n",
      "FOLD: 4, EPOCH: 45, valid_loss: 0.019203598109575417\n",
      "FOLD: 4, EPOCH: 46, train_loss: 0.016624767006047675\n",
      "FOLD: 4, EPOCH: 46, valid_loss: 0.019208157148498755\n",
      "FOLD: 4, EPOCH: 47, train_loss: 0.016610985898689645\n",
      "FOLD: 4, EPOCH: 47, valid_loss: 0.019226310774683952\n",
      "FOLD: 4, EPOCH: 48, train_loss: 0.01664647554374627\n",
      "FOLD: 4, EPOCH: 48, valid_loss: 0.019233189666500457\n",
      "Epoch    49: reducing learning rate of group 0 to 8.9345e-05.\n",
      "FOLD: 4, EPOCH: 49, train_loss: 0.016646241781780043\n",
      "FOLD: 4, EPOCH: 49, valid_loss: 0.019210968166589737\n",
      "FOLD: 5, EPOCH: 0, train_loss: 0.050613202356003424\n",
      "FOLD: 5, EPOCH: 0, valid_loss: 0.023396282098614253\n",
      "FOLD: 5, EPOCH: 1, train_loss: 0.022397341609403893\n",
      "FOLD: 5, EPOCH: 1, valid_loss: 0.021808171788087256\n",
      "FOLD: 5, EPOCH: 2, train_loss: 0.021649391037990916\n",
      "FOLD: 5, EPOCH: 2, valid_loss: 0.021215085943157855\n",
      "FOLD: 5, EPOCH: 3, train_loss: 0.021165575983153807\n",
      "FOLD: 5, EPOCH: 3, valid_loss: 0.020751049312261436\n",
      "FOLD: 5, EPOCH: 4, train_loss: 0.02092703670968075\n",
      "FOLD: 5, EPOCH: 4, valid_loss: 0.02067577323088279\n",
      "FOLD: 5, EPOCH: 5, train_loss: 0.020674382135070658\n",
      "FOLD: 5, EPOCH: 5, valid_loss: 0.02086774059213125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 5, EPOCH: 6, train_loss: 0.020437168611868006\n",
      "FOLD: 5, EPOCH: 6, valid_loss: 0.02021872395506272\n",
      "FOLD: 5, EPOCH: 7, train_loss: 0.020234037719264224\n",
      "FOLD: 5, EPOCH: 7, valid_loss: 0.019972931307095747\n",
      "FOLD: 5, EPOCH: 8, train_loss: 0.020103295560221415\n",
      "FOLD: 5, EPOCH: 8, valid_loss: 0.019957246258854866\n",
      "FOLD: 5, EPOCH: 9, train_loss: 0.019906902937470255\n",
      "FOLD: 5, EPOCH: 9, valid_loss: 0.01989455736027314\n",
      "FOLD: 5, EPOCH: 10, train_loss: 0.019703131320106017\n",
      "FOLD: 5, EPOCH: 10, valid_loss: 0.01972333900630474\n",
      "FOLD: 5, EPOCH: 11, train_loss: 0.01961669167252006\n",
      "FOLD: 5, EPOCH: 11, valid_loss: 0.01966142439498351\n",
      "FOLD: 5, EPOCH: 12, train_loss: 0.01949122454971075\n",
      "FOLD: 5, EPOCH: 12, valid_loss: 0.019615286388076268\n",
      "FOLD: 5, EPOCH: 13, train_loss: 0.01932739046075054\n",
      "FOLD: 5, EPOCH: 13, valid_loss: 0.01944657902304943\n",
      "FOLD: 5, EPOCH: 14, train_loss: 0.019199122196516476\n",
      "FOLD: 5, EPOCH: 14, valid_loss: 0.019365249774776973\n",
      "FOLD: 5, EPOCH: 15, train_loss: 0.01910655917851506\n",
      "FOLD: 5, EPOCH: 15, valid_loss: 0.01936314455591715\n",
      "FOLD: 5, EPOCH: 16, train_loss: 0.018880223326787755\n",
      "FOLD: 5, EPOCH: 16, valid_loss: 0.019348447855848532\n",
      "FOLD: 5, EPOCH: 17, train_loss: 0.018838364580595814\n",
      "FOLD: 5, EPOCH: 17, valid_loss: 0.019254100150786914\n",
      "FOLD: 5, EPOCH: 18, train_loss: 0.018707965651678073\n",
      "FOLD: 5, EPOCH: 18, valid_loss: 0.019166252504174527\n",
      "FOLD: 5, EPOCH: 19, train_loss: 0.018579563276993262\n",
      "FOLD: 5, EPOCH: 19, valid_loss: 0.019283356001743905\n",
      "FOLD: 5, EPOCH: 20, train_loss: 0.01842681091983576\n",
      "FOLD: 5, EPOCH: 20, valid_loss: 0.01910516734306629\n",
      "FOLD: 5, EPOCH: 21, train_loss: 0.018312523795946223\n",
      "FOLD: 5, EPOCH: 21, valid_loss: 0.019126015786941234\n",
      "FOLD: 5, EPOCH: 22, train_loss: 0.01818711605124377\n",
      "FOLD: 5, EPOCH: 22, valid_loss: 0.019124160019251015\n",
      "FOLD: 5, EPOCH: 23, train_loss: 0.018090134624995896\n",
      "FOLD: 5, EPOCH: 23, valid_loss: 0.019024572120263025\n",
      "FOLD: 5, EPOCH: 24, train_loss: 0.017921953889969235\n",
      "FOLD: 5, EPOCH: 24, valid_loss: 0.019095405363119565\n",
      "FOLD: 5, EPOCH: 25, train_loss: 0.017760265845100622\n",
      "FOLD: 5, EPOCH: 25, valid_loss: 0.019037188675541144\n",
      "FOLD: 5, EPOCH: 26, train_loss: 0.01771101217112831\n",
      "FOLD: 5, EPOCH: 26, valid_loss: 0.019114888488100126\n",
      "Epoch    27: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 5, EPOCH: 27, train_loss: 0.017324317448042536\n",
      "FOLD: 5, EPOCH: 27, valid_loss: 0.018969078763173178\n",
      "FOLD: 5, EPOCH: 28, train_loss: 0.01720050470651807\n",
      "FOLD: 5, EPOCH: 28, valid_loss: 0.01892593875527382\n",
      "FOLD: 5, EPOCH: 29, train_loss: 0.01712741467447297\n",
      "FOLD: 5, EPOCH: 29, valid_loss: 0.01891208253800869\n",
      "FOLD: 5, EPOCH: 30, train_loss: 0.01698666511455903\n",
      "FOLD: 5, EPOCH: 30, valid_loss: 0.018903805516087092\n",
      "FOLD: 5, EPOCH: 31, train_loss: 0.01702422696492962\n",
      "FOLD: 5, EPOCH: 31, valid_loss: 0.01888710890824978\n",
      "FOLD: 5, EPOCH: 32, train_loss: 0.016945685813757213\n",
      "FOLD: 5, EPOCH: 32, valid_loss: 0.01887016471188802\n",
      "FOLD: 5, EPOCH: 33, train_loss: 0.016871189711162367\n",
      "FOLD: 5, EPOCH: 33, valid_loss: 0.018884087841098126\n",
      "FOLD: 5, EPOCH: 34, train_loss: 0.01684804936568882\n",
      "FOLD: 5, EPOCH: 34, valid_loss: 0.018882213733517207\n",
      "FOLD: 5, EPOCH: 35, train_loss: 0.01674878497834544\n",
      "FOLD: 5, EPOCH: 35, valid_loss: 0.018865649660046283\n",
      "FOLD: 5, EPOCH: 36, train_loss: 0.01673233792236125\n",
      "FOLD: 5, EPOCH: 36, valid_loss: 0.01885526355069417\n",
      "FOLD: 5, EPOCH: 37, train_loss: 0.016720619606407913\n",
      "FOLD: 5, EPOCH: 37, valid_loss: 0.018865434882732537\n",
      "FOLD: 5, EPOCH: 38, train_loss: 0.016650017992769543\n",
      "FOLD: 5, EPOCH: 38, valid_loss: 0.018864429054351952\n",
      "FOLD: 5, EPOCH: 39, train_loss: 0.016683257940049108\n",
      "FOLD: 5, EPOCH: 39, valid_loss: 0.018850449902506974\n",
      "FOLD: 5, EPOCH: 40, train_loss: 0.016594960152901506\n",
      "FOLD: 5, EPOCH: 40, valid_loss: 0.01884182642858762\n",
      "FOLD: 5, EPOCH: 41, train_loss: 0.016561219305102085\n",
      "FOLD: 5, EPOCH: 41, valid_loss: 0.018853675287503462\n",
      "FOLD: 5, EPOCH: 42, train_loss: 0.016496572341467883\n",
      "FOLD: 5, EPOCH: 42, valid_loss: 0.018846842388694104\n",
      "FOLD: 5, EPOCH: 43, train_loss: 0.016494913958013058\n",
      "FOLD: 5, EPOCH: 43, valid_loss: 0.018851597865040485\n",
      "Epoch    44: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 5, EPOCH: 44, train_loss: 0.01643435182553288\n",
      "FOLD: 5, EPOCH: 44, valid_loss: 0.018850315648775835\n",
      "FOLD: 5, EPOCH: 45, train_loss: 0.01646155514125083\n",
      "FOLD: 5, EPOCH: 45, valid_loss: 0.01883586782675523\n",
      "FOLD: 5, EPOCH: 46, train_loss: 0.016476081558377355\n",
      "FOLD: 5, EPOCH: 46, valid_loss: 0.01886376366019249\n",
      "FOLD: 5, EPOCH: 47, train_loss: 0.01644408587064292\n",
      "FOLD: 5, EPOCH: 47, valid_loss: 0.018845477929482095\n",
      "FOLD: 5, EPOCH: 48, train_loss: 0.016463740955333452\n",
      "FOLD: 5, EPOCH: 48, valid_loss: 0.01884720975962969\n",
      "Epoch    49: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 5, EPOCH: 49, train_loss: 0.016448889720580867\n",
      "FOLD: 5, EPOCH: 49, valid_loss: 0.018853817135095596\n",
      "FOLD: 6, EPOCH: 0, train_loss: 0.04960122367216123\n",
      "FOLD: 6, EPOCH: 0, valid_loss: 0.02302881358907773\n",
      "FOLD: 6, EPOCH: 1, train_loss: 0.0223784550168627\n",
      "FOLD: 6, EPOCH: 1, valid_loss: 0.024791486847859163\n",
      "FOLD: 6, EPOCH: 2, train_loss: 0.02158327219454018\n",
      "FOLD: 6, EPOCH: 2, valid_loss: 0.021066933010633174\n",
      "FOLD: 6, EPOCH: 3, train_loss: 0.021179985280173855\n",
      "FOLD: 6, EPOCH: 3, valid_loss: 0.0206749840424611\n",
      "FOLD: 6, EPOCH: 4, train_loss: 0.02084087799429088\n",
      "FOLD: 6, EPOCH: 4, valid_loss: 0.02033881413248869\n",
      "FOLD: 6, EPOCH: 5, train_loss: 0.020631402806454414\n",
      "FOLD: 6, EPOCH: 5, valid_loss: 0.020571109767143544\n",
      "FOLD: 6, EPOCH: 6, train_loss: 0.020419738507149992\n",
      "FOLD: 6, EPOCH: 6, valid_loss: 0.020240154403906602\n",
      "FOLD: 6, EPOCH: 7, train_loss: 0.020165979434308166\n",
      "FOLD: 6, EPOCH: 7, valid_loss: 0.019925066341574375\n",
      "FOLD: 6, EPOCH: 8, train_loss: 0.02005568629986531\n",
      "FOLD: 6, EPOCH: 8, valid_loss: 0.02053284587768408\n",
      "FOLD: 6, EPOCH: 9, train_loss: 0.019890414953634545\n",
      "FOLD: 6, EPOCH: 9, valid_loss: 0.019761603325605392\n",
      "FOLD: 6, EPOCH: 10, train_loss: 0.01974291794908208\n",
      "FOLD: 6, EPOCH: 10, valid_loss: 0.01957580690773634\n",
      "FOLD: 6, EPOCH: 11, train_loss: 0.019575805529146582\n",
      "FOLD: 6, EPOCH: 11, valid_loss: 0.019516580236645844\n",
      "FOLD: 6, EPOCH: 12, train_loss: 0.01945374470606849\n",
      "FOLD: 6, EPOCH: 12, valid_loss: 0.01944573911336752\n",
      "FOLD: 6, EPOCH: 13, train_loss: 0.01930562056903098\n",
      "FOLD: 6, EPOCH: 13, valid_loss: 0.019404778686853554\n",
      "FOLD: 6, EPOCH: 14, train_loss: 0.019162024522351252\n",
      "FOLD: 6, EPOCH: 14, valid_loss: 0.019399344347990476\n",
      "FOLD: 6, EPOCH: 15, train_loss: 0.019103884470422525\n",
      "FOLD: 6, EPOCH: 15, valid_loss: 0.01926895402945005\n",
      "FOLD: 6, EPOCH: 16, train_loss: 0.01891461747220239\n",
      "FOLD: 6, EPOCH: 16, valid_loss: 0.019261446709816273\n",
      "FOLD: 6, EPOCH: 17, train_loss: 0.01883506482920131\n",
      "FOLD: 6, EPOCH: 17, valid_loss: 0.019201998527233418\n",
      "FOLD: 6, EPOCH: 18, train_loss: 0.018748840795376816\n",
      "FOLD: 6, EPOCH: 18, valid_loss: 0.019180493859144356\n",
      "FOLD: 6, EPOCH: 19, train_loss: 0.018641051534261252\n",
      "FOLD: 6, EPOCH: 19, valid_loss: 0.019392532081558153\n",
      "FOLD: 6, EPOCH: 20, train_loss: 0.018531259073800332\n",
      "FOLD: 6, EPOCH: 20, valid_loss: 0.01915635884954379\n",
      "FOLD: 6, EPOCH: 21, train_loss: 0.018368467587876965\n",
      "FOLD: 6, EPOCH: 21, valid_loss: 0.0190768067080241\n",
      "FOLD: 6, EPOCH: 22, train_loss: 0.018225858466246643\n",
      "FOLD: 6, EPOCH: 22, valid_loss: 0.019007494673132896\n",
      "FOLD: 6, EPOCH: 23, train_loss: 0.018156145145562855\n",
      "FOLD: 6, EPOCH: 23, valid_loss: 0.019049281254410744\n",
      "FOLD: 6, EPOCH: 24, train_loss: 0.017999837879796286\n",
      "FOLD: 6, EPOCH: 24, valid_loss: 0.019042334590966884\n",
      "FOLD: 6, EPOCH: 25, train_loss: 0.0179435598185739\n",
      "FOLD: 6, EPOCH: 25, valid_loss: 0.019058319095235605\n",
      "Epoch    26: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 6, EPOCH: 26, train_loss: 0.01762904498625446\n",
      "FOLD: 6, EPOCH: 26, valid_loss: 0.018881317801200427\n",
      "FOLD: 6, EPOCH: 27, train_loss: 0.01747142878436559\n",
      "FOLD: 6, EPOCH: 27, valid_loss: 0.01887031916815501\n",
      "FOLD: 6, EPOCH: 28, train_loss: 0.01737873231035632\n",
      "FOLD: 6, EPOCH: 28, valid_loss: 0.018844990632854976\n",
      "FOLD: 6, EPOCH: 29, train_loss: 0.01733974678240515\n",
      "FOLD: 6, EPOCH: 29, valid_loss: 0.018860045533913832\n",
      "FOLD: 6, EPOCH: 30, train_loss: 0.01728004642535706\n",
      "FOLD: 6, EPOCH: 30, valid_loss: 0.01885536814538332\n",
      "FOLD: 6, EPOCH: 31, train_loss: 0.01725315227097756\n",
      "FOLD: 6, EPOCH: 31, valid_loss: 0.018831961286755707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 6, EPOCH: 32, train_loss: 0.017212905983969167\n",
      "FOLD: 6, EPOCH: 32, valid_loss: 0.018813236974752866\n",
      "FOLD: 6, EPOCH: 33, train_loss: 0.017155244216524267\n",
      "FOLD: 6, EPOCH: 33, valid_loss: 0.018822044993822392\n",
      "FOLD: 6, EPOCH: 34, train_loss: 0.01711703067947481\n",
      "FOLD: 6, EPOCH: 34, valid_loss: 0.01881418663721818\n",
      "FOLD: 6, EPOCH: 35, train_loss: 0.01707313199703758\n",
      "FOLD: 6, EPOCH: 35, valid_loss: 0.01881986068418393\n",
      "Epoch    36: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 6, EPOCH: 36, train_loss: 0.017036265039162057\n",
      "FOLD: 6, EPOCH: 36, valid_loss: 0.01879924736343897\n",
      "FOLD: 6, EPOCH: 37, train_loss: 0.017006144279966485\n",
      "FOLD: 6, EPOCH: 37, valid_loss: 0.018805392946188267\n",
      "FOLD: 6, EPOCH: 38, train_loss: 0.017001652888752317\n",
      "FOLD: 6, EPOCH: 38, valid_loss: 0.018819789330546673\n",
      "FOLD: 6, EPOCH: 39, train_loss: 0.017007831660275523\n",
      "FOLD: 6, EPOCH: 39, valid_loss: 0.018813977304559488\n",
      "Epoch    40: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 6, EPOCH: 40, train_loss: 0.0169795823812082\n",
      "FOLD: 6, EPOCH: 40, valid_loss: 0.018801239820627067\n",
      "FOLD: 6, EPOCH: 41, train_loss: 0.016990321122009207\n",
      "FOLD: 6, EPOCH: 41, valid_loss: 0.01879971044567915\n",
      "FOLD: 6, EPOCH: 42, train_loss: 0.017035298452184006\n",
      "FOLD: 6, EPOCH: 42, valid_loss: 0.018818852276756212\n",
      "Epoch    43: reducing learning rate of group 0 to 8.9345e-05.\n",
      "FOLD: 6, EPOCH: 43, train_loss: 0.017015287954662298\n",
      "FOLD: 6, EPOCH: 43, valid_loss: 0.018808064982295036\n",
      "FOLD: 6, EPOCH: 44, train_loss: 0.01698480365244118\n",
      "FOLD: 6, EPOCH: 44, valid_loss: 0.01881455544095773\n",
      "FOLD: 6, EPOCH: 45, train_loss: 0.016989595684650784\n",
      "FOLD: 6, EPOCH: 45, valid_loss: 0.01879544111971672\n",
      "FOLD: 6, EPOCH: 46, train_loss: 0.016989556481004565\n",
      "FOLD: 6, EPOCH: 46, valid_loss: 0.01881194831087039\n",
      "FOLD: 6, EPOCH: 47, train_loss: 0.016998133823476935\n",
      "FOLD: 6, EPOCH: 47, valid_loss: 0.018802342506555412\n",
      "FOLD: 6, EPOCH: 48, train_loss: 0.01699363629056795\n",
      "FOLD: 6, EPOCH: 48, valid_loss: 0.018788832598007642\n",
      "FOLD: 6, EPOCH: 49, train_loss: 0.017039190877128293\n",
      "FOLD: 6, EPOCH: 49, valid_loss: 0.018794367089867592\n",
      "FOLD: 0, EPOCH: 0, train_loss: 0.04621077240822283\n",
      "FOLD: 0, EPOCH: 0, valid_loss: 0.023398331438119594\n",
      "FOLD: 0, EPOCH: 1, train_loss: 0.022349559299245075\n",
      "FOLD: 0, EPOCH: 1, valid_loss: 0.02149160149005743\n",
      "FOLD: 0, EPOCH: 2, train_loss: 0.02166041626116714\n",
      "FOLD: 0, EPOCH: 2, valid_loss: 0.020943691237614706\n",
      "FOLD: 0, EPOCH: 3, train_loss: 0.021229817708199088\n",
      "FOLD: 0, EPOCH: 3, valid_loss: 0.020419132680847094\n",
      "FOLD: 0, EPOCH: 4, train_loss: 0.020869587120172138\n",
      "FOLD: 0, EPOCH: 4, valid_loss: 0.020519865246919487\n",
      "FOLD: 0, EPOCH: 5, train_loss: 0.020700313981521775\n",
      "FOLD: 0, EPOCH: 5, valid_loss: 0.020322197331832006\n",
      "FOLD: 0, EPOCH: 6, train_loss: 0.02049526051190254\n",
      "FOLD: 0, EPOCH: 6, valid_loss: 0.020194756010404\n",
      "FOLD: 0, EPOCH: 7, train_loss: 0.020278557354735362\n",
      "FOLD: 0, EPOCH: 7, valid_loss: 0.01975815385006941\n",
      "FOLD: 0, EPOCH: 8, train_loss: 0.02011041542062083\n",
      "FOLD: 0, EPOCH: 8, valid_loss: 0.01974057936324523\n",
      "FOLD: 0, EPOCH: 9, train_loss: 0.019960686388249334\n",
      "FOLD: 0, EPOCH: 9, valid_loss: 0.019704826176166534\n",
      "FOLD: 0, EPOCH: 10, train_loss: 0.01977290608290885\n",
      "FOLD: 0, EPOCH: 10, valid_loss: 0.019597730814264372\n",
      "FOLD: 0, EPOCH: 11, train_loss: 0.019722270552773733\n",
      "FOLD: 0, EPOCH: 11, valid_loss: 0.019493928752266444\n",
      "FOLD: 0, EPOCH: 12, train_loss: 0.01952328427216491\n",
      "FOLD: 0, EPOCH: 12, valid_loss: 0.019406527280807495\n",
      "FOLD: 0, EPOCH: 13, train_loss: 0.01937231907268634\n",
      "FOLD: 0, EPOCH: 13, valid_loss: 0.019322547058646496\n",
      "FOLD: 0, EPOCH: 14, train_loss: 0.019269261836401513\n",
      "FOLD: 0, EPOCH: 14, valid_loss: 0.019371450807039555\n",
      "FOLD: 0, EPOCH: 15, train_loss: 0.01916975593446074\n",
      "FOLD: 0, EPOCH: 15, valid_loss: 0.019454794004559517\n",
      "FOLD: 0, EPOCH: 16, train_loss: 0.01902227451068324\n",
      "FOLD: 0, EPOCH: 16, valid_loss: 0.019154045874109633\n",
      "FOLD: 0, EPOCH: 17, train_loss: 0.01890326089955665\n",
      "FOLD: 0, EPOCH: 17, valid_loss: 0.019124288398485918\n",
      "FOLD: 0, EPOCH: 18, train_loss: 0.01875312706908664\n",
      "FOLD: 0, EPOCH: 18, valid_loss: 0.01913272259900203\n",
      "FOLD: 0, EPOCH: 19, train_loss: 0.018699815094068244\n",
      "FOLD: 0, EPOCH: 19, valid_loss: 0.01913717561043226\n",
      "FOLD: 0, EPOCH: 20, train_loss: 0.01859621940231001\n",
      "FOLD: 0, EPOCH: 20, valid_loss: 0.019090168751203097\n",
      "FOLD: 0, EPOCH: 21, train_loss: 0.01850852669795623\n",
      "FOLD: 0, EPOCH: 21, valid_loss: 0.019088219564694624\n",
      "FOLD: 0, EPOCH: 22, train_loss: 0.018367788477523905\n",
      "FOLD: 0, EPOCH: 22, valid_loss: 0.019031450582238343\n",
      "FOLD: 0, EPOCH: 23, train_loss: 0.018295609090175177\n",
      "FOLD: 0, EPOCH: 23, valid_loss: 0.01897766713339549\n",
      "FOLD: 0, EPOCH: 24, train_loss: 0.01815584676994665\n",
      "FOLD: 0, EPOCH: 24, valid_loss: 0.018948840263944406\n",
      "FOLD: 0, EPOCH: 25, train_loss: 0.01801419892423862\n",
      "FOLD: 0, EPOCH: 25, valid_loss: 0.018956592879616298\n",
      "FOLD: 0, EPOCH: 26, train_loss: 0.017942725429059687\n",
      "FOLD: 0, EPOCH: 26, valid_loss: 0.018929085622613247\n",
      "FOLD: 0, EPOCH: 27, train_loss: 0.01778907228160549\n",
      "FOLD: 0, EPOCH: 27, valid_loss: 0.018914241916858233\n",
      "FOLD: 0, EPOCH: 28, train_loss: 0.01765038220312547\n",
      "FOLD: 0, EPOCH: 28, valid_loss: 0.01885228116924946\n",
      "FOLD: 0, EPOCH: 29, train_loss: 0.01763161168609922\n",
      "FOLD: 0, EPOCH: 29, valid_loss: 0.01892086462332652\n",
      "FOLD: 0, EPOCH: 30, train_loss: 0.017426955916390225\n",
      "FOLD: 0, EPOCH: 30, valid_loss: 0.01887903606089262\n",
      "FOLD: 0, EPOCH: 31, train_loss: 0.017418367486145045\n",
      "FOLD: 0, EPOCH: 31, valid_loss: 0.018865649946607076\n",
      "Epoch    32: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 0, EPOCH: 32, train_loss: 0.01703630046717621\n",
      "FOLD: 0, EPOCH: 32, valid_loss: 0.018813376816419456\n",
      "FOLD: 0, EPOCH: 33, train_loss: 0.016918488186658233\n",
      "FOLD: 0, EPOCH: 33, valid_loss: 0.018769256197489224\n",
      "FOLD: 0, EPOCH: 34, train_loss: 0.01682912412326078\n",
      "FOLD: 0, EPOCH: 34, valid_loss: 0.018744450349074144\n",
      "FOLD: 0, EPOCH: 35, train_loss: 0.0167734554616382\n",
      "FOLD: 0, EPOCH: 35, valid_loss: 0.018768190334622677\n",
      "FOLD: 0, EPOCH: 36, train_loss: 0.016769668729220692\n",
      "FOLD: 0, EPOCH: 36, valid_loss: 0.018739932431624487\n",
      "FOLD: 0, EPOCH: 37, train_loss: 0.016678660277377914\n",
      "FOLD: 0, EPOCH: 37, valid_loss: 0.018722410671986066\n",
      "FOLD: 0, EPOCH: 38, train_loss: 0.016655328419260883\n",
      "FOLD: 0, EPOCH: 38, valid_loss: 0.01874481327831745\n",
      "FOLD: 0, EPOCH: 39, train_loss: 0.016590845582352298\n",
      "FOLD: 0, EPOCH: 39, valid_loss: 0.01871788014586155\n",
      "FOLD: 0, EPOCH: 40, train_loss: 0.016545318054488382\n",
      "FOLD: 0, EPOCH: 40, valid_loss: 0.018737357396345872\n",
      "FOLD: 0, EPOCH: 41, train_loss: 0.016544424211354675\n",
      "FOLD: 0, EPOCH: 41, valid_loss: 0.01875123782799794\n",
      "FOLD: 0, EPOCH: 42, train_loss: 0.01646477679998891\n",
      "FOLD: 0, EPOCH: 42, valid_loss: 0.018719361521876775\n",
      "Epoch    43: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 0, EPOCH: 43, train_loss: 0.01642450865207089\n",
      "FOLD: 0, EPOCH: 43, valid_loss: 0.01872165300525152\n",
      "FOLD: 0, EPOCH: 44, train_loss: 0.016471918433199863\n",
      "FOLD: 0, EPOCH: 44, valid_loss: 0.01873056017435514\n",
      "FOLD: 0, EPOCH: 45, train_loss: 0.016489141079521662\n",
      "FOLD: 0, EPOCH: 45, valid_loss: 0.018741792927567776\n",
      "Epoch    46: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 0, EPOCH: 46, train_loss: 0.016425111091922264\n",
      "FOLD: 0, EPOCH: 46, valid_loss: 0.018739231790487584\n",
      "FOLD: 0, EPOCH: 47, train_loss: 0.016438294441212673\n",
      "FOLD: 0, EPOCH: 47, valid_loss: 0.018764082772227433\n",
      "FOLD: 0, EPOCH: 48, train_loss: 0.016432566039666936\n",
      "FOLD: 0, EPOCH: 48, valid_loss: 0.018735840630072814\n",
      "Epoch    49: reducing learning rate of group 0 to 8.9345e-05.\n",
      "FOLD: 0, EPOCH: 49, train_loss: 0.016468299112307863\n",
      "FOLD: 0, EPOCH: 49, valid_loss: 0.018735247305952586\n",
      "FOLD: 1, EPOCH: 0, train_loss: 0.0496178696284423\n",
      "FOLD: 1, EPOCH: 0, valid_loss: 0.02290959985783467\n",
      "FOLD: 1, EPOCH: 1, train_loss: 0.022335170894055754\n",
      "FOLD: 1, EPOCH: 1, valid_loss: 0.024822006861750897\n",
      "FOLD: 1, EPOCH: 2, train_loss: 0.02169436719771978\n",
      "FOLD: 1, EPOCH: 2, valid_loss: 0.021693014468138035\n",
      "FOLD: 1, EPOCH: 3, train_loss: 0.021298975281014636\n",
      "FOLD: 1, EPOCH: 3, valid_loss: 0.023362697603610847\n",
      "FOLD: 1, EPOCH: 4, train_loss: 0.020932170512104355\n",
      "FOLD: 1, EPOCH: 4, valid_loss: 0.020986885978625372\n",
      "FOLD: 1, EPOCH: 5, train_loss: 0.020673944514144112\n",
      "FOLD: 1, EPOCH: 5, valid_loss: 0.021951577267967738\n",
      "FOLD: 1, EPOCH: 6, train_loss: 0.020463357352324435\n",
      "FOLD: 1, EPOCH: 6, valid_loss: 0.02101430368538086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 1, EPOCH: 7, train_loss: 0.020299399548487085\n",
      "FOLD: 1, EPOCH: 7, valid_loss: 0.020404616943918742\n",
      "FOLD: 1, EPOCH: 8, train_loss: 0.020119524435014337\n",
      "FOLD: 1, EPOCH: 8, valid_loss: 0.020245074222867306\n",
      "FOLD: 1, EPOCH: 9, train_loss: 0.020010655745863914\n",
      "FOLD: 1, EPOCH: 9, valid_loss: 0.02001202550645058\n",
      "FOLD: 1, EPOCH: 10, train_loss: 0.01981890735191268\n",
      "FOLD: 1, EPOCH: 10, valid_loss: 0.019879810655346282\n",
      "FOLD: 1, EPOCH: 11, train_loss: 0.019640007106637634\n",
      "FOLD: 1, EPOCH: 11, valid_loss: 0.019807045992750388\n",
      "FOLD: 1, EPOCH: 12, train_loss: 0.019436009530280088\n",
      "FOLD: 1, EPOCH: 12, valid_loss: 0.019735825033142015\n",
      "FOLD: 1, EPOCH: 13, train_loss: 0.019363631267805357\n",
      "FOLD: 1, EPOCH: 13, valid_loss: 0.019983963611034248\n",
      "FOLD: 1, EPOCH: 14, train_loss: 0.01918348204344511\n",
      "FOLD: 1, EPOCH: 14, valid_loss: 0.019585331042225543\n",
      "FOLD: 1, EPOCH: 15, train_loss: 0.019029874430113548\n",
      "FOLD: 1, EPOCH: 15, valid_loss: 0.019498771343093652\n",
      "FOLD: 1, EPOCH: 16, train_loss: 0.018917202999865688\n",
      "FOLD: 1, EPOCH: 16, valid_loss: 0.019480244614757024\n",
      "FOLD: 1, EPOCH: 17, train_loss: 0.018852234239111077\n",
      "FOLD: 1, EPOCH: 17, valid_loss: 0.019468244308462508\n",
      "FOLD: 1, EPOCH: 18, train_loss: 0.01865437566428571\n",
      "FOLD: 1, EPOCH: 18, valid_loss: 0.019444028918559734\n",
      "FOLD: 1, EPOCH: 19, train_loss: 0.018515114697652893\n",
      "FOLD: 1, EPOCH: 19, valid_loss: 0.019292964241825618\n",
      "FOLD: 1, EPOCH: 20, train_loss: 0.018415512189873168\n",
      "FOLD: 1, EPOCH: 20, valid_loss: 0.019278460683730934\n",
      "FOLD: 1, EPOCH: 21, train_loss: 0.01826126402797731\n",
      "FOLD: 1, EPOCH: 21, valid_loss: 0.019324046058150437\n",
      "FOLD: 1, EPOCH: 22, train_loss: 0.01818685996270663\n",
      "FOLD: 1, EPOCH: 22, valid_loss: 0.019317557891974084\n",
      "FOLD: 1, EPOCH: 23, train_loss: 0.018008085861298685\n",
      "FOLD: 1, EPOCH: 23, valid_loss: 0.019280410300080594\n",
      "Epoch    24: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 1, EPOCH: 24, train_loss: 0.017644402117946663\n",
      "FOLD: 1, EPOCH: 24, valid_loss: 0.019100199095331706\n",
      "FOLD: 1, EPOCH: 25, train_loss: 0.017486375026606226\n",
      "FOLD: 1, EPOCH: 25, valid_loss: 0.01908395253121853\n",
      "FOLD: 1, EPOCH: 26, train_loss: 0.01737922350750179\n",
      "FOLD: 1, EPOCH: 26, valid_loss: 0.01904414164332243\n",
      "FOLD: 1, EPOCH: 27, train_loss: 0.017368157331303164\n",
      "FOLD: 1, EPOCH: 27, valid_loss: 0.019039821166258592\n",
      "FOLD: 1, EPOCH: 28, train_loss: 0.017247935757040977\n",
      "FOLD: 1, EPOCH: 28, valid_loss: 0.019007507998209733\n",
      "FOLD: 1, EPOCH: 29, train_loss: 0.017229115288402583\n",
      "FOLD: 1, EPOCH: 29, valid_loss: 0.019036353637392704\n",
      "FOLD: 1, EPOCH: 30, train_loss: 0.01719555865369133\n",
      "FOLD: 1, EPOCH: 30, valid_loss: 0.019010531930969313\n",
      "FOLD: 1, EPOCH: 31, train_loss: 0.01712525696367831\n",
      "FOLD: 1, EPOCH: 31, valid_loss: 0.01901921257376671\n",
      "Epoch    32: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 1, EPOCH: 32, train_loss: 0.017035822082008864\n",
      "FOLD: 1, EPOCH: 32, valid_loss: 0.01899679177082502\n",
      "FOLD: 1, EPOCH: 33, train_loss: 0.017050541232566576\n",
      "FOLD: 1, EPOCH: 33, valid_loss: 0.019012056577664155\n",
      "FOLD: 1, EPOCH: 34, train_loss: 0.017005217714688263\n",
      "FOLD: 1, EPOCH: 34, valid_loss: 0.019013434935074586\n",
      "FOLD: 1, EPOCH: 35, train_loss: 0.017052702668651536\n",
      "FOLD: 1, EPOCH: 35, valid_loss: 0.018992354090397175\n",
      "FOLD: 1, EPOCH: 36, train_loss: 0.0170028546472659\n",
      "FOLD: 1, EPOCH: 36, valid_loss: 0.018992182010641463\n",
      "FOLD: 1, EPOCH: 37, train_loss: 0.017052651735375058\n",
      "FOLD: 1, EPOCH: 37, valid_loss: 0.018991805613040924\n",
      "FOLD: 1, EPOCH: 38, train_loss: 0.017032300638085283\n",
      "FOLD: 1, EPOCH: 38, valid_loss: 0.019003713646760352\n",
      "FOLD: 1, EPOCH: 39, train_loss: 0.016971685697098036\n",
      "FOLD: 1, EPOCH: 39, valid_loss: 0.019012931877603896\n",
      "FOLD: 1, EPOCH: 40, train_loss: 0.016991271863918047\n",
      "FOLD: 1, EPOCH: 40, valid_loss: 0.018989324283141356\n",
      "FOLD: 1, EPOCH: 41, train_loss: 0.016988042452548806\n",
      "FOLD: 1, EPOCH: 41, valid_loss: 0.01900596744739092\n",
      "FOLD: 1, EPOCH: 42, train_loss: 0.016958364688262745\n",
      "FOLD: 1, EPOCH: 42, valid_loss: 0.01898802129121927\n",
      "FOLD: 1, EPOCH: 43, train_loss: 0.016977602150291204\n",
      "FOLD: 1, EPOCH: 43, valid_loss: 0.019037058290380698\n",
      "FOLD: 1, EPOCH: 44, train_loss: 0.016977980707746906\n",
      "FOLD: 1, EPOCH: 44, valid_loss: 0.018986883071752694\n",
      "FOLD: 1, EPOCH: 45, train_loss: 0.016959146395124292\n",
      "FOLD: 1, EPOCH: 45, valid_loss: 0.01899355320403209\n",
      "FOLD: 1, EPOCH: 46, train_loss: 0.016947491207738984\n",
      "FOLD: 1, EPOCH: 46, valid_loss: 0.019000133929344323\n",
      "FOLD: 1, EPOCH: 47, train_loss: 0.016974877578684607\n",
      "FOLD: 1, EPOCH: 47, valid_loss: 0.019016552573213212\n",
      "Epoch    48: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 1, EPOCH: 48, train_loss: 0.01694944517284229\n",
      "FOLD: 1, EPOCH: 48, valid_loss: 0.019004922216901414\n",
      "FOLD: 1, EPOCH: 49, train_loss: 0.0169423600105015\n",
      "FOLD: 1, EPOCH: 49, valid_loss: 0.01897708885371685\n",
      "FOLD: 2, EPOCH: 0, train_loss: 0.04880431422812713\n",
      "FOLD: 2, EPOCH: 0, valid_loss: 0.02377064569065204\n",
      "FOLD: 2, EPOCH: 1, train_loss: 0.022456268805104332\n",
      "FOLD: 2, EPOCH: 1, valid_loss: 0.02324288710951805\n",
      "FOLD: 2, EPOCH: 2, train_loss: 0.02176263918345039\n",
      "FOLD: 2, EPOCH: 2, valid_loss: 0.02149141622850528\n",
      "FOLD: 2, EPOCH: 3, train_loss: 0.021283581148128252\n",
      "FOLD: 2, EPOCH: 3, valid_loss: 0.021292047001994573\n",
      "FOLD: 2, EPOCH: 4, train_loss: 0.021016805558591277\n",
      "FOLD: 2, EPOCH: 4, valid_loss: 0.021039712600983106\n",
      "FOLD: 2, EPOCH: 5, train_loss: 0.020697879730849654\n",
      "FOLD: 2, EPOCH: 5, valid_loss: 0.02108144201338291\n",
      "FOLD: 2, EPOCH: 6, train_loss: 0.020466828794294113\n",
      "FOLD: 2, EPOCH: 6, valid_loss: 0.020617358529796966\n",
      "FOLD: 2, EPOCH: 7, train_loss: 0.02026982665867419\n",
      "FOLD: 2, EPOCH: 7, valid_loss: 0.020456557377026632\n",
      "FOLD: 2, EPOCH: 8, train_loss: 0.020118397056452325\n",
      "FOLD: 2, EPOCH: 8, valid_loss: 0.02022958417924551\n",
      "FOLD: 2, EPOCH: 9, train_loss: 0.019943059900322475\n",
      "FOLD: 2, EPOCH: 9, valid_loss: 0.020189412797872838\n",
      "FOLD: 2, EPOCH: 10, train_loss: 0.019820017840813945\n",
      "FOLD: 2, EPOCH: 10, valid_loss: 0.02000506164935919\n",
      "FOLD: 2, EPOCH: 11, train_loss: 0.019639389715283305\n",
      "FOLD: 2, EPOCH: 11, valid_loss: 0.020014302231944524\n",
      "FOLD: 2, EPOCH: 12, train_loss: 0.019453379677961003\n",
      "FOLD: 2, EPOCH: 12, valid_loss: 0.019835237986766376\n",
      "FOLD: 2, EPOCH: 13, train_loss: 0.019360040943767573\n",
      "FOLD: 2, EPOCH: 13, valid_loss: 0.020580057341318864\n",
      "FOLD: 2, EPOCH: 14, train_loss: 0.01916510966681951\n",
      "FOLD: 2, EPOCH: 14, valid_loss: 0.019700812462430734\n",
      "FOLD: 2, EPOCH: 15, train_loss: 0.019050098124992202\n",
      "FOLD: 2, EPOCH: 15, valid_loss: 0.0195998612504739\n",
      "FOLD: 2, EPOCH: 16, train_loss: 0.01893204465709828\n",
      "FOLD: 2, EPOCH: 16, valid_loss: 0.019565239262122374\n",
      "FOLD: 2, EPOCH: 17, train_loss: 0.018791438457933633\n",
      "FOLD: 2, EPOCH: 17, valid_loss: 0.01973364674128019\n",
      "FOLD: 2, EPOCH: 18, train_loss: 0.018699925342524373\n",
      "FOLD: 2, EPOCH: 18, valid_loss: 0.019512818123285588\n",
      "FOLD: 2, EPOCH: 19, train_loss: 0.018522032486224495\n",
      "FOLD: 2, EPOCH: 19, valid_loss: 0.019472310319542885\n",
      "FOLD: 2, EPOCH: 20, train_loss: 0.018432896055683896\n",
      "FOLD: 2, EPOCH: 20, valid_loss: 0.019460460027823083\n",
      "FOLD: 2, EPOCH: 21, train_loss: 0.01829277693822577\n",
      "FOLD: 2, EPOCH: 21, valid_loss: 0.01941066149335641\n",
      "FOLD: 2, EPOCH: 22, train_loss: 0.01823659641416492\n",
      "FOLD: 2, EPOCH: 22, valid_loss: 0.01940861215385107\n",
      "FOLD: 2, EPOCH: 23, train_loss: 0.018041605532572075\n",
      "FOLD: 2, EPOCH: 23, valid_loss: 0.019414004798118886\n",
      "FOLD: 2, EPOCH: 24, train_loss: 0.017910663114004844\n",
      "FOLD: 2, EPOCH: 24, valid_loss: 0.019378982484340668\n",
      "FOLD: 2, EPOCH: 25, train_loss: 0.017779038916971232\n",
      "FOLD: 2, EPOCH: 25, valid_loss: 0.019366715246668227\n",
      "FOLD: 2, EPOCH: 26, train_loss: 0.01766454881510219\n",
      "FOLD: 2, EPOCH: 26, valid_loss: 0.01935079822746607\n",
      "FOLD: 2, EPOCH: 27, train_loss: 0.017565470138514363\n",
      "FOLD: 2, EPOCH: 27, valid_loss: 0.019351735424536925\n",
      "FOLD: 2, EPOCH: 28, train_loss: 0.01738193940774009\n",
      "FOLD: 2, EPOCH: 28, valid_loss: 0.0193123841801515\n",
      "FOLD: 2, EPOCH: 29, train_loss: 0.017335266378280277\n",
      "FOLD: 2, EPOCH: 29, valid_loss: 0.019318849134903688\n",
      "FOLD: 2, EPOCH: 30, train_loss: 0.017143403570092208\n",
      "FOLD: 2, EPOCH: 30, valid_loss: 0.01931503816292836\n",
      "FOLD: 2, EPOCH: 31, train_loss: 0.017049747179388196\n",
      "FOLD: 2, EPOCH: 31, valid_loss: 0.019283877399105292\n",
      "FOLD: 2, EPOCH: 32, train_loss: 0.016910511660515458\n",
      "FOLD: 2, EPOCH: 32, valid_loss: 0.019310825289441988\n",
      "FOLD: 2, EPOCH: 33, train_loss: 0.016794926977741556\n",
      "FOLD: 2, EPOCH: 33, valid_loss: 0.019305281341075897\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 2, EPOCH: 34, train_loss: 0.016631914516659203\n",
      "FOLD: 2, EPOCH: 34, valid_loss: 0.01932168551362478\n",
      "Epoch    35: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 2, EPOCH: 35, train_loss: 0.016293211047210404\n",
      "FOLD: 2, EPOCH: 35, valid_loss: 0.019218263144676503\n",
      "FOLD: 2, EPOCH: 36, train_loss: 0.01614680450812385\n",
      "FOLD: 2, EPOCH: 36, valid_loss: 0.019188737926574852\n",
      "FOLD: 2, EPOCH: 37, train_loss: 0.01610965035050302\n",
      "FOLD: 2, EPOCH: 37, valid_loss: 0.01916800969495223\n",
      "FOLD: 2, EPOCH: 38, train_loss: 0.016009325476212276\n",
      "FOLD: 2, EPOCH: 38, valid_loss: 0.01919376148054233\n",
      "FOLD: 2, EPOCH: 39, train_loss: 0.01597806638912172\n",
      "FOLD: 2, EPOCH: 39, valid_loss: 0.019208112731575966\n",
      "FOLD: 2, EPOCH: 40, train_loss: 0.015931327487467915\n",
      "FOLD: 2, EPOCH: 40, valid_loss: 0.01916143326805188\n",
      "FOLD: 2, EPOCH: 41, train_loss: 0.01581421897218034\n",
      "FOLD: 2, EPOCH: 41, valid_loss: 0.01916903228713916\n",
      "FOLD: 2, EPOCH: 42, train_loss: 0.0157697565375349\n",
      "FOLD: 2, EPOCH: 42, valid_loss: 0.01919498839057409\n",
      "FOLD: 2, EPOCH: 43, train_loss: 0.015729831405789464\n",
      "FOLD: 2, EPOCH: 43, valid_loss: 0.019195672124624252\n",
      "Epoch    44: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 2, EPOCH: 44, train_loss: 0.015672959085251834\n",
      "FOLD: 2, EPOCH: 44, valid_loss: 0.019176891503425744\n",
      "FOLD: 2, EPOCH: 45, train_loss: 0.015687602244921633\n",
      "FOLD: 2, EPOCH: 45, valid_loss: 0.019180315331770822\n",
      "FOLD: 2, EPOCH: 46, train_loss: 0.015685198249647748\n",
      "FOLD: 2, EPOCH: 46, valid_loss: 0.019170844354308568\n",
      "Epoch    47: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 2, EPOCH: 47, train_loss: 0.015674197870130475\n",
      "FOLD: 2, EPOCH: 47, valid_loss: 0.019178542380149547\n",
      "FOLD: 2, EPOCH: 48, train_loss: 0.015685564976789662\n",
      "FOLD: 2, EPOCH: 48, valid_loss: 0.019199082627892494\n",
      "FOLD: 2, EPOCH: 49, train_loss: 0.015650268316873023\n",
      "FOLD: 2, EPOCH: 49, valid_loss: 0.019165905049213998\n",
      "Epoch    50: reducing learning rate of group 0 to 8.9345e-05.\n",
      "FOLD: 3, EPOCH: 0, train_loss: 0.048427870270569585\n",
      "FOLD: 3, EPOCH: 0, valid_loss: 0.02257790373494992\n",
      "FOLD: 3, EPOCH: 1, train_loss: 0.022416213376296533\n",
      "FOLD: 3, EPOCH: 1, valid_loss: 0.022167836101009294\n",
      "FOLD: 3, EPOCH: 2, train_loss: 0.021708906099602982\n",
      "FOLD: 3, EPOCH: 2, valid_loss: 0.021314875294382755\n",
      "FOLD: 3, EPOCH: 3, train_loss: 0.02127441846941774\n",
      "FOLD: 3, EPOCH: 3, valid_loss: 0.021086812305908937\n",
      "FOLD: 3, EPOCH: 4, train_loss: 0.02095867524779326\n",
      "FOLD: 3, EPOCH: 4, valid_loss: 0.020958624207056485\n",
      "FOLD: 3, EPOCH: 5, train_loss: 0.02071436431661651\n",
      "FOLD: 3, EPOCH: 5, valid_loss: 0.021059125661849976\n",
      "FOLD: 3, EPOCH: 6, train_loss: 0.020485344015665957\n",
      "FOLD: 3, EPOCH: 6, valid_loss: 0.02036834780413371\n",
      "FOLD: 3, EPOCH: 7, train_loss: 0.02029379369137255\n",
      "FOLD: 3, EPOCH: 7, valid_loss: 0.020138484640763357\n",
      "FOLD: 3, EPOCH: 8, train_loss: 0.020116529981228145\n",
      "FOLD: 3, EPOCH: 8, valid_loss: 0.020007855617083035\n",
      "FOLD: 3, EPOCH: 9, train_loss: 0.01990979122048294\n",
      "FOLD: 3, EPOCH: 9, valid_loss: 0.019911169719237547\n",
      "FOLD: 3, EPOCH: 10, train_loss: 0.019806691848144337\n",
      "FOLD: 3, EPOCH: 10, valid_loss: 0.019682996404858735\n",
      "FOLD: 3, EPOCH: 11, train_loss: 0.01961225487694547\n",
      "FOLD: 3, EPOCH: 11, valid_loss: 0.01964061248760957\n",
      "FOLD: 3, EPOCH: 12, train_loss: 0.019478791393339634\n",
      "FOLD: 3, EPOCH: 12, valid_loss: 0.019666185459265344\n",
      "FOLD: 3, EPOCH: 13, train_loss: 0.019290150344573164\n",
      "FOLD: 3, EPOCH: 13, valid_loss: 0.019583351766833894\n",
      "FOLD: 3, EPOCH: 14, train_loss: 0.019152661659628957\n",
      "FOLD: 3, EPOCH: 14, valid_loss: 0.019500928572737254\n",
      "FOLD: 3, EPOCH: 15, train_loss: 0.0190601811266026\n",
      "FOLD: 3, EPOCH: 15, valid_loss: 0.019413824694660995\n",
      "FOLD: 3, EPOCH: 16, train_loss: 0.01894426509435918\n",
      "FOLD: 3, EPOCH: 16, valid_loss: 0.019593492723428287\n",
      "FOLD: 3, EPOCH: 17, train_loss: 0.018792942216670192\n",
      "FOLD: 3, EPOCH: 17, valid_loss: 0.01933083454003701\n",
      "FOLD: 3, EPOCH: 18, train_loss: 0.018712376898809058\n",
      "FOLD: 3, EPOCH: 18, valid_loss: 0.02003558653478439\n",
      "FOLD: 3, EPOCH: 19, train_loss: 0.018552947386696533\n",
      "FOLD: 3, EPOCH: 19, valid_loss: 0.019216841373306055\n",
      "FOLD: 3, EPOCH: 20, train_loss: 0.01843325816396926\n",
      "FOLD: 3, EPOCH: 20, valid_loss: 0.019272542486970242\n",
      "FOLD: 3, EPOCH: 21, train_loss: 0.01827672780868975\n",
      "FOLD: 3, EPOCH: 21, valid_loss: 0.01912898699251505\n",
      "FOLD: 3, EPOCH: 22, train_loss: 0.01819515150241755\n",
      "FOLD: 3, EPOCH: 22, valid_loss: 0.019151917873666838\n",
      "FOLD: 3, EPOCH: 23, train_loss: 0.018061159606519585\n",
      "FOLD: 3, EPOCH: 23, valid_loss: 0.019164843054918144\n",
      "FOLD: 3, EPOCH: 24, train_loss: 0.01792676155329556\n",
      "FOLD: 3, EPOCH: 24, valid_loss: 0.01910997167802774\n",
      "FOLD: 3, EPOCH: 25, train_loss: 0.01782592265187083\n",
      "FOLD: 3, EPOCH: 25, valid_loss: 0.01913105108990119\n",
      "FOLD: 3, EPOCH: 26, train_loss: 0.017666733483909756\n",
      "FOLD: 3, EPOCH: 26, valid_loss: 0.0191271067238771\n",
      "FOLD: 3, EPOCH: 27, train_loss: 0.01751688137851857\n",
      "FOLD: 3, EPOCH: 27, valid_loss: 0.019103539391205862\n",
      "FOLD: 3, EPOCH: 28, train_loss: 0.017388739321078803\n",
      "FOLD: 3, EPOCH: 28, valid_loss: 0.019082747113246184\n",
      "FOLD: 3, EPOCH: 29, train_loss: 0.01732925246343822\n",
      "FOLD: 3, EPOCH: 29, valid_loss: 0.01907250886926284\n",
      "FOLD: 3, EPOCH: 30, train_loss: 0.017159539816045278\n",
      "FOLD: 3, EPOCH: 30, valid_loss: 0.019063247224459283\n",
      "FOLD: 3, EPOCH: 31, train_loss: 0.01702220019305477\n",
      "FOLD: 3, EPOCH: 31, valid_loss: 0.019134312868118286\n",
      "FOLD: 3, EPOCH: 32, train_loss: 0.016883115611366323\n",
      "FOLD: 3, EPOCH: 32, valid_loss: 0.019093306591877572\n",
      "FOLD: 3, EPOCH: 33, train_loss: 0.01677093395610919\n",
      "FOLD: 3, EPOCH: 33, valid_loss: 0.01905086149389927\n",
      "FOLD: 3, EPOCH: 34, train_loss: 0.01659586573825092\n",
      "FOLD: 3, EPOCH: 34, valid_loss: 0.01910523081628176\n",
      "FOLD: 3, EPOCH: 35, train_loss: 0.016494434905817378\n",
      "FOLD: 3, EPOCH: 35, valid_loss: 0.01905135839031293\n",
      "FOLD: 3, EPOCH: 36, train_loss: 0.016370156175784162\n",
      "FOLD: 3, EPOCH: 36, valid_loss: 0.019061566688693486\n",
      "Epoch    37: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 3, EPOCH: 37, train_loss: 0.016040178431147658\n",
      "FOLD: 3, EPOCH: 37, valid_loss: 0.0190029892210777\n",
      "FOLD: 3, EPOCH: 38, train_loss: 0.01592895292953865\n",
      "FOLD: 3, EPOCH: 38, valid_loss: 0.018978374222150214\n",
      "FOLD: 3, EPOCH: 39, train_loss: 0.01582049938371858\n",
      "FOLD: 3, EPOCH: 39, valid_loss: 0.0189969502389431\n",
      "FOLD: 3, EPOCH: 40, train_loss: 0.015755279870653473\n",
      "FOLD: 3, EPOCH: 40, valid_loss: 0.018957123303642638\n",
      "FOLD: 3, EPOCH: 41, train_loss: 0.015750450409344724\n",
      "FOLD: 3, EPOCH: 41, valid_loss: 0.018948029870024093\n",
      "FOLD: 3, EPOCH: 42, train_loss: 0.015669348901389418\n",
      "FOLD: 3, EPOCH: 42, valid_loss: 0.018965915991709784\n",
      "FOLD: 3, EPOCH: 43, train_loss: 0.015611628573891279\n",
      "FOLD: 3, EPOCH: 43, valid_loss: 0.018984489429455537\n",
      "FOLD: 3, EPOCH: 44, train_loss: 0.015596570912748575\n",
      "FOLD: 3, EPOCH: 44, valid_loss: 0.018940046143073302\n",
      "FOLD: 3, EPOCH: 45, train_loss: 0.01553967293049838\n",
      "FOLD: 3, EPOCH: 45, valid_loss: 0.0189297220741327\n",
      "FOLD: 3, EPOCH: 46, train_loss: 0.015467550895000631\n",
      "FOLD: 3, EPOCH: 46, valid_loss: 0.018941117593875297\n",
      "FOLD: 3, EPOCH: 47, train_loss: 0.015443339465639076\n",
      "FOLD: 3, EPOCH: 47, valid_loss: 0.01895652224238102\n",
      "FOLD: 3, EPOCH: 48, train_loss: 0.015409990956352369\n",
      "FOLD: 3, EPOCH: 48, valid_loss: 0.01897133872486078\n",
      "Epoch    49: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 3, EPOCH: 49, train_loss: 0.015359690438049871\n",
      "FOLD: 3, EPOCH: 49, valid_loss: 0.01896446370161497\n",
      "FOLD: 4, EPOCH: 0, train_loss: 0.049605095381470954\n",
      "FOLD: 4, EPOCH: 0, valid_loss: 0.023205559127605878\n",
      "FOLD: 4, EPOCH: 1, train_loss: 0.02236687170492636\n",
      "FOLD: 4, EPOCH: 1, valid_loss: 0.028129527488580115\n",
      "FOLD: 4, EPOCH: 2, train_loss: 0.02170769981033093\n",
      "FOLD: 4, EPOCH: 2, valid_loss: 0.022033700719475746\n",
      "FOLD: 4, EPOCH: 3, train_loss: 0.021222716172200604\n",
      "FOLD: 4, EPOCH: 3, valid_loss: 0.02127693650814203\n",
      "FOLD: 4, EPOCH: 4, train_loss: 0.020875612349324935\n",
      "FOLD: 4, EPOCH: 4, valid_loss: 0.021020062984182283\n",
      "FOLD: 4, EPOCH: 5, train_loss: 0.02063895553048398\n",
      "FOLD: 4, EPOCH: 5, valid_loss: 0.020760879636957094\n",
      "FOLD: 4, EPOCH: 6, train_loss: 0.020459617563598865\n",
      "FOLD: 4, EPOCH: 6, valid_loss: 0.020686074948081605\n",
      "FOLD: 4, EPOCH: 7, train_loss: 0.020274821543008893\n",
      "FOLD: 4, EPOCH: 7, valid_loss: 0.020548864339406673\n",
      "FOLD: 4, EPOCH: 8, train_loss: 0.02004535506303246\n",
      "FOLD: 4, EPOCH: 8, valid_loss: 0.020268294530419204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 4, EPOCH: 9, train_loss: 0.019911694239724328\n",
      "FOLD: 4, EPOCH: 9, valid_loss: 0.020488454602085628\n",
      "FOLD: 4, EPOCH: 10, train_loss: 0.01977526645704701\n",
      "FOLD: 4, EPOCH: 10, valid_loss: 0.020118279812427666\n",
      "FOLD: 4, EPOCH: 11, train_loss: 0.01958576501724688\n",
      "FOLD: 4, EPOCH: 11, valid_loss: 0.02009783685207367\n",
      "FOLD: 4, EPOCH: 12, train_loss: 0.019454004444383288\n",
      "FOLD: 4, EPOCH: 12, valid_loss: 0.020021661256368343\n",
      "FOLD: 4, EPOCH: 13, train_loss: 0.019264971585692587\n",
      "FOLD: 4, EPOCH: 13, valid_loss: 0.01993207074701786\n",
      "FOLD: 4, EPOCH: 14, train_loss: 0.019167912217813568\n",
      "FOLD: 4, EPOCH: 14, valid_loss: 0.01978059571522933\n",
      "FOLD: 4, EPOCH: 15, train_loss: 0.018992558373390016\n",
      "FOLD: 4, EPOCH: 15, valid_loss: 0.019878117224344842\n",
      "FOLD: 4, EPOCH: 16, train_loss: 0.018888878535378625\n",
      "FOLD: 4, EPOCH: 16, valid_loss: 0.019625589538079042\n",
      "FOLD: 4, EPOCH: 17, train_loss: 0.018778007326496614\n",
      "FOLD: 4, EPOCH: 17, valid_loss: 0.019749191088172104\n",
      "FOLD: 4, EPOCH: 18, train_loss: 0.0186510193035812\n",
      "FOLD: 4, EPOCH: 18, valid_loss: 0.019632514852743883\n",
      "FOLD: 4, EPOCH: 19, train_loss: 0.01847766201339058\n",
      "FOLD: 4, EPOCH: 19, valid_loss: 0.019560070994954843\n",
      "FOLD: 4, EPOCH: 20, train_loss: 0.018324316328240407\n",
      "FOLD: 4, EPOCH: 20, valid_loss: 0.01944807257789832\n",
      "FOLD: 4, EPOCH: 21, train_loss: 0.018285207576244265\n",
      "FOLD: 4, EPOCH: 21, valid_loss: 0.01954609900712967\n",
      "FOLD: 4, EPOCH: 22, train_loss: 0.018145968256568588\n",
      "FOLD: 4, EPOCH: 22, valid_loss: 0.01949523575603962\n",
      "FOLD: 4, EPOCH: 23, train_loss: 0.0179491061465563\n",
      "FOLD: 4, EPOCH: 23, valid_loss: 0.019453089111126386\n",
      "Epoch    24: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 4, EPOCH: 24, train_loss: 0.01757344651363186\n",
      "FOLD: 4, EPOCH: 24, valid_loss: 0.019302573484870102\n",
      "FOLD: 4, EPOCH: 25, train_loss: 0.017466464381967042\n",
      "FOLD: 4, EPOCH: 25, valid_loss: 0.019258256858358018\n",
      "FOLD: 4, EPOCH: 26, train_loss: 0.01739236309721663\n",
      "FOLD: 4, EPOCH: 26, valid_loss: 0.01924613734277395\n",
      "FOLD: 4, EPOCH: 27, train_loss: 0.017288351758710435\n",
      "FOLD: 4, EPOCH: 27, valid_loss: 0.01925531975351847\n",
      "FOLD: 4, EPOCH: 28, train_loss: 0.017224580880153825\n",
      "FOLD: 4, EPOCH: 28, valid_loss: 0.01924392466361706\n",
      "FOLD: 4, EPOCH: 29, train_loss: 0.017187439509340235\n",
      "FOLD: 4, EPOCH: 29, valid_loss: 0.019256024549786862\n",
      "FOLD: 4, EPOCH: 30, train_loss: 0.017136684354596043\n",
      "FOLD: 4, EPOCH: 30, valid_loss: 0.019261837865297612\n",
      "FOLD: 4, EPOCH: 31, train_loss: 0.017078646383172757\n",
      "FOLD: 4, EPOCH: 31, valid_loss: 0.019240340791069545\n",
      "FOLD: 4, EPOCH: 32, train_loss: 0.01703559355558576\n",
      "FOLD: 4, EPOCH: 32, valid_loss: 0.019236141815781593\n",
      "FOLD: 4, EPOCH: 33, train_loss: 0.01699065544164261\n",
      "FOLD: 4, EPOCH: 33, valid_loss: 0.019229315507870454\n",
      "FOLD: 4, EPOCH: 34, train_loss: 0.016997334673195273\n",
      "FOLD: 4, EPOCH: 34, valid_loss: 0.01924809254705906\n",
      "FOLD: 4, EPOCH: 35, train_loss: 0.016907363664358854\n",
      "FOLD: 4, EPOCH: 35, valid_loss: 0.019228494081359643\n",
      "FOLD: 4, EPOCH: 36, train_loss: 0.016891927835909096\n",
      "FOLD: 4, EPOCH: 36, valid_loss: 0.019228937104344368\n",
      "FOLD: 4, EPOCH: 37, train_loss: 0.016849784243449167\n",
      "FOLD: 4, EPOCH: 37, valid_loss: 0.019224575649087246\n",
      "FOLD: 4, EPOCH: 38, train_loss: 0.01681948293352852\n",
      "FOLD: 4, EPOCH: 38, valid_loss: 0.01922196952196268\n",
      "FOLD: 4, EPOCH: 39, train_loss: 0.016819649564756733\n",
      "FOLD: 4, EPOCH: 39, valid_loss: 0.019257654794133626\n",
      "FOLD: 4, EPOCH: 40, train_loss: 0.016756244431677704\n",
      "FOLD: 4, EPOCH: 40, valid_loss: 0.019239475664037924\n",
      "FOLD: 4, EPOCH: 41, train_loss: 0.01671385530986496\n",
      "FOLD: 4, EPOCH: 41, valid_loss: 0.01925011666921469\n",
      "Epoch    42: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 4, EPOCH: 42, train_loss: 0.01664793717901449\n",
      "FOLD: 4, EPOCH: 42, valid_loss: 0.019244701959765874\n",
      "FOLD: 4, EPOCH: 43, train_loss: 0.016658666996738396\n",
      "FOLD: 4, EPOCH: 43, valid_loss: 0.019256826633444198\n",
      "FOLD: 4, EPOCH: 44, train_loss: 0.01665361525138488\n",
      "FOLD: 4, EPOCH: 44, valid_loss: 0.01923331618309021\n",
      "Epoch    45: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 4, EPOCH: 45, train_loss: 0.01667860697803868\n",
      "FOLD: 4, EPOCH: 45, valid_loss: 0.019231334042090636\n",
      "FOLD: 4, EPOCH: 46, train_loss: 0.016635805481692422\n",
      "FOLD: 4, EPOCH: 46, valid_loss: 0.019234928803948257\n",
      "FOLD: 4, EPOCH: 47, train_loss: 0.016633442779247824\n",
      "FOLD: 4, EPOCH: 47, valid_loss: 0.019239656770458587\n",
      "Epoch    48: reducing learning rate of group 0 to 8.9345e-05.\n",
      "FOLD: 4, EPOCH: 48, train_loss: 0.016649892918664862\n",
      "FOLD: 4, EPOCH: 48, valid_loss: 0.01924343865651351\n",
      "FOLD: 4, EPOCH: 49, train_loss: 0.016668291259053593\n",
      "FOLD: 4, EPOCH: 49, valid_loss: 0.01922952842253905\n",
      "FOLD: 5, EPOCH: 0, train_loss: 0.047730210312717666\n",
      "FOLD: 5, EPOCH: 0, valid_loss: 0.023695055968486346\n",
      "FOLD: 5, EPOCH: 1, train_loss: 0.02231848343099291\n",
      "FOLD: 5, EPOCH: 1, valid_loss: 0.022340269902577765\n",
      "FOLD: 5, EPOCH: 2, train_loss: 0.021591412557943446\n",
      "FOLD: 5, EPOCH: 2, valid_loss: 0.02113427436695649\n",
      "FOLD: 5, EPOCH: 3, train_loss: 0.02115299492268949\n",
      "FOLD: 5, EPOCH: 3, valid_loss: 0.020774256581297286\n",
      "FOLD: 5, EPOCH: 4, train_loss: 0.02090572485247174\n",
      "FOLD: 5, EPOCH: 4, valid_loss: 0.020546008761112507\n",
      "FOLD: 5, EPOCH: 5, train_loss: 0.020689655850465234\n",
      "FOLD: 5, EPOCH: 5, valid_loss: 0.020335305052307937\n",
      "FOLD: 5, EPOCH: 6, train_loss: 0.020483216900076415\n",
      "FOLD: 5, EPOCH: 6, valid_loss: 0.020252528958595716\n",
      "FOLD: 5, EPOCH: 7, train_loss: 0.02023770078714635\n",
      "FOLD: 5, EPOCH: 7, valid_loss: 0.019969497592403337\n",
      "FOLD: 5, EPOCH: 8, train_loss: 0.020057846540333452\n",
      "FOLD: 5, EPOCH: 8, valid_loss: 0.01989707723259926\n",
      "FOLD: 5, EPOCH: 9, train_loss: 0.019942471002404753\n",
      "FOLD: 5, EPOCH: 9, valid_loss: 0.019709562309659444\n",
      "FOLD: 5, EPOCH: 10, train_loss: 0.019758862367755658\n",
      "FOLD: 5, EPOCH: 10, valid_loss: 0.019780282217722673\n",
      "FOLD: 5, EPOCH: 11, train_loss: 0.019645041182033113\n",
      "FOLD: 5, EPOCH: 11, valid_loss: 0.019641408696770668\n",
      "FOLD: 5, EPOCH: 12, train_loss: 0.019432580375389474\n",
      "FOLD: 5, EPOCH: 12, valid_loss: 0.019620917021082\n",
      "FOLD: 5, EPOCH: 13, train_loss: 0.019328875290985044\n",
      "FOLD: 5, EPOCH: 13, valid_loss: 0.019455454097344324\n",
      "FOLD: 5, EPOCH: 14, train_loss: 0.019231044032887834\n",
      "FOLD: 5, EPOCH: 14, valid_loss: 0.019346790531506904\n",
      "FOLD: 5, EPOCH: 15, train_loss: 0.01910477113985532\n",
      "FOLD: 5, EPOCH: 15, valid_loss: 0.01970124187377783\n",
      "FOLD: 5, EPOCH: 16, train_loss: 0.01896666647312609\n",
      "FOLD: 5, EPOCH: 16, valid_loss: 0.01923227711365773\n",
      "FOLD: 5, EPOCH: 17, train_loss: 0.01884800794760923\n",
      "FOLD: 5, EPOCH: 17, valid_loss: 0.019215259271172378\n",
      "FOLD: 5, EPOCH: 18, train_loss: 0.01872760324260673\n",
      "FOLD: 5, EPOCH: 18, valid_loss: 0.01920419358290159\n",
      "FOLD: 5, EPOCH: 19, train_loss: 0.018617102473571494\n",
      "FOLD: 5, EPOCH: 19, valid_loss: 0.019184099080470893\n",
      "FOLD: 5, EPOCH: 20, train_loss: 0.018492871366843983\n",
      "FOLD: 5, EPOCH: 20, valid_loss: 0.01911373106906047\n",
      "FOLD: 5, EPOCH: 21, train_loss: 0.01840538961963879\n",
      "FOLD: 5, EPOCH: 21, valid_loss: 0.019099570524234038\n",
      "FOLD: 5, EPOCH: 22, train_loss: 0.018245781343933697\n",
      "FOLD: 5, EPOCH: 22, valid_loss: 0.019063260549536117\n",
      "FOLD: 5, EPOCH: 23, train_loss: 0.018142614488464756\n",
      "FOLD: 5, EPOCH: 23, valid_loss: 0.019076586629335698\n",
      "FOLD: 5, EPOCH: 24, train_loss: 0.01803961509486308\n",
      "FOLD: 5, EPOCH: 24, valid_loss: 0.01907460305553216\n",
      "FOLD: 5, EPOCH: 25, train_loss: 0.01794220575106305\n",
      "FOLD: 5, EPOCH: 25, valid_loss: 0.019065402448177338\n",
      "Epoch    26: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 5, EPOCH: 26, train_loss: 0.017591103332469594\n",
      "FOLD: 5, EPOCH: 26, valid_loss: 0.018933995698507015\n",
      "FOLD: 5, EPOCH: 27, train_loss: 0.01747904934391782\n",
      "FOLD: 5, EPOCH: 27, valid_loss: 0.018914621466627486\n",
      "FOLD: 5, EPOCH: 28, train_loss: 0.017316404025296907\n",
      "FOLD: 5, EPOCH: 28, valid_loss: 0.01889115314070995\n",
      "FOLD: 5, EPOCH: 29, train_loss: 0.017315395830853564\n",
      "FOLD: 5, EPOCH: 29, valid_loss: 0.0188702058333617\n",
      "FOLD: 5, EPOCH: 30, train_loss: 0.017259318922721856\n",
      "FOLD: 5, EPOCH: 30, valid_loss: 0.01887481430402169\n",
      "FOLD: 5, EPOCH: 31, train_loss: 0.01724952127079706\n",
      "FOLD: 5, EPOCH: 31, valid_loss: 0.01887397697338691\n",
      "FOLD: 5, EPOCH: 32, train_loss: 0.017194947114566695\n",
      "FOLD: 5, EPOCH: 32, valid_loss: 0.018867973524790544\n",
      "FOLD: 5, EPOCH: 33, train_loss: 0.017169680823949544\n",
      "FOLD: 5, EPOCH: 33, valid_loss: 0.018845431363353364\n",
      "FOLD: 5, EPOCH: 34, train_loss: 0.01708081757297387\n",
      "FOLD: 5, EPOCH: 34, valid_loss: 0.018850527273920867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 5, EPOCH: 35, train_loss: 0.017095841995969013\n",
      "FOLD: 5, EPOCH: 35, valid_loss: 0.018856853246688843\n",
      "FOLD: 5, EPOCH: 36, train_loss: 0.01698720091450456\n",
      "FOLD: 5, EPOCH: 36, valid_loss: 0.0188570932413523\n",
      "Epoch    37: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 5, EPOCH: 37, train_loss: 0.016947684167708095\n",
      "FOLD: 5, EPOCH: 37, valid_loss: 0.01882612776870911\n",
      "FOLD: 5, EPOCH: 38, train_loss: 0.016953870515666297\n",
      "FOLD: 5, EPOCH: 38, valid_loss: 0.01883897973367801\n",
      "FOLD: 5, EPOCH: 39, train_loss: 0.017011959760172946\n",
      "FOLD: 5, EPOCH: 39, valid_loss: 0.018838583563382808\n",
      "FOLD: 5, EPOCH: 40, train_loss: 0.016925444927167247\n",
      "FOLD: 5, EPOCH: 40, valid_loss: 0.018828160917529695\n",
      "Epoch    41: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 5, EPOCH: 41, train_loss: 0.016939117510274455\n",
      "FOLD: 5, EPOCH: 41, valid_loss: 0.018845122307538986\n",
      "FOLD: 5, EPOCH: 42, train_loss: 0.016887817708020274\n",
      "FOLD: 5, EPOCH: 42, valid_loss: 0.01884371844621805\n",
      "FOLD: 5, EPOCH: 43, train_loss: 0.016971391764142224\n",
      "FOLD: 5, EPOCH: 43, valid_loss: 0.01886309153185441\n",
      "Epoch    44: reducing learning rate of group 0 to 8.9345e-05.\n",
      "FOLD: 5, EPOCH: 44, train_loss: 0.01696255770386071\n",
      "FOLD: 5, EPOCH: 44, valid_loss: 0.018852390778752472\n",
      "FOLD: 5, EPOCH: 45, train_loss: 0.016952162470064452\n",
      "FOLD: 5, EPOCH: 45, valid_loss: 0.018855940980406907\n",
      "FOLD: 5, EPOCH: 46, train_loss: 0.016936256990742846\n",
      "FOLD: 5, EPOCH: 46, valid_loss: 0.01884775164608772\n",
      "Epoch    47: reducing learning rate of group 0 to 1.3371e-05.\n",
      "FOLD: 5, EPOCH: 47, train_loss: 0.016929653636809136\n",
      "FOLD: 5, EPOCH: 47, valid_loss: 0.018855556272543393\n",
      "FOLD: 5, EPOCH: 48, train_loss: 0.016946254209992854\n",
      "FOLD: 5, EPOCH: 48, valid_loss: 0.018845644134741563\n",
      "FOLD: 5, EPOCH: 49, train_loss: 0.016925720711918297\n",
      "FOLD: 5, EPOCH: 49, valid_loss: 0.01883718127814623\n",
      "Epoch    50: reducing learning rate of group 0 to 2.0012e-06.\n",
      "FOLD: 6, EPOCH: 0, train_loss: 0.04647395596210215\n",
      "FOLD: 6, EPOCH: 0, valid_loss: 0.024235442853890933\n",
      "FOLD: 6, EPOCH: 1, train_loss: 0.02227882091962808\n",
      "FOLD: 6, EPOCH: 1, valid_loss: 0.021381677916416757\n",
      "FOLD: 6, EPOCH: 2, train_loss: 0.02161180623177741\n",
      "FOLD: 6, EPOCH: 2, valid_loss: 0.021099029968564328\n",
      "FOLD: 6, EPOCH: 3, train_loss: 0.021100247152955144\n",
      "FOLD: 6, EPOCH: 3, valid_loss: 0.02089007943868637\n",
      "FOLD: 6, EPOCH: 4, train_loss: 0.020891838983909505\n",
      "FOLD: 6, EPOCH: 4, valid_loss: 0.02068292650465782\n",
      "FOLD: 6, EPOCH: 5, train_loss: 0.020623348602974736\n",
      "FOLD: 6, EPOCH: 5, valid_loss: 0.02029430651320861\n",
      "FOLD: 6, EPOCH: 6, train_loss: 0.02042475883924478\n",
      "FOLD: 6, EPOCH: 6, valid_loss: 0.020133600928462468\n",
      "FOLD: 6, EPOCH: 7, train_loss: 0.02022197120193694\n",
      "FOLD: 6, EPOCH: 7, valid_loss: 0.019912230424009837\n",
      "FOLD: 6, EPOCH: 8, train_loss: 0.02007130943742153\n",
      "FOLD: 6, EPOCH: 8, valid_loss: 0.019782866566227034\n",
      "FOLD: 6, EPOCH: 9, train_loss: 0.019924982853636548\n",
      "FOLD: 6, EPOCH: 9, valid_loss: 0.019781729349723227\n",
      "FOLD: 6, EPOCH: 10, train_loss: 0.01977645666212649\n",
      "FOLD: 6, EPOCH: 10, valid_loss: 0.019684135340727292\n",
      "FOLD: 6, EPOCH: 11, train_loss: 0.019658347012827527\n",
      "FOLD: 6, EPOCH: 11, valid_loss: 0.01962477255326051\n",
      "FOLD: 6, EPOCH: 12, train_loss: 0.019536058267427457\n",
      "FOLD: 6, EPOCH: 12, valid_loss: 0.01952034793794155\n",
      "FOLD: 6, EPOCH: 13, train_loss: 0.01934840137491355\n",
      "FOLD: 6, EPOCH: 13, valid_loss: 0.0193435950921132\n",
      "FOLD: 6, EPOCH: 14, train_loss: 0.019282459382068465\n",
      "FOLD: 6, EPOCH: 14, valid_loss: 0.019367720215366438\n",
      "FOLD: 6, EPOCH: 15, train_loss: 0.01915188992043605\n",
      "FOLD: 6, EPOCH: 15, valid_loss: 0.01942093068590531\n",
      "FOLD: 6, EPOCH: 16, train_loss: 0.018999403984462086\n",
      "FOLD: 6, EPOCH: 16, valid_loss: 0.019189717821203746\n",
      "FOLD: 6, EPOCH: 17, train_loss: 0.018878978878461027\n",
      "FOLD: 6, EPOCH: 17, valid_loss: 0.01926176264308966\n",
      "FOLD: 6, EPOCH: 18, train_loss: 0.01876939649416788\n",
      "FOLD: 6, EPOCH: 18, valid_loss: 0.019205553743701715\n",
      "FOLD: 6, EPOCH: 19, train_loss: 0.01872270556821211\n",
      "FOLD: 6, EPOCH: 19, valid_loss: 0.019241503941325042\n",
      "Epoch    20: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 6, EPOCH: 20, train_loss: 0.01837230657504217\n",
      "FOLD: 6, EPOCH: 20, valid_loss: 0.01898814207659318\n",
      "FOLD: 6, EPOCH: 21, train_loss: 0.01816726634530602\n",
      "FOLD: 6, EPOCH: 21, valid_loss: 0.018956037094959848\n",
      "FOLD: 6, EPOCH: 22, train_loss: 0.018143322948064353\n",
      "FOLD: 6, EPOCH: 22, valid_loss: 0.01894203143624159\n",
      "FOLD: 6, EPOCH: 23, train_loss: 0.018072276149649878\n",
      "FOLD: 6, EPOCH: 23, valid_loss: 0.018928313771119483\n",
      "FOLD: 6, EPOCH: 24, train_loss: 0.018044130247388337\n",
      "FOLD: 6, EPOCH: 24, valid_loss: 0.01891993759916379\n",
      "FOLD: 6, EPOCH: 25, train_loss: 0.017961961041028436\n",
      "FOLD: 6, EPOCH: 25, valid_loss: 0.01891287401891672\n",
      "FOLD: 6, EPOCH: 26, train_loss: 0.017950905084207252\n",
      "FOLD: 6, EPOCH: 26, valid_loss: 0.018901160416694786\n",
      "FOLD: 6, EPOCH: 27, train_loss: 0.017902803506601502\n",
      "FOLD: 6, EPOCH: 27, valid_loss: 0.018898549704597548\n",
      "FOLD: 6, EPOCH: 28, train_loss: 0.017833507947019628\n",
      "FOLD: 6, EPOCH: 28, valid_loss: 0.018892952599204503\n",
      "FOLD: 6, EPOCH: 29, train_loss: 0.017820446047227125\n",
      "FOLD: 6, EPOCH: 29, valid_loss: 0.018884913279460028\n",
      "FOLD: 6, EPOCH: 30, train_loss: 0.017793039514406306\n",
      "FOLD: 6, EPOCH: 30, valid_loss: 0.018867719918489456\n",
      "FOLD: 6, EPOCH: 31, train_loss: 0.01778695427787465\n",
      "FOLD: 6, EPOCH: 31, valid_loss: 0.018860678689984176\n",
      "FOLD: 6, EPOCH: 32, train_loss: 0.01771238418547688\n",
      "FOLD: 6, EPOCH: 32, valid_loss: 0.0188633743673563\n",
      "FOLD: 6, EPOCH: 33, train_loss: 0.017695430691379146\n",
      "FOLD: 6, EPOCH: 33, valid_loss: 0.018839064412392102\n",
      "FOLD: 6, EPOCH: 34, train_loss: 0.01768705384755457\n",
      "FOLD: 6, EPOCH: 34, valid_loss: 0.018857551308778617\n",
      "FOLD: 6, EPOCH: 35, train_loss: 0.017647170939960995\n",
      "FOLD: 6, EPOCH: 35, valid_loss: 0.01884732524362894\n",
      "FOLD: 6, EPOCH: 36, train_loss: 0.017608193051372026\n",
      "FOLD: 6, EPOCH: 36, valid_loss: 0.01885279568915184\n",
      "Epoch    37: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 6, EPOCH: 37, train_loss: 0.017524984111455647\n",
      "FOLD: 6, EPOCH: 37, valid_loss: 0.018848908921846978\n",
      "FOLD: 6, EPOCH: 38, train_loss: 0.017553716293863347\n",
      "FOLD: 6, EPOCH: 38, valid_loss: 0.018849059509543273\n",
      "FOLD: 6, EPOCH: 39, train_loss: 0.01756582109609971\n",
      "FOLD: 6, EPOCH: 39, valid_loss: 0.018852978514937255\n",
      "Epoch    40: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 6, EPOCH: 40, train_loss: 0.01751378586364759\n",
      "FOLD: 6, EPOCH: 40, valid_loss: 0.01884569815145089\n",
      "FOLD: 6, EPOCH: 41, train_loss: 0.017547506260106694\n",
      "FOLD: 6, EPOCH: 41, valid_loss: 0.018854339105578568\n",
      "FOLD: 6, EPOCH: 42, train_loss: 0.017567400115768652\n",
      "FOLD: 6, EPOCH: 42, valid_loss: 0.018844768978082217\n",
      "Epoch    43: reducing learning rate of group 0 to 8.9345e-05.\n",
      "FOLD: 6, EPOCH: 43, train_loss: 0.01755123118542739\n",
      "FOLD: 6, EPOCH: 43, valid_loss: 0.018850811255665924\n",
      "FOLD: 6, EPOCH: 44, train_loss: 0.017570796095438906\n",
      "FOLD: 6, EPOCH: 44, valid_loss: 0.018851802182885315\n",
      "FOLD: 6, EPOCH: 45, train_loss: 0.0175590709959333\n",
      "FOLD: 6, EPOCH: 45, valid_loss: 0.01885271932070072\n",
      "Epoch    46: reducing learning rate of group 0 to 1.3371e-05.\n",
      "FOLD: 6, EPOCH: 46, train_loss: 0.017524090054369456\n",
      "FOLD: 6, EPOCH: 46, valid_loss: 0.018849907729488153\n",
      "FOLD: 6, EPOCH: 47, train_loss: 0.01753417928577275\n",
      "FOLD: 6, EPOCH: 47, valid_loss: 0.01885678920035179\n",
      "FOLD: 6, EPOCH: 48, train_loss: 0.01755776004614057\n",
      "FOLD: 6, EPOCH: 48, valid_loss: 0.018848217880496614\n",
      "Epoch    49: reducing learning rate of group 0 to 2.0012e-06.\n",
      "FOLD: 6, EPOCH: 49, train_loss: 0.017574333283748175\n",
      "FOLD: 6, EPOCH: 49, valid_loss: 0.018830320582940027\n",
      "FOLD: 0, EPOCH: 0, train_loss: 0.048594899400061856\n",
      "FOLD: 0, EPOCH: 0, valid_loss: 0.023158996580885008\n",
      "FOLD: 0, EPOCH: 1, train_loss: 0.022412279218032554\n",
      "FOLD: 0, EPOCH: 1, valid_loss: 0.022057234954375487\n",
      "FOLD: 0, EPOCH: 2, train_loss: 0.021734796766493772\n",
      "FOLD: 0, EPOCH: 2, valid_loss: 0.020883221752368487\n",
      "FOLD: 0, EPOCH: 3, train_loss: 0.021266604975067282\n",
      "FOLD: 0, EPOCH: 3, valid_loss: 0.020656053979809467\n",
      "FOLD: 0, EPOCH: 4, train_loss: 0.020940241252852453\n",
      "FOLD: 0, EPOCH: 4, valid_loss: 0.020362151929965384\n",
      "FOLD: 0, EPOCH: 5, train_loss: 0.020687384329534864\n",
      "FOLD: 0, EPOCH: 5, valid_loss: 0.020361373344293006\n",
      "FOLD: 0, EPOCH: 6, train_loss: 0.02041719531690752\n",
      "FOLD: 0, EPOCH: 6, valid_loss: 0.020076001922671612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 0, EPOCH: 7, train_loss: 0.020329699448838428\n",
      "FOLD: 0, EPOCH: 7, valid_loss: 0.019917563177072085\n",
      "FOLD: 0, EPOCH: 8, train_loss: 0.020121264271438122\n",
      "FOLD: 0, EPOCH: 8, valid_loss: 0.02015346374649268\n",
      "FOLD: 0, EPOCH: 9, train_loss: 0.019971843758547627\n",
      "FOLD: 0, EPOCH: 9, valid_loss: 0.019704866867799025\n",
      "FOLD: 0, EPOCH: 10, train_loss: 0.019774818667084783\n",
      "FOLD: 0, EPOCH: 10, valid_loss: 0.019522119886600055\n",
      "FOLD: 0, EPOCH: 11, train_loss: 0.019639673592472397\n",
      "FOLD: 0, EPOCH: 11, valid_loss: 0.01940762925033386\n",
      "FOLD: 0, EPOCH: 12, train_loss: 0.019521380573309755\n",
      "FOLD: 0, EPOCH: 12, valid_loss: 0.01955756745659388\n",
      "FOLD: 0, EPOCH: 13, train_loss: 0.019302217516343336\n",
      "FOLD: 0, EPOCH: 13, valid_loss: 0.019296553988869373\n",
      "FOLD: 0, EPOCH: 14, train_loss: 0.019193799612489907\n",
      "FOLD: 0, EPOCH: 14, valid_loss: 0.01924011354836134\n",
      "FOLD: 0, EPOCH: 15, train_loss: 0.01908868571390977\n",
      "FOLD: 0, EPOCH: 15, valid_loss: 0.019265892557226695\n",
      "FOLD: 0, EPOCH: 16, train_loss: 0.018961087951587664\n",
      "FOLD: 0, EPOCH: 16, valid_loss: 0.01925377347148382\n",
      "FOLD: 0, EPOCH: 17, train_loss: 0.01884090278700397\n",
      "FOLD: 0, EPOCH: 17, valid_loss: 0.019103095221977968\n",
      "FOLD: 0, EPOCH: 18, train_loss: 0.018715914666048578\n",
      "FOLD: 0, EPOCH: 18, valid_loss: 0.019135143178013656\n",
      "FOLD: 0, EPOCH: 19, train_loss: 0.01863949151860701\n",
      "FOLD: 0, EPOCH: 19, valid_loss: 0.019360098128135387\n",
      "FOLD: 0, EPOCH: 20, train_loss: 0.018458679595307725\n",
      "FOLD: 0, EPOCH: 20, valid_loss: 0.019096802060420696\n",
      "FOLD: 0, EPOCH: 21, train_loss: 0.018394402756884292\n",
      "FOLD: 0, EPOCH: 21, valid_loss: 0.019020876059165366\n",
      "FOLD: 0, EPOCH: 22, train_loss: 0.01823641827984436\n",
      "FOLD: 0, EPOCH: 22, valid_loss: 0.019019570058354966\n",
      "FOLD: 0, EPOCH: 23, train_loss: 0.01813570027415817\n",
      "FOLD: 0, EPOCH: 23, valid_loss: 0.018970009655906603\n",
      "FOLD: 0, EPOCH: 24, train_loss: 0.01803006017832337\n",
      "FOLD: 0, EPOCH: 24, valid_loss: 0.019009164319588587\n",
      "FOLD: 0, EPOCH: 25, train_loss: 0.01790474123648695\n",
      "FOLD: 0, EPOCH: 25, valid_loss: 0.018946822589406602\n",
      "FOLD: 0, EPOCH: 26, train_loss: 0.01784203971761304\n",
      "FOLD: 0, EPOCH: 26, valid_loss: 0.018857120321347162\n",
      "FOLD: 0, EPOCH: 27, train_loss: 0.01765125816234866\n",
      "FOLD: 0, EPOCH: 27, valid_loss: 0.01899203285574913\n",
      "FOLD: 0, EPOCH: 28, train_loss: 0.01757169370161923\n",
      "FOLD: 0, EPOCH: 28, valid_loss: 0.018842359431661092\n",
      "FOLD: 0, EPOCH: 29, train_loss: 0.017524251059905904\n",
      "FOLD: 0, EPOCH: 29, valid_loss: 0.01892095173780735\n",
      "FOLD: 0, EPOCH: 30, train_loss: 0.017372897335302992\n",
      "FOLD: 0, EPOCH: 30, valid_loss: 0.01886849864744223\n",
      "FOLD: 0, EPOCH: 31, train_loss: 0.01729567028028337\n",
      "FOLD: 0, EPOCH: 31, valid_loss: 0.018826989600291617\n",
      "FOLD: 0, EPOCH: 32, train_loss: 0.017178789158729283\n",
      "FOLD: 0, EPOCH: 32, valid_loss: 0.018855416860717993\n",
      "FOLD: 0, EPOCH: 33, train_loss: 0.017042930816879142\n",
      "FOLD: 0, EPOCH: 33, valid_loss: 0.01889809722510668\n",
      "FOLD: 0, EPOCH: 34, train_loss: 0.01687798389812579\n",
      "FOLD: 0, EPOCH: 34, valid_loss: 0.01884797545006642\n",
      "Epoch    35: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 0, EPOCH: 35, train_loss: 0.016585930162487\n",
      "FOLD: 0, EPOCH: 35, valid_loss: 0.018809163369811498\n",
      "FOLD: 0, EPOCH: 36, train_loss: 0.016519838440659886\n",
      "FOLD: 0, EPOCH: 36, valid_loss: 0.018749308700744923\n",
      "FOLD: 0, EPOCH: 37, train_loss: 0.016386995834575313\n",
      "FOLD: 0, EPOCH: 37, valid_loss: 0.01876905230948558\n",
      "FOLD: 0, EPOCH: 38, train_loss: 0.0163289153998768\n",
      "FOLD: 0, EPOCH: 38, valid_loss: 0.018765109232985057\n",
      "FOLD: 0, EPOCH: 39, train_loss: 0.016334579326212406\n",
      "FOLD: 0, EPOCH: 39, valid_loss: 0.01877701483093775\n",
      "Epoch    40: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 0, EPOCH: 40, train_loss: 0.016259923627650417\n",
      "FOLD: 0, EPOCH: 40, valid_loss: 0.018782116042879913\n",
      "FOLD: 0, EPOCH: 41, train_loss: 0.016252473223249655\n",
      "FOLD: 0, EPOCH: 41, valid_loss: 0.018757235545378465\n",
      "FOLD: 0, EPOCH: 42, train_loss: 0.016199051427680092\n",
      "FOLD: 0, EPOCH: 42, valid_loss: 0.01877031332025161\n",
      "Epoch    43: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 0, EPOCH: 43, train_loss: 0.016221094020717853\n",
      "FOLD: 0, EPOCH: 43, valid_loss: 0.018766231978168853\n",
      "FOLD: 0, EPOCH: 44, train_loss: 0.016191438205439498\n",
      "FOLD: 0, EPOCH: 44, valid_loss: 0.018750135571910784\n",
      "FOLD: 0, EPOCH: 45, train_loss: 0.01623471387083063\n",
      "FOLD: 0, EPOCH: 45, valid_loss: 0.018757186686763398\n",
      "Epoch    46: reducing learning rate of group 0 to 8.9345e-05.\n",
      "FOLD: 0, EPOCH: 46, train_loss: 0.016203834058565868\n",
      "FOLD: 0, EPOCH: 46, valid_loss: 0.018753009346815255\n",
      "FOLD: 0, EPOCH: 47, train_loss: 0.016211843130656996\n",
      "FOLD: 0, EPOCH: 47, valid_loss: 0.018743820058611724\n",
      "FOLD: 0, EPOCH: 48, train_loss: 0.016242991365190293\n",
      "FOLD: 0, EPOCH: 48, valid_loss: 0.018772732753020067\n",
      "FOLD: 0, EPOCH: 49, train_loss: 0.016198576918828325\n",
      "FOLD: 0, EPOCH: 49, valid_loss: 0.018732336134864733\n",
      "FOLD: 1, EPOCH: 0, train_loss: 0.04656233399401646\n",
      "FOLD: 1, EPOCH: 0, valid_loss: 0.022625184403016016\n",
      "FOLD: 1, EPOCH: 1, train_loss: 0.022261733667471924\n",
      "FOLD: 1, EPOCH: 1, valid_loss: 0.022185460162850525\n",
      "FOLD: 1, EPOCH: 2, train_loss: 0.021557479366861487\n",
      "FOLD: 1, EPOCH: 2, valid_loss: 0.021864716680004045\n",
      "FOLD: 1, EPOCH: 3, train_loss: 0.021144421897023112\n",
      "FOLD: 1, EPOCH: 3, valid_loss: 0.020882605360104486\n",
      "FOLD: 1, EPOCH: 4, train_loss: 0.02085972114189251\n",
      "FOLD: 1, EPOCH: 4, valid_loss: 0.020998787708007373\n",
      "FOLD: 1, EPOCH: 5, train_loss: 0.020639129939514236\n",
      "FOLD: 1, EPOCH: 5, valid_loss: 0.020610439806030348\n",
      "FOLD: 1, EPOCH: 6, train_loss: 0.020452632367409563\n",
      "FOLD: 1, EPOCH: 6, valid_loss: 0.020585141216333095\n",
      "FOLD: 1, EPOCH: 7, train_loss: 0.020287028564190544\n",
      "FOLD: 1, EPOCH: 7, valid_loss: 0.020256518171383783\n",
      "FOLD: 1, EPOCH: 8, train_loss: 0.020084200175227346\n",
      "FOLD: 1, EPOCH: 8, valid_loss: 0.020099367373264752\n",
      "FOLD: 1, EPOCH: 9, train_loss: 0.01995840444657448\n",
      "FOLD: 1, EPOCH: 9, valid_loss: 0.019983777919640906\n",
      "FOLD: 1, EPOCH: 10, train_loss: 0.01981107550799041\n",
      "FOLD: 1, EPOCH: 10, valid_loss: 0.019925760821654245\n",
      "FOLD: 1, EPOCH: 11, train_loss: 0.019662878828475606\n",
      "FOLD: 1, EPOCH: 11, valid_loss: 0.019803539348336365\n",
      "FOLD: 1, EPOCH: 12, train_loss: 0.01953018710923356\n",
      "FOLD: 1, EPOCH: 12, valid_loss: 0.01979347920188537\n",
      "FOLD: 1, EPOCH: 13, train_loss: 0.019432710458499355\n",
      "FOLD: 1, EPOCH: 13, valid_loss: 0.01969452345600495\n",
      "FOLD: 1, EPOCH: 14, train_loss: 0.01933473806727577\n",
      "FOLD: 1, EPOCH: 14, valid_loss: 0.01961072921179808\n",
      "FOLD: 1, EPOCH: 15, train_loss: 0.0191882987431175\n",
      "FOLD: 1, EPOCH: 15, valid_loss: 0.019487804948137358\n",
      "FOLD: 1, EPOCH: 16, train_loss: 0.019066933089414158\n",
      "FOLD: 1, EPOCH: 16, valid_loss: 0.0198040040066609\n",
      "FOLD: 1, EPOCH: 17, train_loss: 0.018980680536982174\n",
      "FOLD: 1, EPOCH: 17, valid_loss: 0.019425408055002872\n",
      "FOLD: 1, EPOCH: 18, train_loss: 0.018832780697659868\n",
      "FOLD: 1, EPOCH: 18, valid_loss: 0.019323336533628978\n",
      "FOLD: 1, EPOCH: 19, train_loss: 0.018738819135201944\n",
      "FOLD: 1, EPOCH: 19, valid_loss: 0.019421234010503843\n",
      "FOLD: 1, EPOCH: 20, train_loss: 0.018622207656704092\n",
      "FOLD: 1, EPOCH: 20, valid_loss: 0.019325970170589592\n",
      "FOLD: 1, EPOCH: 21, train_loss: 0.018550663683060055\n",
      "FOLD: 1, EPOCH: 21, valid_loss: 0.019244941954429332\n",
      "FOLD: 1, EPOCH: 22, train_loss: 0.01839563318503064\n",
      "FOLD: 1, EPOCH: 22, valid_loss: 0.019248712378052566\n",
      "FOLD: 1, EPOCH: 23, train_loss: 0.018302632193710353\n",
      "FOLD: 1, EPOCH: 23, valid_loss: 0.01924824299147496\n",
      "FOLD: 1, EPOCH: 24, train_loss: 0.01821951228319793\n",
      "FOLD: 1, EPOCH: 24, valid_loss: 0.019250355947476167\n",
      "Epoch    25: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 1, EPOCH: 25, train_loss: 0.017857677454279887\n",
      "FOLD: 1, EPOCH: 25, valid_loss: 0.019070696085691452\n",
      "FOLD: 1, EPOCH: 26, train_loss: 0.017718763996821804\n",
      "FOLD: 1, EPOCH: 26, valid_loss: 0.01902301537875946\n",
      "FOLD: 1, EPOCH: 27, train_loss: 0.017687389727782558\n",
      "FOLD: 1, EPOCH: 27, valid_loss: 0.01903639346934282\n",
      "FOLD: 1, EPOCH: 28, train_loss: 0.017617219983524567\n",
      "FOLD: 1, EPOCH: 28, valid_loss: 0.01900125567156535\n",
      "FOLD: 1, EPOCH: 29, train_loss: 0.017556227919821803\n",
      "FOLD: 1, EPOCH: 29, valid_loss: 0.019003167175329648\n",
      "FOLD: 1, EPOCH: 30, train_loss: 0.017477000925085834\n",
      "FOLD: 1, EPOCH: 30, valid_loss: 0.018988448123519238\n",
      "FOLD: 1, EPOCH: 31, train_loss: 0.01749078365596565\n",
      "FOLD: 1, EPOCH: 31, valid_loss: 0.01897283786764512\n",
      "FOLD: 1, EPOCH: 32, train_loss: 0.017426255184250908\n",
      "FOLD: 1, EPOCH: 32, valid_loss: 0.018968133685680535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 1, EPOCH: 33, train_loss: 0.01740641282821024\n",
      "FOLD: 1, EPOCH: 33, valid_loss: 0.01897606196311804\n",
      "FOLD: 1, EPOCH: 34, train_loss: 0.017349742378133373\n",
      "FOLD: 1, EPOCH: 34, valid_loss: 0.018970798127926312\n",
      "FOLD: 1, EPOCH: 35, train_loss: 0.017305268704689836\n",
      "FOLD: 1, EPOCH: 35, valid_loss: 0.0189628628297494\n",
      "FOLD: 1, EPOCH: 36, train_loss: 0.017311374329634616\n",
      "FOLD: 1, EPOCH: 36, valid_loss: 0.018966863504969157\n",
      "FOLD: 1, EPOCH: 37, train_loss: 0.017245334308795834\n",
      "FOLD: 1, EPOCH: 37, valid_loss: 0.01896517680814633\n",
      "FOLD: 1, EPOCH: 38, train_loss: 0.01727060109024515\n",
      "FOLD: 1, EPOCH: 38, valid_loss: 0.018949805543972895\n",
      "FOLD: 1, EPOCH: 39, train_loss: 0.01721925922744983\n",
      "FOLD: 1, EPOCH: 39, valid_loss: 0.018957475630136635\n",
      "FOLD: 1, EPOCH: 40, train_loss: 0.01716149746868256\n",
      "FOLD: 1, EPOCH: 40, valid_loss: 0.01894567648951824\n",
      "FOLD: 1, EPOCH: 41, train_loss: 0.01711760399661757\n",
      "FOLD: 1, EPOCH: 41, valid_loss: 0.018951227888464928\n",
      "FOLD: 1, EPOCH: 42, train_loss: 0.01713451710403771\n",
      "FOLD: 1, EPOCH: 42, valid_loss: 0.01893544440659193\n",
      "FOLD: 1, EPOCH: 43, train_loss: 0.017114767603374815\n",
      "FOLD: 1, EPOCH: 43, valid_loss: 0.01895103188088307\n",
      "FOLD: 1, EPOCH: 44, train_loss: 0.017072291377730465\n",
      "FOLD: 1, EPOCH: 44, valid_loss: 0.018921921459528115\n",
      "FOLD: 1, EPOCH: 45, train_loss: 0.017051514414315287\n",
      "FOLD: 1, EPOCH: 45, valid_loss: 0.018941972261438005\n",
      "FOLD: 1, EPOCH: 46, train_loss: 0.01701695960317109\n",
      "FOLD: 1, EPOCH: 46, valid_loss: 0.018939125853089187\n",
      "FOLD: 1, EPOCH: 47, train_loss: 0.016984785340625693\n",
      "FOLD: 1, EPOCH: 47, valid_loss: 0.018940162343474534\n",
      "Epoch    48: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 1, EPOCH: 48, train_loss: 0.016945028954462427\n",
      "FOLD: 1, EPOCH: 48, valid_loss: 0.018946112491763555\n",
      "FOLD: 1, EPOCH: 49, train_loss: 0.016967244949695225\n",
      "FOLD: 1, EPOCH: 49, valid_loss: 0.018929872231987808\n",
      "FOLD: 2, EPOCH: 0, train_loss: 0.04900609538261149\n",
      "FOLD: 2, EPOCH: 0, valid_loss: 0.02330923968782792\n",
      "FOLD: 2, EPOCH: 1, train_loss: 0.022327758044608542\n",
      "FOLD: 2, EPOCH: 1, valid_loss: 0.022161601254573234\n",
      "FOLD: 2, EPOCH: 2, train_loss: 0.021556783694069128\n",
      "FOLD: 2, EPOCH: 2, valid_loss: 0.021781549144249696\n",
      "FOLD: 2, EPOCH: 3, train_loss: 0.021177946136811295\n",
      "FOLD: 2, EPOCH: 3, valid_loss: 0.021024369419767305\n",
      "FOLD: 2, EPOCH: 4, train_loss: 0.021025582443217974\n",
      "FOLD: 2, EPOCH: 4, valid_loss: 0.020715600166183252\n",
      "FOLD: 2, EPOCH: 5, train_loss: 0.020628021149015106\n",
      "FOLD: 2, EPOCH: 5, valid_loss: 0.020841245754407003\n",
      "FOLD: 2, EPOCH: 6, train_loss: 0.02040531123812134\n",
      "FOLD: 2, EPOCH: 6, valid_loss: 0.02038307344684234\n",
      "FOLD: 2, EPOCH: 7, train_loss: 0.02021319915011928\n",
      "FOLD: 2, EPOCH: 7, valid_loss: 0.02038778679875227\n",
      "FOLD: 2, EPOCH: 8, train_loss: 0.020028724284792267\n",
      "FOLD: 2, EPOCH: 8, valid_loss: 0.020108647357959013\n",
      "FOLD: 2, EPOCH: 9, train_loss: 0.019873472710920347\n",
      "FOLD: 2, EPOCH: 9, valid_loss: 0.020314007997512817\n",
      "FOLD: 2, EPOCH: 10, train_loss: 0.019733353895512788\n",
      "FOLD: 2, EPOCH: 10, valid_loss: 0.019958222858034648\n",
      "FOLD: 2, EPOCH: 11, train_loss: 0.019610604371976207\n",
      "FOLD: 2, EPOCH: 11, valid_loss: 0.019966578254332908\n",
      "FOLD: 2, EPOCH: 12, train_loss: 0.01946458562805846\n",
      "FOLD: 2, EPOCH: 12, valid_loss: 0.0198036555487376\n",
      "FOLD: 2, EPOCH: 13, train_loss: 0.019352544023579842\n",
      "FOLD: 2, EPOCH: 13, valid_loss: 0.019735733333688516\n",
      "FOLD: 2, EPOCH: 14, train_loss: 0.019203161694914907\n",
      "FOLD: 2, EPOCH: 14, valid_loss: 0.019675281471931018\n",
      "FOLD: 2, EPOCH: 15, train_loss: 0.019067056930145702\n",
      "FOLD: 2, EPOCH: 15, valid_loss: 0.019486878353815813\n",
      "FOLD: 2, EPOCH: 16, train_loss: 0.018972484797642037\n",
      "FOLD: 2, EPOCH: 16, valid_loss: 0.020285270248468105\n",
      "FOLD: 2, EPOCH: 17, train_loss: 0.01884450003303386\n",
      "FOLD: 2, EPOCH: 17, valid_loss: 0.01951668970286846\n",
      "FOLD: 2, EPOCH: 18, train_loss: 0.01867207825636944\n",
      "FOLD: 2, EPOCH: 18, valid_loss: 0.01943209022283554\n",
      "FOLD: 2, EPOCH: 19, train_loss: 0.01859314281594109\n",
      "FOLD: 2, EPOCH: 19, valid_loss: 0.019449138440764867\n",
      "FOLD: 2, EPOCH: 20, train_loss: 0.0184769840860689\n",
      "FOLD: 2, EPOCH: 20, valid_loss: 0.01952597011740391\n",
      "FOLD: 2, EPOCH: 21, train_loss: 0.01836608033123854\n",
      "FOLD: 2, EPOCH: 21, valid_loss: 0.01937419018493249\n",
      "FOLD: 2, EPOCH: 22, train_loss: 0.018239981368989557\n",
      "FOLD: 2, EPOCH: 22, valid_loss: 0.0192556965809602\n",
      "FOLD: 2, EPOCH: 23, train_loss: 0.018145731197217026\n",
      "FOLD: 2, EPOCH: 23, valid_loss: 0.019298259168863297\n",
      "FOLD: 2, EPOCH: 24, train_loss: 0.01803208902679585\n",
      "FOLD: 2, EPOCH: 24, valid_loss: 0.01922934803252037\n",
      "FOLD: 2, EPOCH: 25, train_loss: 0.01792984883728865\n",
      "FOLD: 2, EPOCH: 25, valid_loss: 0.019280315161897585\n",
      "FOLD: 2, EPOCH: 26, train_loss: 0.017775384709239006\n",
      "FOLD: 2, EPOCH: 26, valid_loss: 0.019255231636074874\n",
      "FOLD: 2, EPOCH: 27, train_loss: 0.01769726393694008\n",
      "FOLD: 2, EPOCH: 27, valid_loss: 0.019283832265780523\n",
      "Epoch    28: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 2, EPOCH: 28, train_loss: 0.01730982116045984\n",
      "FOLD: 2, EPOCH: 28, valid_loss: 0.01916386688557955\n",
      "FOLD: 2, EPOCH: 29, train_loss: 0.017238340894314082\n",
      "FOLD: 2, EPOCH: 29, valid_loss: 0.019127221778035164\n",
      "FOLD: 2, EPOCH: 30, train_loss: 0.017136941336699435\n",
      "FOLD: 2, EPOCH: 30, valid_loss: 0.019106257993441362\n",
      "FOLD: 2, EPOCH: 31, train_loss: 0.017088447999511217\n",
      "FOLD: 2, EPOCH: 31, valid_loss: 0.019083203461307745\n",
      "FOLD: 2, EPOCH: 32, train_loss: 0.017008197644876467\n",
      "FOLD: 2, EPOCH: 32, valid_loss: 0.01909036418566337\n",
      "FOLD: 2, EPOCH: 33, train_loss: 0.016987214544536295\n",
      "FOLD: 2, EPOCH: 33, valid_loss: 0.019079206941219475\n",
      "FOLD: 2, EPOCH: 34, train_loss: 0.016973763691714487\n",
      "FOLD: 2, EPOCH: 34, valid_loss: 0.01909470844727296\n",
      "FOLD: 2, EPOCH: 35, train_loss: 0.016896562940264877\n",
      "FOLD: 2, EPOCH: 35, valid_loss: 0.01906148401590494\n",
      "FOLD: 2, EPOCH: 36, train_loss: 0.016865449605157245\n",
      "FOLD: 2, EPOCH: 36, valid_loss: 0.0190671061953673\n",
      "FOLD: 2, EPOCH: 37, train_loss: 0.01687019140887502\n",
      "FOLD: 2, EPOCH: 37, valid_loss: 0.019068979299985446\n",
      "FOLD: 2, EPOCH: 38, train_loss: 0.016786498860833613\n",
      "FOLD: 2, EPOCH: 38, valid_loss: 0.01907382131769107\n",
      "Epoch    39: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 2, EPOCH: 39, train_loss: 0.016803687350270716\n",
      "FOLD: 2, EPOCH: 39, valid_loss: 0.019086248742846344\n",
      "FOLD: 2, EPOCH: 40, train_loss: 0.01671664552712763\n",
      "FOLD: 2, EPOCH: 40, valid_loss: 0.019058396180088703\n",
      "FOLD: 2, EPOCH: 41, train_loss: 0.01674388118742688\n",
      "FOLD: 2, EPOCH: 41, valid_loss: 0.019081049956954442\n",
      "FOLD: 2, EPOCH: 42, train_loss: 0.016746284956162847\n",
      "FOLD: 2, EPOCH: 42, valid_loss: 0.019046303028097518\n",
      "FOLD: 2, EPOCH: 43, train_loss: 0.01669860428601906\n",
      "FOLD: 2, EPOCH: 43, valid_loss: 0.01908127633998027\n",
      "FOLD: 2, EPOCH: 44, train_loss: 0.016711585299187415\n",
      "FOLD: 2, EPOCH: 44, valid_loss: 0.01906508278961365\n",
      "FOLD: 2, EPOCH: 45, train_loss: 0.016780266223626362\n",
      "FOLD: 2, EPOCH: 45, valid_loss: 0.01905754480797511\n",
      "Epoch    46: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 2, EPOCH: 46, train_loss: 0.016699107930164884\n",
      "FOLD: 2, EPOCH: 46, valid_loss: 0.019043591160040636\n",
      "FOLD: 2, EPOCH: 47, train_loss: 0.016706084694109252\n",
      "FOLD: 2, EPOCH: 47, valid_loss: 0.01906020065339712\n",
      "FOLD: 2, EPOCH: 48, train_loss: 0.016716168664798543\n",
      "FOLD: 2, EPOCH: 48, valid_loss: 0.01908053171176177\n",
      "FOLD: 2, EPOCH: 49, train_loss: 0.01672819066742385\n",
      "FOLD: 2, EPOCH: 49, valid_loss: 0.01908313311063326\n",
      "Epoch    50: reducing learning rate of group 0 to 8.9345e-05.\n",
      "FOLD: 3, EPOCH: 0, train_loss: 0.048014537229932645\n",
      "FOLD: 3, EPOCH: 0, valid_loss: 0.022593820181030493\n",
      "FOLD: 3, EPOCH: 1, train_loss: 0.02240268115860385\n",
      "FOLD: 3, EPOCH: 1, valid_loss: 0.021798245752087005\n",
      "FOLD: 3, EPOCH: 2, train_loss: 0.02155886180195454\n",
      "FOLD: 3, EPOCH: 2, valid_loss: 0.02171448890406352\n",
      "FOLD: 3, EPOCH: 3, train_loss: 0.021294231023136025\n",
      "FOLD: 3, EPOCH: 3, valid_loss: 0.020734739274932787\n",
      "FOLD: 3, EPOCH: 4, train_loss: 0.0208926378321406\n",
      "FOLD: 3, EPOCH: 4, valid_loss: 0.020497810525389817\n",
      "FOLD: 3, EPOCH: 5, train_loss: 0.02063551124789425\n",
      "FOLD: 3, EPOCH: 5, valid_loss: 0.02055623124425228\n",
      "FOLD: 3, EPOCH: 6, train_loss: 0.020478266291320324\n",
      "FOLD: 3, EPOCH: 6, valid_loss: 0.020047988456029158\n",
      "FOLD: 3, EPOCH: 7, train_loss: 0.020247910700335697\n",
      "FOLD: 3, EPOCH: 7, valid_loss: 0.020093898360545818\n",
      "FOLD: 3, EPOCH: 8, train_loss: 0.019997725917680842\n",
      "FOLD: 3, EPOCH: 8, valid_loss: 0.019930131590137116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 3, EPOCH: 9, train_loss: 0.01988876877805671\n",
      "FOLD: 3, EPOCH: 9, valid_loss: 0.019787366430346783\n",
      "FOLD: 3, EPOCH: 10, train_loss: 0.019736376037267415\n",
      "FOLD: 3, EPOCH: 10, valid_loss: 0.019658304607638948\n",
      "FOLD: 3, EPOCH: 11, train_loss: 0.01960484217852354\n",
      "FOLD: 3, EPOCH: 11, valid_loss: 0.019651436891693335\n",
      "FOLD: 3, EPOCH: 12, train_loss: 0.0194864676308793\n",
      "FOLD: 3, EPOCH: 12, valid_loss: 0.0195966841509709\n",
      "FOLD: 3, EPOCH: 13, train_loss: 0.01933270866504392\n",
      "FOLD: 3, EPOCH: 13, valid_loss: 0.019544123600308713\n",
      "FOLD: 3, EPOCH: 14, train_loss: 0.01915360921742143\n",
      "FOLD: 3, EPOCH: 14, valid_loss: 0.019385039519805174\n",
      "FOLD: 3, EPOCH: 15, train_loss: 0.01911104646687572\n",
      "FOLD: 3, EPOCH: 15, valid_loss: 0.019382893322752073\n",
      "FOLD: 3, EPOCH: 16, train_loss: 0.018982504293121195\n",
      "FOLD: 3, EPOCH: 16, valid_loss: 0.01939112449494692\n",
      "FOLD: 3, EPOCH: 17, train_loss: 0.01886448953804132\n",
      "FOLD: 3, EPOCH: 17, valid_loss: 0.019318954159434024\n",
      "FOLD: 3, EPOCH: 18, train_loss: 0.01873425839821229\n",
      "FOLD: 3, EPOCH: 18, valid_loss: 0.01927945762872696\n",
      "FOLD: 3, EPOCH: 19, train_loss: 0.018637729431124957\n",
      "FOLD: 3, EPOCH: 19, valid_loss: 0.019193296678937398\n",
      "FOLD: 3, EPOCH: 20, train_loss: 0.018528154900146497\n",
      "FOLD: 3, EPOCH: 20, valid_loss: 0.01919161915205992\n",
      "FOLD: 3, EPOCH: 21, train_loss: 0.018432872017493118\n",
      "FOLD: 3, EPOCH: 21, valid_loss: 0.019169012514444497\n",
      "FOLD: 3, EPOCH: 22, train_loss: 0.018291867992563826\n",
      "FOLD: 3, EPOCH: 22, valid_loss: 0.01905430366213505\n",
      "FOLD: 3, EPOCH: 23, train_loss: 0.018196536832161853\n",
      "FOLD: 3, EPOCH: 23, valid_loss: 0.019083511370878953\n",
      "FOLD: 3, EPOCH: 24, train_loss: 0.018075608547676255\n",
      "FOLD: 3, EPOCH: 24, valid_loss: 0.019139631149860527\n",
      "FOLD: 3, EPOCH: 25, train_loss: 0.017973603529704583\n",
      "FOLD: 3, EPOCH: 25, valid_loss: 0.019105923147155687\n",
      "Epoch    26: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 3, EPOCH: 26, train_loss: 0.017629195457777462\n",
      "FOLD: 3, EPOCH: 26, valid_loss: 0.01899697631597519\n",
      "FOLD: 3, EPOCH: 27, train_loss: 0.017517530686549238\n",
      "FOLD: 3, EPOCH: 27, valid_loss: 0.01896686594073589\n",
      "FOLD: 3, EPOCH: 28, train_loss: 0.01745412838519425\n",
      "FOLD: 3, EPOCH: 28, valid_loss: 0.018940459507016037\n",
      "FOLD: 3, EPOCH: 29, train_loss: 0.017373156708640022\n",
      "FOLD: 3, EPOCH: 29, valid_loss: 0.018958178277199086\n",
      "FOLD: 3, EPOCH: 30, train_loss: 0.01730832178145647\n",
      "FOLD: 3, EPOCH: 30, valid_loss: 0.018927099039921395\n",
      "FOLD: 3, EPOCH: 31, train_loss: 0.0172510735212347\n",
      "FOLD: 3, EPOCH: 31, valid_loss: 0.01890223430326352\n",
      "FOLD: 3, EPOCH: 32, train_loss: 0.01720008548550509\n",
      "FOLD: 3, EPOCH: 32, valid_loss: 0.018906909685868483\n",
      "FOLD: 3, EPOCH: 33, train_loss: 0.01719043796529641\n",
      "FOLD: 3, EPOCH: 33, valid_loss: 0.0189139752720411\n",
      "FOLD: 3, EPOCH: 34, train_loss: 0.017157059678858198\n",
      "FOLD: 3, EPOCH: 34, valid_loss: 0.018941578670189932\n",
      "Epoch    35: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 3, EPOCH: 35, train_loss: 0.017105408793164266\n",
      "FOLD: 3, EPOCH: 35, valid_loss: 0.018928756650823813\n",
      "FOLD: 3, EPOCH: 36, train_loss: 0.01709846716776893\n",
      "FOLD: 3, EPOCH: 36, valid_loss: 0.018940742772359114\n",
      "FOLD: 3, EPOCH: 37, train_loss: 0.017102905763061466\n",
      "FOLD: 3, EPOCH: 37, valid_loss: 0.018912737329418842\n",
      "Epoch    38: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 3, EPOCH: 38, train_loss: 0.017066006976607685\n",
      "FOLD: 3, EPOCH: 38, valid_loss: 0.01892409731562321\n",
      "FOLD: 3, EPOCH: 39, train_loss: 0.01703716309841823\n",
      "FOLD: 3, EPOCH: 39, valid_loss: 0.01893918402493\n",
      "FOLD: 3, EPOCH: 40, train_loss: 0.017090050969272852\n",
      "FOLD: 3, EPOCH: 40, valid_loss: 0.018916955647560265\n",
      "Epoch    41: reducing learning rate of group 0 to 8.9345e-05.\n",
      "FOLD: 3, EPOCH: 41, train_loss: 0.017052048917960475\n",
      "FOLD: 3, EPOCH: 41, valid_loss: 0.01893143656735237\n",
      "FOLD: 3, EPOCH: 42, train_loss: 0.017094651312642806\n",
      "FOLD: 3, EPOCH: 42, valid_loss: 0.018916393558566388\n",
      "FOLD: 3, EPOCH: 43, train_loss: 0.01711109541104855\n",
      "FOLD: 3, EPOCH: 43, valid_loss: 0.018928933172271803\n",
      "Epoch    44: reducing learning rate of group 0 to 1.3371e-05.\n",
      "FOLD: 3, EPOCH: 44, train_loss: 0.017074724558640172\n",
      "FOLD: 3, EPOCH: 44, valid_loss: 0.01891986094415188\n",
      "FOLD: 3, EPOCH: 45, train_loss: 0.017093961806716147\n",
      "FOLD: 3, EPOCH: 45, valid_loss: 0.01891787966283468\n",
      "FOLD: 3, EPOCH: 46, train_loss: 0.01707191308456901\n",
      "FOLD: 3, EPOCH: 46, valid_loss: 0.018917174436725102\n",
      "Epoch    47: reducing learning rate of group 0 to 2.0012e-06.\n",
      "FOLD: 3, EPOCH: 47, train_loss: 0.017096171973625552\n",
      "FOLD: 3, EPOCH: 47, valid_loss: 0.01892289533638037\n",
      "FOLD: 3, EPOCH: 48, train_loss: 0.017050313247317397\n",
      "FOLD: 3, EPOCH: 48, valid_loss: 0.01891753521676247\n",
      "FOLD: 3, EPOCH: 49, train_loss: 0.017108559318994347\n",
      "FOLD: 3, EPOCH: 49, valid_loss: 0.018941101116629746\n",
      "Epoch    50: reducing learning rate of group 0 to 2.9950e-07.\n",
      "FOLD: 4, EPOCH: 0, train_loss: 0.047153490092101936\n",
      "FOLD: 4, EPOCH: 0, valid_loss: 0.02290451884842836\n",
      "FOLD: 4, EPOCH: 1, train_loss: 0.022237728576402407\n",
      "FOLD: 4, EPOCH: 1, valid_loss: 0.022153325092334013\n",
      "FOLD: 4, EPOCH: 2, train_loss: 0.02156414866850183\n",
      "FOLD: 4, EPOCH: 2, valid_loss: 0.021662821993231773\n",
      "FOLD: 4, EPOCH: 3, train_loss: 0.021195555609223003\n",
      "FOLD: 4, EPOCH: 3, valid_loss: 0.02113892940374521\n",
      "FOLD: 4, EPOCH: 4, train_loss: 0.020848341185498883\n",
      "FOLD: 4, EPOCH: 4, valid_loss: 0.021058980088967543\n",
      "FOLD: 4, EPOCH: 5, train_loss: 0.020594910089228605\n",
      "FOLD: 4, EPOCH: 5, valid_loss: 0.020822214535795726\n",
      "FOLD: 4, EPOCH: 6, train_loss: 0.02041420524285452\n",
      "FOLD: 4, EPOCH: 6, valid_loss: 0.020705939055635378\n",
      "FOLD: 4, EPOCH: 7, train_loss: 0.02028213267692843\n",
      "FOLD: 4, EPOCH: 7, valid_loss: 0.020705949085263107\n",
      "FOLD: 4, EPOCH: 8, train_loss: 0.020108351760820764\n",
      "FOLD: 4, EPOCH: 8, valid_loss: 0.020522800632394277\n",
      "FOLD: 4, EPOCH: 9, train_loss: 0.01991307214405891\n",
      "FOLD: 4, EPOCH: 9, valid_loss: 0.020239331687872227\n",
      "FOLD: 4, EPOCH: 10, train_loss: 0.019738324137555587\n",
      "FOLD: 4, EPOCH: 10, valid_loss: 0.02017349835771781\n",
      "FOLD: 4, EPOCH: 11, train_loss: 0.019663401678003168\n",
      "FOLD: 4, EPOCH: 11, valid_loss: 0.020137667942505617\n",
      "FOLD: 4, EPOCH: 12, train_loss: 0.0195184203519209\n",
      "FOLD: 4, EPOCH: 12, valid_loss: 0.019921973777505066\n",
      "FOLD: 4, EPOCH: 13, train_loss: 0.01937731254745174\n",
      "FOLD: 4, EPOCH: 13, valid_loss: 0.019964075432373926\n",
      "FOLD: 4, EPOCH: 14, train_loss: 0.019234309904277325\n",
      "FOLD: 4, EPOCH: 14, valid_loss: 0.019711131659837868\n",
      "FOLD: 4, EPOCH: 15, train_loss: 0.01914294675697346\n",
      "FOLD: 4, EPOCH: 15, valid_loss: 0.019703422744686786\n",
      "FOLD: 4, EPOCH: 16, train_loss: 0.019036879486127478\n",
      "FOLD: 4, EPOCH: 16, valid_loss: 0.01965850620315625\n",
      "FOLD: 4, EPOCH: 17, train_loss: 0.018900641793938907\n",
      "FOLD: 4, EPOCH: 17, valid_loss: 0.019662251982551355\n",
      "FOLD: 4, EPOCH: 18, train_loss: 0.018782993023459975\n",
      "FOLD: 4, EPOCH: 18, valid_loss: 0.01964326690022762\n",
      "FOLD: 4, EPOCH: 19, train_loss: 0.018664705768428946\n",
      "FOLD: 4, EPOCH: 19, valid_loss: 0.019585733946699362\n",
      "FOLD: 4, EPOCH: 20, train_loss: 0.018591011068909556\n",
      "FOLD: 4, EPOCH: 20, valid_loss: 0.019543821565233745\n",
      "FOLD: 4, EPOCH: 21, train_loss: 0.018446795390667143\n",
      "FOLD: 4, EPOCH: 21, valid_loss: 0.019419867402085893\n",
      "FOLD: 4, EPOCH: 22, train_loss: 0.018318105891749665\n",
      "FOLD: 4, EPOCH: 22, valid_loss: 0.019474806980444834\n",
      "FOLD: 4, EPOCH: 23, train_loss: 0.018277805223054177\n",
      "FOLD: 4, EPOCH: 23, valid_loss: 0.0194823110046295\n",
      "FOLD: 4, EPOCH: 24, train_loss: 0.018175864541852795\n",
      "FOLD: 4, EPOCH: 24, valid_loss: 0.019426567336687676\n",
      "Epoch    25: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 4, EPOCH: 25, train_loss: 0.017784588756291447\n",
      "FOLD: 4, EPOCH: 25, valid_loss: 0.019280502715936072\n",
      "FOLD: 4, EPOCH: 26, train_loss: 0.017691812427664124\n",
      "FOLD: 4, EPOCH: 26, valid_loss: 0.01925176281768542\n",
      "FOLD: 4, EPOCH: 27, train_loss: 0.017630280133940884\n",
      "FOLD: 4, EPOCH: 27, valid_loss: 0.019253604830457613\n",
      "FOLD: 4, EPOCH: 28, train_loss: 0.0175236812666864\n",
      "FOLD: 4, EPOCH: 28, valid_loss: 0.019247921040424935\n",
      "FOLD: 4, EPOCH: 29, train_loss: 0.017484704863179375\n",
      "FOLD: 4, EPOCH: 29, valid_loss: 0.019247088151482437\n",
      "FOLD: 4, EPOCH: 30, train_loss: 0.01746766425266459\n",
      "FOLD: 4, EPOCH: 30, valid_loss: 0.019241084559605673\n",
      "FOLD: 4, EPOCH: 31, train_loss: 0.017432857934083487\n",
      "FOLD: 4, EPOCH: 31, valid_loss: 0.019214111738480054\n",
      "FOLD: 4, EPOCH: 32, train_loss: 0.017362822979890013\n",
      "FOLD: 4, EPOCH: 32, valid_loss: 0.0192089589455953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 4, EPOCH: 33, train_loss: 0.01735186279826873\n",
      "FOLD: 4, EPOCH: 33, valid_loss: 0.019220992922782898\n",
      "FOLD: 4, EPOCH: 34, train_loss: 0.017323415940375748\n",
      "FOLD: 4, EPOCH: 34, valid_loss: 0.019218180042046767\n",
      "FOLD: 4, EPOCH: 35, train_loss: 0.0172627676857283\n",
      "FOLD: 4, EPOCH: 35, valid_loss: 0.0191975231640614\n",
      "FOLD: 4, EPOCH: 36, train_loss: 0.017261961147793242\n",
      "FOLD: 4, EPOCH: 36, valid_loss: 0.019216191166868575\n",
      "FOLD: 4, EPOCH: 37, train_loss: 0.017195918723135382\n",
      "FOLD: 4, EPOCH: 37, valid_loss: 0.01920261835822692\n",
      "FOLD: 4, EPOCH: 38, train_loss: 0.0171957299667033\n",
      "FOLD: 4, EPOCH: 38, valid_loss: 0.01918753222204172\n",
      "FOLD: 4, EPOCH: 39, train_loss: 0.017175410849017067\n",
      "FOLD: 4, EPOCH: 39, valid_loss: 0.019196188220610984\n",
      "FOLD: 4, EPOCH: 40, train_loss: 0.01713409914156875\n",
      "FOLD: 4, EPOCH: 40, valid_loss: 0.019202750032910935\n",
      "FOLD: 4, EPOCH: 41, train_loss: 0.017080374640991557\n",
      "FOLD: 4, EPOCH: 41, valid_loss: 0.019212599843740463\n",
      "Epoch    42: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 4, EPOCH: 42, train_loss: 0.017111223870636644\n",
      "FOLD: 4, EPOCH: 42, valid_loss: 0.019191813440277025\n",
      "FOLD: 4, EPOCH: 43, train_loss: 0.017075218902145688\n",
      "FOLD: 4, EPOCH: 43, valid_loss: 0.019202248838085394\n",
      "FOLD: 4, EPOCH: 44, train_loss: 0.017086020570147683\n",
      "FOLD: 4, EPOCH: 44, valid_loss: 0.019187316585045595\n",
      "FOLD: 4, EPOCH: 45, train_loss: 0.017064067937836453\n",
      "FOLD: 4, EPOCH: 45, valid_loss: 0.019193994167905588\n",
      "FOLD: 4, EPOCH: 46, train_loss: 0.017102769236206204\n",
      "FOLD: 4, EPOCH: 46, valid_loss: 0.01918416155072359\n",
      "FOLD: 4, EPOCH: 47, train_loss: 0.017056437284761184\n",
      "FOLD: 4, EPOCH: 47, valid_loss: 0.019189800923833482\n",
      "FOLD: 4, EPOCH: 48, train_loss: 0.0170405231718276\n",
      "FOLD: 4, EPOCH: 48, valid_loss: 0.019210298903859578\n",
      "FOLD: 4, EPOCH: 49, train_loss: 0.017091127527827345\n",
      "FOLD: 4, EPOCH: 49, valid_loss: 0.01919218783195202\n",
      "Epoch    50: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 5, EPOCH: 0, train_loss: 0.046192362275276636\n",
      "FOLD: 5, EPOCH: 0, valid_loss: 0.02282884229834263\n",
      "FOLD: 5, EPOCH: 1, train_loss: 0.02230587840785046\n",
      "FOLD: 5, EPOCH: 1, valid_loss: 0.023916542530059814\n",
      "FOLD: 5, EPOCH: 2, train_loss: 0.0215779094377885\n",
      "FOLD: 5, EPOCH: 2, valid_loss: 0.020969318512540597\n",
      "FOLD: 5, EPOCH: 3, train_loss: 0.02117064273035204\n",
      "FOLD: 5, EPOCH: 3, valid_loss: 0.0209172756339495\n",
      "FOLD: 5, EPOCH: 4, train_loss: 0.020842936972307193\n",
      "FOLD: 5, EPOCH: 4, valid_loss: 0.020642327144742012\n",
      "FOLD: 5, EPOCH: 5, train_loss: 0.020651822607662226\n",
      "FOLD: 5, EPOCH: 5, valid_loss: 0.020493591347566016\n",
      "FOLD: 5, EPOCH: 6, train_loss: 0.02043818818354929\n",
      "FOLD: 5, EPOCH: 6, valid_loss: 0.020200765906618193\n",
      "FOLD: 5, EPOCH: 7, train_loss: 0.020260651016960275\n",
      "FOLD: 5, EPOCH: 7, valid_loss: 0.020026723782603558\n",
      "FOLD: 5, EPOCH: 8, train_loss: 0.020050813318104356\n",
      "FOLD: 5, EPOCH: 8, valid_loss: 0.019992809742689133\n",
      "FOLD: 5, EPOCH: 9, train_loss: 0.01987138156451889\n",
      "FOLD: 5, EPOCH: 9, valid_loss: 0.019834676614174478\n",
      "FOLD: 5, EPOCH: 10, train_loss: 0.019774890076872463\n",
      "FOLD: 5, EPOCH: 10, valid_loss: 0.019670921736038648\n",
      "FOLD: 5, EPOCH: 11, train_loss: 0.01963164924165687\n",
      "FOLD: 5, EPOCH: 11, valid_loss: 0.019569547990193732\n",
      "FOLD: 5, EPOCH: 12, train_loss: 0.019563927944447543\n",
      "FOLD: 5, EPOCH: 12, valid_loss: 0.01958854768711787\n",
      "FOLD: 5, EPOCH: 13, train_loss: 0.019361308448620745\n",
      "FOLD: 5, EPOCH: 13, valid_loss: 0.019455695094970558\n",
      "FOLD: 5, EPOCH: 14, train_loss: 0.01930879081624585\n",
      "FOLD: 5, EPOCH: 14, valid_loss: 0.01944267491881664\n",
      "FOLD: 5, EPOCH: 15, train_loss: 0.019168914044024172\n",
      "FOLD: 5, EPOCH: 15, valid_loss: 0.01942466629239229\n",
      "FOLD: 5, EPOCH: 16, train_loss: 0.019044024366382008\n",
      "FOLD: 5, EPOCH: 16, valid_loss: 0.01930782958292044\n",
      "FOLD: 5, EPOCH: 17, train_loss: 0.01891150647723997\n",
      "FOLD: 5, EPOCH: 17, valid_loss: 0.019341822140491925\n",
      "FOLD: 5, EPOCH: 18, train_loss: 0.018866974935942405\n",
      "FOLD: 5, EPOCH: 18, valid_loss: 0.019251665387016077\n",
      "FOLD: 5, EPOCH: 19, train_loss: 0.018739673233515507\n",
      "FOLD: 5, EPOCH: 19, valid_loss: 0.0191555183667403\n",
      "FOLD: 5, EPOCH: 20, train_loss: 0.018545850960386766\n",
      "FOLD: 5, EPOCH: 20, valid_loss: 0.019150359412798516\n",
      "FOLD: 5, EPOCH: 21, train_loss: 0.018479303179963213\n",
      "FOLD: 5, EPOCH: 21, valid_loss: 0.019135555252432823\n",
      "FOLD: 5, EPOCH: 22, train_loss: 0.018350760527961963\n",
      "FOLD: 5, EPOCH: 22, valid_loss: 0.01914296456827567\n",
      "FOLD: 5, EPOCH: 23, train_loss: 0.018247451859752874\n",
      "FOLD: 5, EPOCH: 23, valid_loss: 0.01914573761706169\n",
      "FOLD: 5, EPOCH: 24, train_loss: 0.018181763287331607\n",
      "FOLD: 5, EPOCH: 24, valid_loss: 0.019156282481092673\n",
      "Epoch    25: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 5, EPOCH: 25, train_loss: 0.01782413506628694\n",
      "FOLD: 5, EPOCH: 25, valid_loss: 0.01896745811861295\n",
      "FOLD: 5, EPOCH: 26, train_loss: 0.017719432724186697\n",
      "FOLD: 5, EPOCH: 26, valid_loss: 0.01894329645885871\n",
      "FOLD: 5, EPOCH: 27, train_loss: 0.017610712606157805\n",
      "FOLD: 5, EPOCH: 27, valid_loss: 0.01892507878633646\n",
      "FOLD: 5, EPOCH: 28, train_loss: 0.01758672111994914\n",
      "FOLD: 5, EPOCH: 28, valid_loss: 0.01891634713571805\n",
      "FOLD: 5, EPOCH: 29, train_loss: 0.01750119835943789\n",
      "FOLD: 5, EPOCH: 29, valid_loss: 0.018906365077082928\n",
      "FOLD: 5, EPOCH: 30, train_loss: 0.017476071742036053\n",
      "FOLD: 5, EPOCH: 30, valid_loss: 0.018905872479081154\n",
      "FOLD: 5, EPOCH: 31, train_loss: 0.0174609179155448\n",
      "FOLD: 5, EPOCH: 31, valid_loss: 0.01888661817289316\n",
      "FOLD: 5, EPOCH: 32, train_loss: 0.01740551650574481\n",
      "FOLD: 5, EPOCH: 32, valid_loss: 0.018871289892838553\n",
      "FOLD: 5, EPOCH: 33, train_loss: 0.017353649251163006\n",
      "FOLD: 5, EPOCH: 33, valid_loss: 0.018885899191865556\n",
      "FOLD: 5, EPOCH: 34, train_loss: 0.017336192528238974\n",
      "FOLD: 5, EPOCH: 34, valid_loss: 0.018862801245771922\n",
      "FOLD: 5, EPOCH: 35, train_loss: 0.017267390456352685\n",
      "FOLD: 5, EPOCH: 35, valid_loss: 0.018866910097690728\n",
      "FOLD: 5, EPOCH: 36, train_loss: 0.01722502554892688\n",
      "FOLD: 5, EPOCH: 36, valid_loss: 0.01884988136589527\n",
      "FOLD: 5, EPOCH: 37, train_loss: 0.01721230866639195\n",
      "FOLD: 5, EPOCH: 37, valid_loss: 0.01886381910970578\n",
      "FOLD: 5, EPOCH: 38, train_loss: 0.017173508786269137\n",
      "FOLD: 5, EPOCH: 38, valid_loss: 0.01887363725556777\n",
      "FOLD: 5, EPOCH: 39, train_loss: 0.017160964601144597\n",
      "FOLD: 5, EPOCH: 39, valid_loss: 0.018843710135955077\n",
      "FOLD: 5, EPOCH: 40, train_loss: 0.017186588809095526\n",
      "FOLD: 5, EPOCH: 40, valid_loss: 0.01885672973898741\n",
      "FOLD: 5, EPOCH: 41, train_loss: 0.01708489569608827\n",
      "FOLD: 5, EPOCH: 41, valid_loss: 0.018873429785554226\n",
      "FOLD: 5, EPOCH: 42, train_loss: 0.017053965542062715\n",
      "FOLD: 5, EPOCH: 42, valid_loss: 0.01885081512423662\n",
      "Epoch    43: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 5, EPOCH: 43, train_loss: 0.017032702453434467\n",
      "FOLD: 5, EPOCH: 43, valid_loss: 0.01886035602253217\n",
      "FOLD: 5, EPOCH: 44, train_loss: 0.01701925722630443\n",
      "FOLD: 5, EPOCH: 44, valid_loss: 0.01885599743288297\n",
      "FOLD: 5, EPOCH: 45, train_loss: 0.0170359964532828\n",
      "FOLD: 5, EPOCH: 45, valid_loss: 0.018853544902343016\n",
      "Epoch    46: reducing learning rate of group 0 to 5.9698e-04.\n",
      "FOLD: 5, EPOCH: 46, train_loss: 0.01701477888619175\n",
      "FOLD: 5, EPOCH: 46, valid_loss: 0.018852287330306493\n",
      "FOLD: 5, EPOCH: 47, train_loss: 0.017006941466919473\n",
      "FOLD: 5, EPOCH: 47, valid_loss: 0.018837266529981907\n",
      "FOLD: 5, EPOCH: 48, train_loss: 0.01700902100954507\n",
      "FOLD: 5, EPOCH: 48, valid_loss: 0.018858154232685383\n",
      "FOLD: 5, EPOCH: 49, train_loss: 0.016997848587060296\n",
      "FOLD: 5, EPOCH: 49, valid_loss: 0.01884594115500267\n",
      "FOLD: 6, EPOCH: 0, train_loss: 0.047177465305336425\n",
      "FOLD: 6, EPOCH: 0, valid_loss: 0.02292322739958763\n",
      "FOLD: 6, EPOCH: 1, train_loss: 0.022454436818087422\n",
      "FOLD: 6, EPOCH: 1, valid_loss: 0.021541904084957562\n",
      "FOLD: 6, EPOCH: 2, train_loss: 0.021672435960656888\n",
      "FOLD: 6, EPOCH: 2, valid_loss: 0.021231095521495894\n",
      "FOLD: 6, EPOCH: 3, train_loss: 0.021227046494951118\n",
      "FOLD: 6, EPOCH: 3, valid_loss: 0.02094941041790522\n",
      "FOLD: 6, EPOCH: 4, train_loss: 0.020966686484580103\n",
      "FOLD: 6, EPOCH: 4, valid_loss: 0.020633666704480465\n",
      "FOLD: 6, EPOCH: 5, train_loss: 0.020690961665398366\n",
      "FOLD: 6, EPOCH: 5, valid_loss: 0.020470448984549597\n",
      "FOLD: 6, EPOCH: 6, train_loss: 0.020491192369042215\n",
      "FOLD: 6, EPOCH: 6, valid_loss: 0.020150605016029798\n",
      "FOLD: 6, EPOCH: 7, train_loss: 0.020252762286848313\n",
      "FOLD: 6, EPOCH: 7, valid_loss: 0.02000094993183246\n",
      "FOLD: 6, EPOCH: 8, train_loss: 0.020092138517144566\n",
      "FOLD: 6, EPOCH: 8, valid_loss: 0.020159002393484116\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 6, EPOCH: 9, train_loss: 0.01992617985485373\n",
      "FOLD: 6, EPOCH: 9, valid_loss: 0.019749299264871158\n",
      "FOLD: 6, EPOCH: 10, train_loss: 0.019790624796941474\n",
      "FOLD: 6, EPOCH: 10, valid_loss: 0.019618956801983025\n",
      "FOLD: 6, EPOCH: 11, train_loss: 0.019583142740098206\n",
      "FOLD: 6, EPOCH: 11, valid_loss: 0.019627758803275917\n",
      "FOLD: 6, EPOCH: 12, train_loss: 0.019464952241931413\n",
      "FOLD: 6, EPOCH: 12, valid_loss: 0.019562008862312023\n",
      "FOLD: 6, EPOCH: 13, train_loss: 0.01930519301645659\n",
      "FOLD: 6, EPOCH: 13, valid_loss: 0.019392884551332548\n",
      "FOLD: 6, EPOCH: 14, train_loss: 0.019157464842538576\n",
      "FOLD: 6, EPOCH: 14, valid_loss: 0.01961417295611822\n",
      "FOLD: 6, EPOCH: 15, train_loss: 0.01902883152502614\n",
      "FOLD: 6, EPOCH: 15, valid_loss: 0.01935536342744644\n",
      "FOLD: 6, EPOCH: 16, train_loss: 0.018931707996572997\n",
      "FOLD: 6, EPOCH: 16, valid_loss: 0.019367372187284324\n",
      "FOLD: 6, EPOCH: 17, train_loss: 0.018781577564171842\n",
      "FOLD: 6, EPOCH: 17, valid_loss: 0.019220932028614558\n",
      "FOLD: 6, EPOCH: 18, train_loss: 0.018662971897503815\n",
      "FOLD: 6, EPOCH: 18, valid_loss: 0.019236273920306794\n",
      "FOLD: 6, EPOCH: 19, train_loss: 0.018574723948699398\n",
      "FOLD: 6, EPOCH: 19, valid_loss: 0.019224249113064546\n",
      "FOLD: 6, EPOCH: 20, train_loss: 0.01843154221471097\n",
      "FOLD: 6, EPOCH: 20, valid_loss: 0.0192005902242202\n",
      "FOLD: 6, EPOCH: 21, train_loss: 0.018308364761036797\n",
      "FOLD: 6, EPOCH: 21, valid_loss: 0.019063435638180144\n",
      "FOLD: 6, EPOCH: 22, train_loss: 0.018186319694929832\n",
      "FOLD: 6, EPOCH: 22, valid_loss: 0.01923044899908396\n",
      "FOLD: 6, EPOCH: 23, train_loss: 0.018060468816878023\n",
      "FOLD: 6, EPOCH: 23, valid_loss: 0.019034963817550585\n",
      "FOLD: 6, EPOCH: 24, train_loss: 0.01792908610927092\n",
      "FOLD: 6, EPOCH: 24, valid_loss: 0.019020960308038272\n",
      "FOLD: 6, EPOCH: 25, train_loss: 0.017767426029250428\n",
      "FOLD: 6, EPOCH: 25, valid_loss: 0.01901987811120657\n",
      "FOLD: 6, EPOCH: 26, train_loss: 0.01764108121042719\n",
      "FOLD: 6, EPOCH: 26, valid_loss: 0.018998511995260533\n",
      "FOLD: 6, EPOCH: 27, train_loss: 0.01752782250578339\n",
      "FOLD: 6, EPOCH: 27, valid_loss: 0.019002641622836772\n",
      "FOLD: 6, EPOCH: 28, train_loss: 0.01747277834628885\n",
      "FOLD: 6, EPOCH: 28, valid_loss: 0.019003065732809212\n",
      "FOLD: 6, EPOCH: 29, train_loss: 0.017323547848374456\n",
      "FOLD: 6, EPOCH: 29, valid_loss: 0.018932626940883122\n",
      "FOLD: 6, EPOCH: 30, train_loss: 0.017155912062908348\n",
      "FOLD: 6, EPOCH: 30, valid_loss: 0.018947313754604414\n",
      "FOLD: 6, EPOCH: 31, train_loss: 0.017060173285269254\n",
      "FOLD: 6, EPOCH: 31, valid_loss: 0.018912859404316314\n",
      "FOLD: 6, EPOCH: 32, train_loss: 0.016856049867095175\n",
      "FOLD: 6, EPOCH: 32, valid_loss: 0.018928557634353638\n",
      "FOLD: 6, EPOCH: 33, train_loss: 0.016794287612208643\n",
      "FOLD: 6, EPOCH: 33, valid_loss: 0.018900915693778258\n",
      "FOLD: 6, EPOCH: 34, train_loss: 0.01661590255192808\n",
      "FOLD: 6, EPOCH: 34, valid_loss: 0.018937745059912022\n",
      "FOLD: 6, EPOCH: 35, train_loss: 0.0164772694980776\n",
      "FOLD: 6, EPOCH: 35, valid_loss: 0.018921959715393875\n",
      "FOLD: 6, EPOCH: 36, train_loss: 0.016341814758709154\n",
      "FOLD: 6, EPOCH: 36, valid_loss: 0.01891691194703946\n",
      "Epoch    37: reducing learning rate of group 0 to 2.6653e-02.\n",
      "FOLD: 6, EPOCH: 37, train_loss: 0.015990765035353804\n",
      "FOLD: 6, EPOCH: 37, valid_loss: 0.01886824805002946\n",
      "FOLD: 6, EPOCH: 38, train_loss: 0.01588682534575865\n",
      "FOLD: 6, EPOCH: 38, valid_loss: 0.018849488347768784\n",
      "FOLD: 6, EPOCH: 39, train_loss: 0.015800647211034555\n",
      "FOLD: 6, EPOCH: 39, valid_loss: 0.018841091113594863\n",
      "FOLD: 6, EPOCH: 40, train_loss: 0.015736853464733105\n",
      "FOLD: 6, EPOCH: 40, valid_loss: 0.018849088738744076\n",
      "FOLD: 6, EPOCH: 41, train_loss: 0.01566654584697775\n",
      "FOLD: 6, EPOCH: 41, valid_loss: 0.018813156307889864\n",
      "FOLD: 6, EPOCH: 42, train_loss: 0.015602846478892339\n",
      "FOLD: 6, EPOCH: 42, valid_loss: 0.018831334864863984\n",
      "FOLD: 6, EPOCH: 43, train_loss: 0.015521241930892339\n",
      "FOLD: 6, EPOCH: 43, valid_loss: 0.018821431467166312\n",
      "FOLD: 6, EPOCH: 44, train_loss: 0.0155655349906836\n",
      "FOLD: 6, EPOCH: 44, valid_loss: 0.018800718136704885\n",
      "FOLD: 6, EPOCH: 45, train_loss: 0.015501134424797586\n",
      "FOLD: 6, EPOCH: 45, valid_loss: 0.018825219227717474\n",
      "FOLD: 6, EPOCH: 46, train_loss: 0.015438237290426687\n",
      "FOLD: 6, EPOCH: 46, valid_loss: 0.018819645047187805\n",
      "FOLD: 6, EPOCH: 47, train_loss: 0.015423192453847543\n",
      "FOLD: 6, EPOCH: 47, valid_loss: 0.01882516650053171\n",
      "Epoch    48: reducing learning rate of group 0 to 3.9889e-03.\n",
      "FOLD: 6, EPOCH: 48, train_loss: 0.015377436940734452\n",
      "FOLD: 6, EPOCH: 48, valid_loss: 0.01882402799450434\n",
      "FOLD: 6, EPOCH: 49, train_loss: 0.015372885630239506\n",
      "FOLD: 6, EPOCH: 49, valid_loss: 0.018796747980209496\n"
     ]
    }
   ],
   "source": [
    "# Averaging on multiple SEEDS\n",
    "\n",
    "SEED = [7, 11, 13, 17, 19, 1881, 1903]\n",
    "oof = np.zeros((len(train), len(target_cols)))\n",
    "predictions = np.zeros((len(test), len(target_cols)))\n",
    "\n",
    "for seed in SEED:\n",
    "    \n",
    "    oof_, predictions_ = run_k_fold(NFOLDS, params_226, seed)\n",
    "    oof += oof_ / len(SEED)\n",
    "    predictions += predictions_ / len(SEED)\n",
    "\n",
    "train[target_cols] = oof\n",
    "test[target_cols] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T11:22:10.152735Z",
     "iopub.status.busy": "2020-10-24T11:22:10.151341Z",
     "iopub.status.idle": "2020-10-24T11:22:11.188632Z",
     "shell.execute_reply": "2020-10-24T11:22:11.189073Z"
    },
    "papermill": {
     "duration": 1.337682,
     "end_time": "2020-10-24T11:22:11.189231",
     "exception": false,
     "start_time": "2020-10-24T11:22:09.851549",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV log_loss:  0.014435448793879781\n"
     ]
    }
   ],
   "source": [
    "valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n",
    "\n",
    "y_true = train_targets_scored_forCV[target_cols].values\n",
    "y_pred = valid_results[target_cols].values\n",
    "\n",
    "score = 0\n",
    "for i in range(len(target_cols)):\n",
    "    score_ = log_loss(y_true[:, i], y_pred[:, i])\n",
    "    score += score_ / target.shape[1]\n",
    "    \n",
    "print(\"CV log_loss: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-10-24T11:22:11.804849Z",
     "iopub.status.busy": "2020-10-24T11:22:11.803290Z",
     "iopub.status.idle": "2020-10-24T11:22:14.014015Z",
     "shell.execute_reply": "2020-10-24T11:22:14.013426Z"
    },
    "papermill": {
     "duration": 2.549373,
     "end_time": "2020-10-24T11:22:14.014191",
     "exception": false,
     "start_time": "2020-10-24T11:22:11.464818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n",
    "sub.to_csv('submission1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "duration": 895.207764,
   "end_time": "2020-10-24T11:22:15.894050",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-10-24T11:07:20.686286",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
