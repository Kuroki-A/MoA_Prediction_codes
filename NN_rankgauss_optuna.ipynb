{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2020-11-04T02:19:26.305911Z",
     "iopub.status.busy": "2020-11-04T02:19:26.305082Z",
     "iopub.status.idle": "2020-11-04T02:19:27.325845Z",
     "shell.execute_reply": "2020-11-04T02:19:27.324539Z"
    },
    "papermill": {
     "duration": 1.06069,
     "end_time": "2020-11-04T02:19:27.325966",
     "exception": false,
     "start_time": "2020-11-04T02:19:26.265276",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../input/iterativestratification')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2020-11-04T02:19:27.403496Z",
     "iopub.status.busy": "2020-11-04T02:19:27.402685Z",
     "iopub.status.idle": "2020-11-04T02:19:29.072073Z",
     "shell.execute_reply": "2020-11-04T02:19:29.071019Z"
    },
    "papermill": {
     "duration": 1.710328,
     "end_time": "2020-11-04T02:19:29.072204",
     "exception": false,
     "start_time": "2020-11-04T02:19:27.361876",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T02:19:29.147795Z",
     "iopub.status.busy": "2020-11-04T02:19:29.146427Z",
     "iopub.status.idle": "2020-11-04T02:19:29.148575Z",
     "shell.execute_reply": "2020-11-04T02:19:29.149085Z"
    },
    "papermill": {
     "duration": 0.041995,
     "end_time": "2020-11-04T02:19:29.149234",
     "exception": false,
     "start_time": "2020-11-04T02:19:29.107239",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2020-11-04T02:19:29.309816Z",
     "iopub.status.busy": "2020-11-04T02:19:29.309164Z",
     "iopub.status.idle": "2020-11-04T02:19:36.620016Z",
     "shell.execute_reply": "2020-11-04T02:19:36.618715Z"
    },
    "papermill": {
     "duration": 7.352751,
     "end_time": "2020-11-04T02:19:36.620151",
     "exception": false,
     "start_time": "2020-11-04T02:19:29.267400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('../lish-moa/train_features.csv')\n",
    "train_targets_scored = pd.read_csv('../lish-moa/train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv('../lish-moa/train_targets_nonscored.csv')\n",
    "\n",
    "test_features = pd.read_csv('../lish-moa/test_features.csv')\n",
    "sample_submission = pd.read_csv('../lish-moa/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T02:19:36.696941Z",
     "iopub.status.busy": "2020-11-04T02:19:36.695209Z",
     "iopub.status.idle": "2020-11-04T02:19:36.697756Z",
     "shell.execute_reply": "2020-11-04T02:19:36.698340Z"
    },
    "papermill": {
     "duration": 0.043663,
     "end_time": "2020-11-04T02:19:36.698480",
     "exception": false,
     "start_time": "2020-11-04T02:19:36.654817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in train_features.columns if col.startswith('c-')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T02:19:44.659734Z",
     "iopub.status.busy": "2020-11-04T02:19:44.658944Z",
     "iopub.status.idle": "2020-11-04T02:19:53.887491Z",
     "shell.execute_reply": "2020-11-04T02:19:53.886883Z"
    },
    "papermill": {
     "duration": 9.277763,
     "end_time": "2020-11-04T02:19:53.887603",
     "exception": false,
     "start_time": "2020-11-04T02:19:44.609840",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#RankGauss\n",
    "\n",
    "for col in (GENES + CELLS):\n",
    "\n",
    "    transformer = QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\")\n",
    "    vec_len = len(train_features[col].values)\n",
    "    vec_len_test = len(test_features[col].values)\n",
    "    raw_vec = train_features[col].values.reshape(vec_len, 1)\n",
    "    transformer.fit(raw_vec)\n",
    "\n",
    "    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n",
    "    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T02:19:54.051155Z",
     "iopub.status.busy": "2020-11-04T02:19:54.050241Z",
     "iopub.status.idle": "2020-11-04T02:19:54.056475Z",
     "shell.execute_reply": "2020-11-04T02:19:54.055881Z"
    },
    "papermill": {
     "duration": 0.052274,
     "end_time": "2020-11-04T02:19:54.056573",
     "exception": false,
     "start_time": "2020-11-04T02:19:54.004299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.043849,
     "end_time": "2020-11-04T02:20:01.332146",
     "exception": false,
     "start_time": "2020-11-04T02:20:01.288297",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# PCA features + Existing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T02:20:01.475540Z",
     "iopub.status.busy": "2020-11-04T02:20:01.474602Z",
     "iopub.status.idle": "2020-11-04T02:20:13.411769Z",
     "shell.execute_reply": "2020-11-04T02:20:13.410746Z"
    },
    "papermill": {
     "duration": 12.035152,
     "end_time": "2020-11-04T02:20:13.411885",
     "exception": false,
     "start_time": "2020-11-04T02:20:01.376733",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GENES\n",
    "n_comp = 600  #<--Update\n",
    "\n",
    "data = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\n",
    "data2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\n",
    "train2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n",
    "\n",
    "train2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "test2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n",
    "\n",
    "# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\n",
    "train_features = pd.concat((train_features, train2), axis=1)\n",
    "test_features = pd.concat((test_features, test2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T02:20:13.519579Z",
     "iopub.status.busy": "2020-11-04T02:20:13.518286Z",
     "iopub.status.idle": "2020-11-04T02:20:14.430308Z",
     "shell.execute_reply": "2020-11-04T02:20:14.445596Z"
    },
    "papermill": {
     "duration": 0.98798,
     "end_time": "2020-11-04T02:20:14.447865",
     "exception": false,
     "start_time": "2020-11-04T02:20:13.459885",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#CELLS\n",
    "n_comp = 50  #<--Update\n",
    "\n",
    "data = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\n",
    "data2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\n",
    "train2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n",
    "\n",
    "train2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n",
    "test2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n",
    "\n",
    "# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\n",
    "train_features = pd.concat((train_features, train2), axis=1)\n",
    "test_features = pd.concat((test_features, test2), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.06148,
     "end_time": "2020-11-04T02:20:14.717600",
     "exception": false,
     "start_time": "2020-11-04T02:20:14.656120",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# feature Selection using Variance Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T02:20:14.843556Z",
     "iopub.status.busy": "2020-11-04T02:20:14.842670Z",
     "iopub.status.idle": "2020-11-04T02:20:15.935138Z",
     "shell.execute_reply": "2020-11-04T02:20:15.934514Z"
    },
    "papermill": {
     "duration": 1.163504,
     "end_time": "2020-11-04T02:20:15.935267",
     "exception": false,
     "start_time": "2020-11-04T02:20:14.771763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "\n",
    "var_thresh = VarianceThreshold(0.8)  #<-- Update\n",
    "data = train_features.append(test_features)\n",
    "data_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n",
    "\n",
    "train_features_transformed = data_transformed[ : train_features.shape[0]]\n",
    "test_features_transformed = data_transformed[-test_features.shape[0] : ]\n",
    "\n",
    "\n",
    "train_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n",
    "                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "\n",
    "train_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n",
    "\n",
    "\n",
    "test_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n",
    "                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n",
    "\n",
    "test_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T02:20:16.043945Z",
     "iopub.status.busy": "2020-11-04T02:20:16.042552Z",
     "iopub.status.idle": "2020-11-04T02:20:16.383293Z",
     "shell.execute_reply": "2020-11-04T02:20:16.382662Z"
    },
    "papermill": {
     "duration": 0.396128,
     "end_time": "2020-11-04T02:20:16.383422",
     "exception": false,
     "start_time": "2020-11-04T02:20:15.987294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train_features.merge(train_targets_scored, on='sig_id')\n",
    "train = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "test = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n",
    "\n",
    "target = train[train_targets_scored.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T02:20:16.557393Z",
     "iopub.status.busy": "2020-11-04T02:20:16.534718Z",
     "iopub.status.idle": "2020-11-04T02:20:16.560155Z",
     "shell.execute_reply": "2020-11-04T02:20:16.559594Z"
    },
    "papermill": {
     "duration": 0.129789,
     "end_time": "2020-11-04T02:20:16.560271",
     "exception": false,
     "start_time": "2020-11-04T02:20:16.430482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train.drop('cp_type', axis=1)\n",
    "test = test.drop('cp_type', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T02:20:18.108523Z",
     "iopub.status.busy": "2020-11-04T02:20:18.106660Z",
     "iopub.status.idle": "2020-11-04T02:20:18.109194Z",
     "shell.execute_reply": "2020-11-04T02:20:18.109654Z"
    },
    "papermill": {
     "duration": 0.067427,
     "end_time": "2020-11-04T02:20:18.109788",
     "exception": false,
     "start_time": "2020-11-04T02:20:18.042361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_cols = target.drop('sig_id', axis=1).columns.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.048123,
     "end_time": "2020-11-04T02:20:18.205644",
     "exception": false,
     "start_time": "2020-11-04T02:20:18.157521",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# CV folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T02:20:18.311189Z",
     "iopub.status.busy": "2020-11-04T02:20:18.309810Z",
     "iopub.status.idle": "2020-11-04T02:20:21.175141Z",
     "shell.execute_reply": "2020-11-04T02:20:21.175810Z"
    },
    "papermill": {
     "duration": 2.922181,
     "end_time": "2020-11-04T02:20:21.175999",
     "exception": false,
     "start_time": "2020-11-04T02:20:18.253818",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "folds = train.copy()\n",
    "\n",
    "mskf = MultilabelStratifiedKFold(n_splits=7)\n",
    "\n",
    "for f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n",
    "    folds.loc[v_idx, 'kfold'] = int(f)\n",
    "\n",
    "folds['kfold'] = folds['kfold'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.04919,
     "end_time": "2020-11-04T02:20:21.278150",
     "exception": false,
     "start_time": "2020-11-04T02:20:21.228960",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T02:20:21.385725Z",
     "iopub.status.busy": "2020-11-04T02:20:21.384816Z",
     "iopub.status.idle": "2020-11-04T02:20:21.387641Z",
     "shell.execute_reply": "2020-11-04T02:20:21.387158Z"
    },
    "papermill": {
     "duration": 0.062186,
     "end_time": "2020-11-04T02:20:21.387741",
     "exception": false,
     "start_time": "2020-11-04T02:20:21.325555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MoADataset:\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
    "            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n",
    "        }\n",
    "        return dct\n",
    "    \n",
    "class TestDataset:\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n",
    "        }\n",
    "        return dct\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T02:20:21.500140Z",
     "iopub.status.busy": "2020-11-04T02:20:21.492736Z",
     "iopub.status.idle": "2020-11-04T02:20:21.503153Z",
     "shell.execute_reply": "2020-11-04T02:20:21.502487Z"
    },
    "papermill": {
     "duration": 0.066242,
     "end_time": "2020-11-04T02:20:21.503261",
     "exception": false,
     "start_time": "2020-11-04T02:20:21.437019",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    \n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "#         print(inputs.shape)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    \n",
    "    return final_loss\n",
    "\n",
    "\n",
    "def valid_fn(model, loss_fn, dataloader, device):\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    valid_preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    final_loss /= len(dataloader)\n",
    "    valid_preds = np.concatenate(valid_preds)\n",
    "    \n",
    "    return final_loss, valid_preds\n",
    "\n",
    "def inference_fn(model, dataloader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    \n",
    "    for data in dataloader:\n",
    "        inputs = data['x'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "        \n",
    "        preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "        \n",
    "    preds = np.concatenate(preds)\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T02:20:21.612790Z",
     "iopub.status.busy": "2020-11-04T02:20:21.611882Z",
     "iopub.status.idle": "2020-11-04T02:20:21.614806Z",
     "shell.execute_reply": "2020-11-04T02:20:21.614291Z"
    },
    "papermill": {
     "duration": 0.062978,
     "end_time": "2020-11-04T02:20:21.614909",
     "exception": false,
     "start_time": "2020-11-04T02:20:21.551931",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "            self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n",
    "\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.050242,
     "end_time": "2020-11-04T02:20:21.714637",
     "exception": false,
     "start_time": "2020-11-04T02:20:21.664395",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T02:20:21.838582Z",
     "iopub.status.busy": "2020-11-04T02:20:21.837804Z",
     "iopub.status.idle": "2020-11-04T02:20:21.840961Z",
     "shell.execute_reply": "2020-11-04T02:20:21.840486Z"
    },
    "papermill": {
     "duration": 0.074471,
     "end_time": "2020-11-04T02:20:21.841077",
     "exception": false,
     "start_time": "2020-11-04T02:20:21.766606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features, num_targets, hidden_size, dropout2, dropout3, negative_slope1, negative_slope2):\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(dropout2)\n",
    "        self.leaky_relu1 = nn.LeakyReLU(negative_slope1)\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout3 = nn.Dropout(dropout3)\n",
    "        self.leaky_relu2 = nn.LeakyReLU(negative_slope2)\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n",
    "        \n",
    "    def recalibrate_layer(self, layer):\n",
    "\n",
    "        if(torch.isnan(layer.weight_v).sum() > 0):\n",
    "            print ('recalibrate layer.weight_v')\n",
    "            layer.weight_v = torch.nn.Parameter(torch.where(torch.isnan(layer.weight_v), torch.zeros_like(layer.weight_v), layer.weight_v))\n",
    "            layer.weight_v = torch.nn.Parameter(layer.weight_v + 1e-7)\n",
    "\n",
    "        if(torch.isnan(layer.weight).sum() > 0):\n",
    "            print ('recalibrate layer.weight')\n",
    "            layer.weight = torch.where(torch.isnan(layer.weight), torch.zeros_like(layer.weight), layer.weight)\n",
    "            layer.weight += 1e-7\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        #self.recalibrate_layer(self.dense1)\n",
    "        x = self.leaky_relu1(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        #self.recalibrate_layer(self.dense2)\n",
    "        x = self.leaky_relu2(self.dense2(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            # true_dist = pred.data.clone()\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.048288,
     "end_time": "2020-11-04T02:20:21.937964",
     "exception": false,
     "start_time": "2020-11-04T02:20:21.889676",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T02:20:22.040223Z",
     "iopub.status.busy": "2020-11-04T02:20:22.039365Z",
     "iopub.status.idle": "2020-11-04T02:20:22.042266Z",
     "shell.execute_reply": "2020-11-04T02:20:22.041704Z"
    },
    "papermill": {
     "duration": 0.055802,
     "end_time": "2020-11-04T02:20:22.042361",
     "exception": false,
     "start_time": "2020-11-04T02:20:21.986559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T02:20:22.148596Z",
     "iopub.status.busy": "2020-11-04T02:20:22.147303Z",
     "iopub.status.idle": "2020-11-04T02:20:22.340630Z",
     "shell.execute_reply": "2020-11-04T02:20:22.341136Z"
    },
    "papermill": {
     "duration": 0.249148,
     "end_time": "2020-11-04T02:20:22.341271",
     "exception": false,
     "start_time": "2020-11-04T02:20:22.092123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_cols = [c for c in process_data(folds).columns if c not in target_cols]\n",
    "feature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T02:20:22.810305Z",
     "iopub.status.busy": "2020-11-04T02:20:22.809524Z",
     "iopub.status.idle": "2020-11-04T02:20:22.813137Z",
     "shell.execute_reply": "2020-11-04T02:20:22.812581Z"
    },
    "papermill": {
     "duration": 0.422622,
     "end_time": "2020-11-04T02:20:22.813257",
     "exception": false,
     "start_time": "2020-11-04T02:20:22.390635",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# HyperParameters\n",
    "\n",
    "DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "EPOCHS = 50\n",
    "#BATCH_SIZE = 192\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "NFOLDS = 7           \n",
    "EARLY_STOPPING_STEPS = 10\n",
    "EARLY_STOP = False\n",
    "\n",
    "num_features=len(feature_cols)\n",
    "num_targets=len(target_cols)\n",
    "#hidden_size=1500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.049477,
     "end_time": "2020-11-04T02:20:22.912178",
     "exception": false,
     "start_time": "2020-11-04T02:20:22.862701",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Single fold training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T02:20:23.036857Z",
     "iopub.status.busy": "2020-11-04T02:20:23.035955Z",
     "iopub.status.idle": "2020-11-04T02:20:23.039277Z",
     "shell.execute_reply": "2020-11-04T02:20:23.038750Z"
    },
    "papermill": {
     "duration": 0.078098,
     "end_time": "2020-11-04T02:20:23.039384",
     "exception": false,
     "start_time": "2020-11-04T02:20:22.961286",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_training(fold, params):\n",
    "    \n",
    "    seed_everything(1)\n",
    "    \n",
    "    train = process_data(folds)\n",
    "    test_ = process_data(test)\n",
    "    \n",
    "    trn_idx = train[train['kfold'] != fold].index\n",
    "    val_idx = train[train['kfold'] == fold].index\n",
    "    \n",
    "    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n",
    "    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n",
    "    \n",
    "    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n",
    "    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n",
    "    \n",
    "    train_dataset = MoADataset(x_train, y_train)\n",
    "    valid_dataset = MoADataset(x_valid, y_valid)\n",
    "    \n",
    "    batch_size_params = params['batch_size']\n",
    "    \n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size_params, shuffle=True)\n",
    "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size_params, shuffle=False)\n",
    "    \n",
    "    model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets=num_targets,\n",
    "        hidden_size=params['hidden_size'],\n",
    "        dropout2=params['dropout2'],\n",
    "        dropout3=params['dropout3'],\n",
    "        negative_slope1=params['negative_slope1'],\n",
    "        negative_slope2=params['negative_slope2']\n",
    "    )\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    #scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n",
    "    #                                          max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n",
    "    \n",
    "    lr_params = params['lr']\n",
    "    lr_decay_params = params['lr_decay']\n",
    "    weight_decay_params = params['weight_decay']\n",
    "    \n",
    "    factor_params = params['factor']\n",
    "    patience_params = params['patience']\n",
    "    \n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=lr_params, lr_decay=lr_decay_params, weight_decay=weight_decay_params, initial_accumulator_value=0, eps=1e-10)  \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=factor_params, patience=patience_params, threshold=1e-8, eps=1e-10, verbose=True)\n",
    "    \n",
    "    smoothing_params = params['smoothing']\n",
    "    \n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    loss_tr = SmoothBCEwLogits(smoothing = smoothing_params)\n",
    "    \n",
    "    early_stopping_steps = EARLY_STOPPING_STEPS\n",
    "    early_step = 0\n",
    "   \n",
    "    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n",
    "        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n",
    "        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n",
    "        print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n",
    "        \n",
    "        scheduler.step(valid_loss)\n",
    "        \n",
    "        if valid_loss < best_loss:\n",
    "            \n",
    "            best_loss = valid_loss\n",
    "            oof[val_idx] = valid_preds\n",
    "            #torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n",
    "        \n",
    "        elif(EARLY_STOP == True):\n",
    "            \n",
    "            early_step += 1\n",
    "            if (early_step >= early_stopping_steps):\n",
    "                break\n",
    "            \n",
    "    \n",
    "    #--------------------- PREDICTION---------------------\n",
    "    #x_test = test_[feature_cols].values\n",
    "    #testdataset = TestDataset(x_test)\n",
    "    #testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    #model = Model(\n",
    "    #    num_features=num_features,\n",
    "    #    num_targets=num_targets,\n",
    "    #    hidden_size=hidden_size,\n",
    "\n",
    "    #)\n",
    "    \n",
    "    #model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n",
    "    #model.to(DEVICE)\n",
    "    \n",
    "    #predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n",
    "    #predictions = inference_fn(model, testloader, DEVICE)\n",
    "    \n",
    "    #return oof, predictions\n",
    "    return best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna import Trial, create_study\n",
    "\n",
    "params={}\n",
    "\n",
    "def get_nn_parameter_suggestions(trial: Trial) -> dict:\n",
    "    return {\n",
    "        \"hidden_size\": trial.suggest_int(\"hidden_size\", 512, 2560, step = 256, log = False), #range_update\n",
    "        \"dropout2\": trial.suggest_float(\"dropout2\", 0.1, 0.95, step = None, log = False), #range_update\n",
    "        \"dropout3\": trial.suggest_float(\"dropout3\", 0.1, 0.95, step = None, log = False), #range_update\n",
    "        \"negative_slope1\":trial.suggest_float(\"negative_slope1\", 0.001, 0.1, step = 0.001, log = False),\n",
    "        \"negative_slope2\":trial.suggest_float(\"negative_slope2\", 0.001, 0.1, step = 0.001, log = False),\n",
    "        \"lr\": trial.suggest_float(\"lr\", 5e-2, 5e-1, step = None, log = False),\n",
    "        \"lr_decay\": trial.suggest_float(\"lr_decay\", 5e-4, 1e-2, step = None, log = False), #range_update\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\",1e-6, 5e-5, step = None, log = False), #range_update\n",
    "        \"factor\": trial.suggest_float(\"factor\",0.1, 0.9, step = None, log = False),\n",
    "        \"patience\": trial.suggest_int(\"patience\",1, 10, step = 1, log =False),\n",
    "        \"batch_size\":trial.suggest_int(\"batch_size\",128, 512, step = 64, log =False),\n",
    "        \"smoothing\":trial.suggest_float(\"smoothing\",0.001, 0.01, step = 0.001, log =False)\n",
    "    }\n",
    "\n",
    "def objective(trial: Trial) -> dict:\n",
    "    \n",
    "    _params = get_nn_parameter_suggestions(trial)\n",
    "    \n",
    "    all_losses=[]\n",
    "    for f_ in range(7): #<-- 5folds\n",
    "        temp_loss=run_training(f_,params=_params)\n",
    "        all_losses.append(temp_loss)\n",
    "    return np.mean(all_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2020-11-12 09:57:10,951]\u001b[0m A new study created in memory with name: no-name-f8f41e2f-fc05-4df1-bfd3-063cccf561e8\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 0, EPOCH: 0, train_loss: 0.0867878794670105\n",
      "FOLD: 0, EPOCH: 0, valid_loss: 0.023894680131758963\n",
      "FOLD: 0, EPOCH: 1, train_loss: 0.046229043265893346\n",
      "FOLD: 0, EPOCH: 1, valid_loss: 0.02283282551382269\n",
      "FOLD: 0, EPOCH: 2, train_loss: 0.04548271665615695\n",
      "FOLD: 0, EPOCH: 2, valid_loss: 0.022579885220953395\n",
      "FOLD: 0, EPOCH: 3, train_loss: 0.0445960954363857\n",
      "FOLD: 0, EPOCH: 3, valid_loss: 0.02208471218390124\n",
      "FOLD: 0, EPOCH: 4, train_loss: 0.0444127352287372\n",
      "FOLD: 0, EPOCH: 4, valid_loss: 0.022495011932083538\n",
      "FOLD: 0, EPOCH: 5, train_loss: 0.04415704097066607\n",
      "FOLD: 0, EPOCH: 5, valid_loss: 0.02337102565382208\n",
      "FOLD: 0, EPOCH: 6, train_loss: 0.044041545529450686\n",
      "FOLD: 0, EPOCH: 6, valid_loss: 0.021748279354401996\n",
      "FOLD: 0, EPOCH: 7, train_loss: 0.04393211805394718\n",
      "FOLD: 0, EPOCH: 7, valid_loss: 0.02202375073518072\n",
      "FOLD: 0, EPOCH: 8, train_loss: 0.0437739807225409\n",
      "FOLD: 0, EPOCH: 8, valid_loss: 0.02143694566828864\n",
      "FOLD: 0, EPOCH: 9, train_loss: 0.04369424602815083\n",
      "FOLD: 0, EPOCH: 9, valid_loss: 0.022374683991074562\n",
      "FOLD: 0, EPOCH: 10, train_loss: 0.043654351450857665\n",
      "FOLD: 0, EPOCH: 10, valid_loss: 0.02149917371571064\n",
      "FOLD: 0, EPOCH: 11, train_loss: 0.04349291413312867\n",
      "FOLD: 0, EPOCH: 11, valid_loss: 0.021297208964824677\n",
      "FOLD: 0, EPOCH: 12, train_loss: 0.04345480618732316\n",
      "FOLD: 0, EPOCH: 12, valid_loss: 0.021049567897404944\n",
      "FOLD: 0, EPOCH: 13, train_loss: 0.04342297500088101\n",
      "FOLD: 0, EPOCH: 13, valid_loss: 0.021501345027770315\n",
      "FOLD: 0, EPOCH: 14, train_loss: 0.043392374579395564\n",
      "FOLD: 0, EPOCH: 14, valid_loss: 0.02120663252260004\n",
      "FOLD: 0, EPOCH: 15, train_loss: nan\n",
      "FOLD: 0, EPOCH: 15, valid_loss: nan\n",
      "FOLD: 0, EPOCH: 16, train_loss: nan\n",
      "FOLD: 0, EPOCH: 16, valid_loss: nan\n",
      "FOLD: 0, EPOCH: 17, train_loss: nan\n",
      "FOLD: 0, EPOCH: 17, valid_loss: nan\n",
      "FOLD: 0, EPOCH: 18, train_loss: nan\n",
      "FOLD: 0, EPOCH: 18, valid_loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-8da996760509>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"minimize\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m                 self._optimize_sequential(\n\u001b[0;32m--> 339\u001b[0;31m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m                 )\n\u001b[1;32m    341\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(self, func, n_trials, timeout, catch, callbacks, gc_after_trial, time_start)\u001b[0m\n\u001b[1;32m    745\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial_and_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_progress_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_seconds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial_and_callbacks\u001b[0;34m(self, func, catch, callbacks, gc_after_trial)\u001b[0m\n\u001b[1;32m    774\u001b[0m     ) -> None:\n\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m         \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/optuna/study.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(self, func, catch, gc_after_trial)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Trial {} pruned. {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-4f9a6a4a5f76>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mall_losses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mf_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#<-- 5folds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mtemp_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mall_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-6ba7247708b0>\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(fold, params)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-c94597f0ef2d>\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(model, optimizer, scheduler, loss_fn, dataloader, device)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfinal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-1cecb33b101c>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         dct = {\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0;34m'x'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0;34m'y'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         }\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "study=optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective,n_trials=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_192 = {'hidden_size': 1280, 'dropout2': 0.8160433144844258, 'dropout3': 0.36274350472386446, 'negative_slope1': 0.077, 'negative_slope2': 0.002, 'lr': 0.2415579776271606, 'lr_decay': 0.0030829965450683383, 'weight_decay': 1.6921803805971663e-06, 'factor': 0.6836043082432074, 'patience': 6, 'batch_size': 256, 'smoothing': 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_final_training(fold, params, seed):\n",
    "    \n",
    "    seed_everything(seed)\n",
    "    \n",
    "    train = process_data(folds)\n",
    "    test_ = process_data(test)\n",
    "    \n",
    "    trn_idx = train[train['kfold'] != fold].index\n",
    "    val_idx = train[train['kfold'] == fold].index\n",
    "    \n",
    "    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n",
    "    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n",
    "    \n",
    "    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n",
    "    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n",
    "    \n",
    "    train_dataset = MoADataset(x_train, y_train)\n",
    "    valid_dataset = MoADataset(x_valid, y_valid)\n",
    "    \n",
    "    batch_size_params = params['batch_size']\n",
    "    \n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size_params, shuffle=True)\n",
    "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size_params, shuffle=False)\n",
    "    \n",
    "    model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets=num_targets,\n",
    "        hidden_size=params['hidden_size'],\n",
    "        dropout2=params['dropout2'],\n",
    "        dropout3=params['dropout3'],\n",
    "        negative_slope1=params['negative_slope1'],\n",
    "        negative_slope2=params['negative_slope2']\n",
    "    )\n",
    "    \n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    #optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "    #scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n",
    "    #                                          max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n",
    "    \n",
    "    lr_params = params['lr']\n",
    "    lr_decay_params = params['lr_decay']\n",
    "    weight_decay_params = params['weight_decay']\n",
    "    \n",
    "    factor_params = params['factor']\n",
    "    patience_params = params['patience']\n",
    "    \n",
    "    optimizer = optim.Adagrad(model.parameters(), lr=lr_params, lr_decay=lr_decay_params, weight_decay=weight_decay_params, initial_accumulator_value=0, eps=1e-10)  \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=factor_params, patience=patience_params, threshold=1e-8, eps=1e-10, verbose=True)\n",
    "    \n",
    "    smoothing_params = params['smoothing']\n",
    "    \n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    loss_tr = SmoothBCEwLogits(smoothing = smoothing_params)\n",
    "    \n",
    "    early_stopping_steps = EARLY_STOPPING_STEPS\n",
    "    early_step = 0\n",
    "   \n",
    "    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n",
    "        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n",
    "        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n",
    "        print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n",
    "        \n",
    "        scheduler.step(valid_loss)\n",
    "        \n",
    "        if valid_loss < best_loss:\n",
    "            \n",
    "            best_loss = valid_loss\n",
    "            oof[val_idx] = valid_preds\n",
    "            torch.save(model.state_dict(), f\"pytorch-01859-rankgauss-SEED{seed}_FOLD{fold}_.pth\")\n",
    "        \n",
    "        elif(EARLY_STOP == True):\n",
    "            \n",
    "            early_step += 1\n",
    "            if (early_step >= early_stopping_steps):\n",
    "                break\n",
    "            \n",
    "    \n",
    "    #--------------------- PREDICTION---------------------\n",
    "    x_test = test_[feature_cols].values\n",
    "    testdataset = TestDataset(x_test)\n",
    "    testloader = torch.utils.data.DataLoader(testdataset, batch_size=batch_size_params, shuffle=False)\n",
    "    \n",
    "    #model = Model(\n",
    "    #    num_features=num_features,\n",
    "    #    num_targets=num_targets,\n",
    "    #    hidden_size=hidden_size,\n",
    "\n",
    "    #)\n",
    "    \n",
    "    model.load_state_dict(torch.load(f\"pytorch-01859-rankgauss-SEED{seed}_FOLD{fold}_.pth\"))\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n",
    "    predictions = inference_fn(model, testloader, DEVICE)\n",
    "    \n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T02:20:23.148222Z",
     "iopub.status.busy": "2020-11-04T02:20:23.147427Z",
     "iopub.status.idle": "2020-11-04T02:20:23.150374Z",
     "shell.execute_reply": "2020-11-04T02:20:23.149758Z"
    },
    "papermill": {
     "duration": 0.061591,
     "end_time": "2020-11-04T02:20:23.150470",
     "exception": false,
     "start_time": "2020-11-04T02:20:23.088879",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_k_fold(NFOLDS, params, seed):\n",
    "    oof = np.zeros((len(train), len(target_cols)))\n",
    "    predictions = np.zeros((len(test), len(target_cols)))\n",
    "    \n",
    "    for fold in range(NFOLDS):\n",
    "        oof_, pred_ = run_final_training(fold, params, seed)\n",
    "        \n",
    "        predictions += pred_ / NFOLDS\n",
    "        oof += oof_\n",
    "        \n",
    "    return oof, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T02:20:23.270646Z",
     "iopub.status.busy": "2020-11-04T02:20:23.269677Z",
     "iopub.status.idle": "2020-11-04T03:15:14.774406Z",
     "shell.execute_reply": "2020-11-04T03:15:14.773318Z"
    },
    "papermill": {
     "duration": 3291.569212,
     "end_time": "2020-11-04T03:15:14.774550",
     "exception": false,
     "start_time": "2020-11-04T02:20:23.205338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 0, EPOCH: 0, train_loss: 0.04423861001693719\n",
      "FOLD: 0, EPOCH: 0, valid_loss: 0.02054226054595067\n",
      "FOLD: 0, EPOCH: 1, train_loss: 0.022654341231729533\n",
      "FOLD: 0, EPOCH: 1, valid_loss: 0.018971411081460807\n",
      "FOLD: 0, EPOCH: 2, train_loss: 0.021762436205470883\n",
      "FOLD: 0, EPOCH: 2, valid_loss: 0.018321416125847742\n",
      "FOLD: 0, EPOCH: 3, train_loss: 0.021281203808816703\n",
      "FOLD: 0, EPOCH: 3, valid_loss: 0.017842077291928805\n",
      "FOLD: 0, EPOCH: 4, train_loss: 0.02097139082144241\n",
      "FOLD: 0, EPOCH: 4, valid_loss: 0.017662380893643085\n",
      "FOLD: 0, EPOCH: 5, train_loss: 0.020710097399313707\n",
      "FOLD: 0, EPOCH: 5, valid_loss: 0.01740763630144871\n",
      "FOLD: 0, EPOCH: 6, train_loss: 0.020528308845855093\n",
      "FOLD: 0, EPOCH: 6, valid_loss: 0.017080991743848875\n",
      "FOLD: 0, EPOCH: 7, train_loss: 0.020330401489863526\n",
      "FOLD: 0, EPOCH: 7, valid_loss: 0.016996620867687922\n",
      "FOLD: 0, EPOCH: 8, train_loss: 0.02018516357182651\n",
      "FOLD: 0, EPOCH: 8, valid_loss: 0.01696788455144717\n",
      "FOLD: 0, EPOCH: 9, train_loss: 0.01998929868175371\n",
      "FOLD: 0, EPOCH: 9, valid_loss: 0.016812024638056755\n",
      "FOLD: 0, EPOCH: 10, train_loss: 0.01991223732663973\n",
      "FOLD: 0, EPOCH: 10, valid_loss: 0.016749319333869677\n",
      "FOLD: 0, EPOCH: 11, train_loss: 0.019704489717008295\n",
      "FOLD: 0, EPOCH: 11, valid_loss: 0.016622785478830338\n",
      "FOLD: 0, EPOCH: 12, train_loss: 0.019594399636057584\n",
      "FOLD: 0, EPOCH: 12, valid_loss: 0.016585343660643466\n",
      "FOLD: 0, EPOCH: 13, train_loss: 0.019468538110723365\n",
      "FOLD: 0, EPOCH: 13, valid_loss: 0.016427793826621313\n",
      "FOLD: 0, EPOCH: 14, train_loss: 0.01933984403070566\n",
      "FOLD: 0, EPOCH: 14, valid_loss: 0.016320471508571736\n",
      "FOLD: 0, EPOCH: 15, train_loss: 0.019226265743978926\n",
      "FOLD: 0, EPOCH: 15, valid_loss: 0.016318710664143928\n",
      "FOLD: 0, EPOCH: 16, train_loss: 0.01908700505422579\n",
      "FOLD: 0, EPOCH: 16, valid_loss: 0.016323194552499514\n",
      "FOLD: 0, EPOCH: 17, train_loss: 0.0189545091896041\n",
      "FOLD: 0, EPOCH: 17, valid_loss: 0.016210659836920407\n",
      "FOLD: 0, EPOCH: 18, train_loss: 0.018871537233526643\n",
      "FOLD: 0, EPOCH: 18, valid_loss: 0.016115136372928437\n",
      "FOLD: 0, EPOCH: 19, train_loss: 0.01880928418422873\n",
      "FOLD: 0, EPOCH: 19, valid_loss: 0.01611577346920967\n",
      "FOLD: 0, EPOCH: 20, train_loss: 0.018657235479032672\n",
      "FOLD: 0, EPOCH: 20, valid_loss: 0.016089423846166868\n",
      "FOLD: 0, EPOCH: 21, train_loss: 0.018586414614440622\n",
      "FOLD: 0, EPOCH: 21, valid_loss: 0.01599581245906078\n",
      "FOLD: 0, EPOCH: 22, train_loss: 0.018469130620360374\n",
      "FOLD: 0, EPOCH: 22, valid_loss: 0.015997574664652348\n",
      "FOLD: 0, EPOCH: 23, train_loss: 0.018366657096791913\n",
      "FOLD: 0, EPOCH: 23, valid_loss: 0.015956486718585856\n",
      "FOLD: 0, EPOCH: 24, train_loss: 0.018272069685563847\n",
      "FOLD: 0, EPOCH: 24, valid_loss: 0.01590605603101162\n",
      "FOLD: 0, EPOCH: 25, train_loss: 0.018149568397249724\n",
      "FOLD: 0, EPOCH: 25, valid_loss: 0.015954859698048003\n",
      "FOLD: 0, EPOCH: 26, train_loss: 0.018096071970019792\n",
      "FOLD: 0, EPOCH: 26, valid_loss: 0.01593490245823677\n",
      "FOLD: 0, EPOCH: 27, train_loss: 0.017943235995197617\n",
      "FOLD: 0, EPOCH: 27, valid_loss: 0.01592312094110709\n",
      "FOLD: 0, EPOCH: 28, train_loss: 0.017868564615177142\n",
      "FOLD: 0, EPOCH: 28, valid_loss: 0.01588498342495698\n",
      "FOLD: 0, EPOCH: 29, train_loss: 0.017732080776949186\n",
      "FOLD: 0, EPOCH: 29, valid_loss: 0.015873006616647426\n",
      "FOLD: 0, EPOCH: 30, train_loss: 0.017629379607938433\n",
      "FOLD: 0, EPOCH: 30, valid_loss: 0.01586977779292143\n",
      "FOLD: 0, EPOCH: 31, train_loss: 0.01754343625460122\n",
      "FOLD: 0, EPOCH: 31, valid_loss: 0.01583462251493564\n",
      "FOLD: 0, EPOCH: 32, train_loss: 0.01742474384907935\n",
      "FOLD: 0, EPOCH: 32, valid_loss: 0.01588267582253768\n",
      "FOLD: 0, EPOCH: 33, train_loss: 0.017314832544306647\n",
      "FOLD: 0, EPOCH: 33, valid_loss: 0.015849939332558557\n",
      "FOLD: 0, EPOCH: 34, train_loss: 0.017226983151222404\n",
      "FOLD: 0, EPOCH: 34, valid_loss: 0.01584993166705737\n",
      "FOLD: 0, EPOCH: 35, train_loss: 0.0171610285351808\n",
      "FOLD: 0, EPOCH: 35, valid_loss: 0.015812547161028936\n",
      "FOLD: 0, EPOCH: 36, train_loss: 0.01704592939868972\n",
      "FOLD: 0, EPOCH: 36, valid_loss: 0.015832914182773\n",
      "FOLD: 0, EPOCH: 37, train_loss: 0.016929461205428518\n",
      "FOLD: 0, EPOCH: 37, valid_loss: 0.015814009838952467\n",
      "FOLD: 0, EPOCH: 38, train_loss: 0.016911962698843028\n",
      "FOLD: 0, EPOCH: 38, valid_loss: 0.015834208506231125\n",
      "FOLD: 0, EPOCH: 39, train_loss: 0.01675340212327806\n",
      "FOLD: 0, EPOCH: 39, valid_loss: 0.015808773083755605\n",
      "FOLD: 0, EPOCH: 40, train_loss: 0.0167208239562004\n",
      "FOLD: 0, EPOCH: 40, valid_loss: 0.015808884914104756\n",
      "FOLD: 0, EPOCH: 41, train_loss: 0.01654260789321081\n",
      "FOLD: 0, EPOCH: 41, valid_loss: 0.015780066713117637\n",
      "FOLD: 0, EPOCH: 42, train_loss: 0.016447113587747555\n",
      "FOLD: 0, EPOCH: 42, valid_loss: 0.01580638510103409\n",
      "FOLD: 0, EPOCH: 43, train_loss: 0.016425098481311184\n",
      "FOLD: 0, EPOCH: 43, valid_loss: 0.01581239041227561\n",
      "FOLD: 0, EPOCH: 44, train_loss: 0.016296575563280163\n",
      "FOLD: 0, EPOCH: 44, valid_loss: 0.01580735009450179\n",
      "FOLD: 0, EPOCH: 45, train_loss: 0.016129742087041203\n",
      "FOLD: 0, EPOCH: 45, valid_loss: 0.015822950822229568\n",
      "FOLD: 0, EPOCH: 46, train_loss: 0.016073146369308233\n",
      "FOLD: 0, EPOCH: 46, valid_loss: 0.015818680708224956\n",
      "FOLD: 0, EPOCH: 47, train_loss: 0.0159940209516601\n",
      "FOLD: 0, EPOCH: 47, valid_loss: 0.01582310162484646\n",
      "FOLD: 0, EPOCH: 48, train_loss: 0.0158250273608074\n",
      "FOLD: 0, EPOCH: 48, valid_loss: 0.01587101981903498\n",
      "Epoch    49: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 0, EPOCH: 49, train_loss: 0.015619659947382437\n",
      "FOLD: 0, EPOCH: 49, valid_loss: 0.015823948841828566\n",
      "FOLD: 1, EPOCH: 0, train_loss: 0.043194008160483195\n",
      "FOLD: 1, EPOCH: 0, valid_loss: 0.020171862811996386\n",
      "FOLD: 1, EPOCH: 1, train_loss: 0.022797470006185608\n",
      "FOLD: 1, EPOCH: 1, valid_loss: 0.018793989546023883\n",
      "FOLD: 1, EPOCH: 2, train_loss: 0.02187813644775668\n",
      "FOLD: 1, EPOCH: 2, valid_loss: 0.017962401159680806\n",
      "FOLD: 1, EPOCH: 3, train_loss: 0.02134216096050836\n",
      "FOLD: 1, EPOCH: 3, valid_loss: 0.017478735401080206\n",
      "FOLD: 1, EPOCH: 4, train_loss: 0.020953913672348938\n",
      "FOLD: 1, EPOCH: 4, valid_loss: 0.01741542065372834\n",
      "FOLD: 1, EPOCH: 5, train_loss: 0.020708605672257976\n",
      "FOLD: 1, EPOCH: 5, valid_loss: 0.016957879353028078\n",
      "FOLD: 1, EPOCH: 6, train_loss: 0.02053306918792628\n",
      "FOLD: 1, EPOCH: 6, valid_loss: 0.016881476801175337\n",
      "FOLD: 1, EPOCH: 7, train_loss: 0.020309851454520546\n",
      "FOLD: 1, EPOCH: 7, valid_loss: 0.0167551410312836\n",
      "FOLD: 1, EPOCH: 8, train_loss: 0.020155981157881184\n",
      "FOLD: 1, EPOCH: 8, valid_loss: 0.016633301973342896\n",
      "FOLD: 1, EPOCH: 9, train_loss: 0.020018836281992292\n",
      "FOLD: 1, EPOCH: 9, valid_loss: 0.01649363238651019\n",
      "FOLD: 1, EPOCH: 10, train_loss: 0.019826395009216423\n",
      "FOLD: 1, EPOCH: 10, valid_loss: 0.01646354063772238\n",
      "FOLD: 1, EPOCH: 11, train_loss: 0.01969516360377138\n",
      "FOLD: 1, EPOCH: 11, valid_loss: 0.016316427347751763\n",
      "FOLD: 1, EPOCH: 12, train_loss: 0.019557543325464468\n",
      "FOLD: 1, EPOCH: 12, valid_loss: 0.016252757551578376\n",
      "FOLD: 1, EPOCH: 13, train_loss: 0.019459486762816842\n",
      "FOLD: 1, EPOCH: 13, valid_loss: 0.01624342605758172\n",
      "FOLD: 1, EPOCH: 14, train_loss: 0.01928726050096589\n",
      "FOLD: 1, EPOCH: 14, valid_loss: 0.01618906117689151\n",
      "FOLD: 1, EPOCH: 15, train_loss: 0.019212224243863207\n",
      "FOLD: 1, EPOCH: 15, valid_loss: 0.016099382335176833\n",
      "FOLD: 1, EPOCH: 16, train_loss: 0.01908259725550542\n",
      "FOLD: 1, EPOCH: 16, valid_loss: 0.016004247304338675\n",
      "FOLD: 1, EPOCH: 17, train_loss: 0.01900776385052784\n",
      "FOLD: 1, EPOCH: 17, valid_loss: 0.01596047729253769\n",
      "FOLD: 1, EPOCH: 18, train_loss: 0.01892003686343496\n",
      "FOLD: 1, EPOCH: 18, valid_loss: 0.015910530247940466\n",
      "FOLD: 1, EPOCH: 19, train_loss: 0.018752556445228087\n",
      "FOLD: 1, EPOCH: 19, valid_loss: 0.015885653332448922\n",
      "FOLD: 1, EPOCH: 20, train_loss: 0.01862801397471009\n",
      "FOLD: 1, EPOCH: 20, valid_loss: 0.015821800423929326\n",
      "FOLD: 1, EPOCH: 21, train_loss: 0.018549114214004698\n",
      "FOLD: 1, EPOCH: 21, valid_loss: 0.015832830363741286\n",
      "FOLD: 1, EPOCH: 22, train_loss: 0.018438090318562212\n",
      "FOLD: 1, EPOCH: 22, valid_loss: 0.01578229687248285\n",
      "FOLD: 1, EPOCH: 23, train_loss: 0.01835131967389906\n",
      "FOLD: 1, EPOCH: 23, valid_loss: 0.015760064913103215\n",
      "FOLD: 1, EPOCH: 24, train_loss: 0.01822111800917097\n",
      "FOLD: 1, EPOCH: 24, valid_loss: 0.015727035557994477\n",
      "FOLD: 1, EPOCH: 25, train_loss: 0.018152960475433518\n",
      "FOLD: 1, EPOCH: 25, valid_loss: 0.015727001815461196\n",
      "FOLD: 1, EPOCH: 26, train_loss: 0.018031365288471855\n",
      "FOLD: 1, EPOCH: 26, valid_loss: 0.015672099919846423\n",
      "FOLD: 1, EPOCH: 27, train_loss: 0.017962414167217305\n",
      "FOLD: 1, EPOCH: 27, valid_loss: 0.015630161103147727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 1, EPOCH: 28, train_loss: 0.017838430112680874\n",
      "FOLD: 1, EPOCH: 28, valid_loss: 0.01563360384450509\n",
      "FOLD: 1, EPOCH: 29, train_loss: 0.01768105278244695\n",
      "FOLD: 1, EPOCH: 29, valid_loss: 0.01564582000271632\n",
      "FOLD: 1, EPOCH: 30, train_loss: 0.01762270215093284\n",
      "FOLD: 1, EPOCH: 30, valid_loss: 0.015597246014154874\n",
      "FOLD: 1, EPOCH: 31, train_loss: 0.01752921213974824\n",
      "FOLD: 1, EPOCH: 31, valid_loss: 0.015615656685370665\n",
      "FOLD: 1, EPOCH: 32, train_loss: 0.017426894121878856\n",
      "FOLD: 1, EPOCH: 32, valid_loss: 0.015639893137491666\n",
      "FOLD: 1, EPOCH: 33, train_loss: 0.01736023197756023\n",
      "FOLD: 1, EPOCH: 33, valid_loss: 0.015631417457300883\n",
      "FOLD: 1, EPOCH: 34, train_loss: 0.01721292162767133\n",
      "FOLD: 1, EPOCH: 34, valid_loss: 0.015573990531265736\n",
      "FOLD: 1, EPOCH: 35, train_loss: 0.017114044695689872\n",
      "FOLD: 1, EPOCH: 35, valid_loss: 0.015551575387899693\n",
      "FOLD: 1, EPOCH: 36, train_loss: 0.017019923814144487\n",
      "FOLD: 1, EPOCH: 36, valid_loss: 0.015573673093548188\n",
      "FOLD: 1, EPOCH: 37, train_loss: 0.016923134580153872\n",
      "FOLD: 1, EPOCH: 37, valid_loss: 0.015572438518015238\n",
      "FOLD: 1, EPOCH: 38, train_loss: 0.016818115147887856\n",
      "FOLD: 1, EPOCH: 38, valid_loss: 0.015563151226020776\n",
      "FOLD: 1, EPOCH: 39, train_loss: 0.016695603615025412\n",
      "FOLD: 1, EPOCH: 39, valid_loss: 0.015587822390863528\n",
      "FOLD: 1, EPOCH: 40, train_loss: 0.01663794110503954\n",
      "FOLD: 1, EPOCH: 40, valid_loss: 0.015575623856141018\n",
      "FOLD: 1, EPOCH: 41, train_loss: 0.0165640267266615\n",
      "FOLD: 1, EPOCH: 41, valid_loss: 0.015581958855573948\n",
      "FOLD: 1, EPOCH: 42, train_loss: 0.01645022112171392\n",
      "FOLD: 1, EPOCH: 42, valid_loss: 0.015588238262213193\n",
      "Epoch    43: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 1, EPOCH: 43, train_loss: 0.01625732472166419\n",
      "FOLD: 1, EPOCH: 43, valid_loss: 0.0155604097705621\n",
      "FOLD: 1, EPOCH: 44, train_loss: 0.016126968709097522\n",
      "FOLD: 1, EPOCH: 44, valid_loss: 0.015563196645906338\n",
      "FOLD: 1, EPOCH: 45, train_loss: 0.016013352502439474\n",
      "FOLD: 1, EPOCH: 45, valid_loss: 0.015580949230262866\n",
      "FOLD: 1, EPOCH: 46, train_loss: 0.015918284415493946\n",
      "FOLD: 1, EPOCH: 46, valid_loss: 0.015593349790343871\n",
      "FOLD: 1, EPOCH: 47, train_loss: 0.01585402782704379\n",
      "FOLD: 1, EPOCH: 47, valid_loss: 0.015583539811464457\n",
      "FOLD: 1, EPOCH: 48, train_loss: 0.015759668564675627\n",
      "FOLD: 1, EPOCH: 48, valid_loss: 0.0155853469354602\n",
      "FOLD: 1, EPOCH: 49, train_loss: 0.0156572023784188\n",
      "FOLD: 1, EPOCH: 49, valid_loss: 0.015610323717387823\n",
      "Epoch    50: reducing learning rate of group 0 to 1.1288e-01.\n",
      "FOLD: 2, EPOCH: 0, train_loss: 0.044115684976851616\n",
      "FOLD: 2, EPOCH: 0, valid_loss: 0.020358082480155505\n",
      "FOLD: 2, EPOCH: 1, train_loss: 0.022865827119833714\n",
      "FOLD: 2, EPOCH: 1, valid_loss: 0.01911393108849342\n",
      "FOLD: 2, EPOCH: 2, train_loss: 0.021903886057033733\n",
      "FOLD: 2, EPOCH: 2, valid_loss: 0.018071165666557275\n",
      "FOLD: 2, EPOCH: 3, train_loss: 0.021312063131984825\n",
      "FOLD: 2, EPOCH: 3, valid_loss: 0.01768291426392702\n",
      "FOLD: 2, EPOCH: 4, train_loss: 0.021002995706087834\n",
      "FOLD: 2, EPOCH: 4, valid_loss: 0.017331092833326414\n",
      "FOLD: 2, EPOCH: 5, train_loss: 0.020760608761495835\n",
      "FOLD: 2, EPOCH: 5, valid_loss: 0.01733816243135012\n",
      "FOLD: 2, EPOCH: 6, train_loss: 0.02051117062266614\n",
      "FOLD: 2, EPOCH: 6, valid_loss: 0.01698046077329379\n",
      "FOLD: 2, EPOCH: 7, train_loss: 0.020358924527426023\n",
      "FOLD: 2, EPOCH: 7, valid_loss: 0.016867300853706323\n",
      "FOLD: 2, EPOCH: 8, train_loss: 0.02021301114881361\n",
      "FOLD: 2, EPOCH: 8, valid_loss: 0.016876740810962822\n",
      "FOLD: 2, EPOCH: 9, train_loss: 0.020015393459313625\n",
      "FOLD: 2, EPOCH: 9, valid_loss: 0.01665878639771388\n",
      "FOLD: 2, EPOCH: 10, train_loss: 0.01988653254670066\n",
      "FOLD: 2, EPOCH: 10, valid_loss: 0.016557851806282997\n",
      "FOLD: 2, EPOCH: 11, train_loss: 0.01975321120305641\n",
      "FOLD: 2, EPOCH: 11, valid_loss: 0.016490728666002933\n",
      "FOLD: 2, EPOCH: 12, train_loss: 0.019627796838412415\n",
      "FOLD: 2, EPOCH: 12, valid_loss: 0.016413979017390654\n",
      "FOLD: 2, EPOCH: 13, train_loss: 0.01950177663584819\n",
      "FOLD: 2, EPOCH: 13, valid_loss: 0.016390102557264842\n",
      "FOLD: 2, EPOCH: 14, train_loss: 0.019394223754470412\n",
      "FOLD: 2, EPOCH: 14, valid_loss: 0.016241651100034896\n",
      "FOLD: 2, EPOCH: 15, train_loss: 0.01926878922795122\n",
      "FOLD: 2, EPOCH: 15, valid_loss: 0.016201714626871623\n",
      "FOLD: 2, EPOCH: 16, train_loss: 0.019144338027045533\n",
      "FOLD: 2, EPOCH: 16, valid_loss: 0.01616041727650624\n",
      "FOLD: 2, EPOCH: 17, train_loss: 0.01905767521443399\n",
      "FOLD: 2, EPOCH: 17, valid_loss: 0.016052224601690587\n",
      "FOLD: 2, EPOCH: 18, train_loss: 0.018893651160839443\n",
      "FOLD: 2, EPOCH: 18, valid_loss: 0.016052219157035533\n",
      "FOLD: 2, EPOCH: 19, train_loss: 0.018796227192757902\n",
      "FOLD: 2, EPOCH: 19, valid_loss: 0.016020840607010402\n",
      "FOLD: 2, EPOCH: 20, train_loss: 0.018656800954124413\n",
      "FOLD: 2, EPOCH: 20, valid_loss: 0.01594564490593397\n",
      "FOLD: 2, EPOCH: 21, train_loss: 0.018594412258952052\n",
      "FOLD: 2, EPOCH: 21, valid_loss: 0.015894232389445487\n",
      "FOLD: 2, EPOCH: 22, train_loss: 0.01843414138499144\n",
      "FOLD: 2, EPOCH: 22, valid_loss: 0.015882843962082498\n",
      "FOLD: 2, EPOCH: 23, train_loss: 0.01838572378698233\n",
      "FOLD: 2, EPOCH: 23, valid_loss: 0.01582957811367053\n",
      "FOLD: 2, EPOCH: 24, train_loss: 0.01830758257592852\n",
      "FOLD: 2, EPOCH: 24, valid_loss: 0.015851098757523756\n",
      "FOLD: 2, EPOCH: 25, train_loss: 0.01820047657836128\n",
      "FOLD: 2, EPOCH: 25, valid_loss: 0.015822427705503427\n",
      "FOLD: 2, EPOCH: 26, train_loss: 0.018112067661776737\n",
      "FOLD: 2, EPOCH: 26, valid_loss: 0.015822663831596192\n",
      "FOLD: 2, EPOCH: 27, train_loss: 0.018018820499246184\n",
      "FOLD: 2, EPOCH: 27, valid_loss: 0.015754866413772106\n",
      "FOLD: 2, EPOCH: 28, train_loss: 0.017922148511216447\n",
      "FOLD: 2, EPOCH: 28, valid_loss: 0.015750500158621714\n",
      "FOLD: 2, EPOCH: 29, train_loss: 0.01785659443939457\n",
      "FOLD: 2, EPOCH: 29, valid_loss: 0.01575379238392298\n",
      "FOLD: 2, EPOCH: 30, train_loss: 0.017763789365621837\n",
      "FOLD: 2, EPOCH: 30, valid_loss: 0.01570177013770892\n",
      "FOLD: 2, EPOCH: 31, train_loss: 0.017600643310091785\n",
      "FOLD: 2, EPOCH: 31, valid_loss: 0.015756589002334155\n",
      "FOLD: 2, EPOCH: 32, train_loss: 0.01755246867400569\n",
      "FOLD: 2, EPOCH: 32, valid_loss: 0.015724587397506602\n",
      "FOLD: 2, EPOCH: 33, train_loss: 0.017414121007597125\n",
      "FOLD: 2, EPOCH: 33, valid_loss: 0.01568824453995778\n",
      "FOLD: 2, EPOCH: 34, train_loss: 0.01734452139636552\n",
      "FOLD: 2, EPOCH: 34, valid_loss: 0.01568369961415346\n",
      "FOLD: 2, EPOCH: 35, train_loss: 0.0172141888808157\n",
      "FOLD: 2, EPOCH: 35, valid_loss: 0.015678236547570962\n",
      "FOLD: 2, EPOCH: 36, train_loss: 0.017104693663281365\n",
      "FOLD: 2, EPOCH: 36, valid_loss: 0.01571120135486126\n",
      "FOLD: 2, EPOCH: 37, train_loss: 0.017019116709864623\n",
      "FOLD: 2, EPOCH: 37, valid_loss: 0.015710976261358995\n",
      "FOLD: 2, EPOCH: 38, train_loss: 0.016895298103525025\n",
      "FOLD: 2, EPOCH: 38, valid_loss: 0.015712443595895402\n",
      "FOLD: 2, EPOCH: 39, train_loss: 0.016800348785378644\n",
      "FOLD: 2, EPOCH: 39, valid_loss: 0.015664448746694967\n",
      "FOLD: 2, EPOCH: 40, train_loss: 0.01670158178721731\n",
      "FOLD: 2, EPOCH: 40, valid_loss: 0.015689280528861742\n",
      "FOLD: 2, EPOCH: 41, train_loss: 0.01663134725311318\n",
      "FOLD: 2, EPOCH: 41, valid_loss: 0.015650212621459596\n",
      "FOLD: 2, EPOCH: 42, train_loss: 0.016531988928044163\n",
      "FOLD: 2, EPOCH: 42, valid_loss: 0.015665217087819025\n",
      "FOLD: 2, EPOCH: 43, train_loss: 0.016439434984145133\n",
      "FOLD: 2, EPOCH: 43, valid_loss: 0.015645219299655694\n",
      "FOLD: 2, EPOCH: 44, train_loss: 0.016314145013992046\n",
      "FOLD: 2, EPOCH: 44, valid_loss: 0.015644373658757944\n",
      "FOLD: 2, EPOCH: 45, train_loss: 0.016260853200848843\n",
      "FOLD: 2, EPOCH: 45, valid_loss: 0.01563879747230273\n",
      "FOLD: 2, EPOCH: 46, train_loss: 0.01614375860505813\n",
      "FOLD: 2, EPOCH: 46, valid_loss: 0.01565095452735057\n",
      "FOLD: 2, EPOCH: 47, train_loss: 0.016089113668312092\n",
      "FOLD: 2, EPOCH: 47, valid_loss: 0.015686322290163774\n",
      "FOLD: 2, EPOCH: 48, train_loss: 0.016006916673300235\n",
      "FOLD: 2, EPOCH: 48, valid_loss: 0.01567745230232294\n",
      "FOLD: 2, EPOCH: 49, train_loss: 0.015850796188051637\n",
      "FOLD: 2, EPOCH: 49, valid_loss: 0.015699102973135617\n",
      "FOLD: 3, EPOCH: 0, train_loss: 0.044224473874311186\n",
      "FOLD: 3, EPOCH: 0, valid_loss: 0.020364901480766442\n",
      "FOLD: 3, EPOCH: 1, train_loss: 0.0227253377135541\n",
      "FOLD: 3, EPOCH: 1, valid_loss: 0.018970019112412747\n",
      "FOLD: 3, EPOCH: 2, train_loss: 0.021852473501820822\n",
      "FOLD: 3, EPOCH: 2, valid_loss: 0.018078075148738347\n",
      "FOLD: 3, EPOCH: 3, train_loss: 0.021321998430869064\n",
      "FOLD: 3, EPOCH: 3, valid_loss: 0.01764743452748427\n",
      "FOLD: 3, EPOCH: 4, train_loss: 0.020957644852633413\n",
      "FOLD: 3, EPOCH: 4, valid_loss: 0.017525079731757823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 3, EPOCH: 5, train_loss: 0.02076798624585609\n",
      "FOLD: 3, EPOCH: 5, valid_loss: 0.017327164514706686\n",
      "FOLD: 3, EPOCH: 6, train_loss: 0.020554148717909247\n",
      "FOLD: 3, EPOCH: 6, valid_loss: 0.017185897924579106\n",
      "FOLD: 3, EPOCH: 7, train_loss: 0.020300085505319608\n",
      "FOLD: 3, EPOCH: 7, valid_loss: 0.01707368530333042\n",
      "FOLD: 3, EPOCH: 8, train_loss: 0.020150391259104818\n",
      "FOLD: 3, EPOCH: 8, valid_loss: 0.01693414359425123\n",
      "FOLD: 3, EPOCH: 9, train_loss: 0.02001273715113466\n",
      "FOLD: 3, EPOCH: 9, valid_loss: 0.016859893042307634\n",
      "FOLD: 3, EPOCH: 10, train_loss: 0.01985811995896133\n",
      "FOLD: 3, EPOCH: 10, valid_loss: 0.016656495344180327\n",
      "FOLD: 3, EPOCH: 11, train_loss: 0.019730413005360076\n",
      "FOLD: 3, EPOCH: 11, valid_loss: 0.016617755907086227\n",
      "FOLD: 3, EPOCH: 12, train_loss: 0.019589165653530007\n",
      "FOLD: 3, EPOCH: 12, valid_loss: 0.016537740969887145\n",
      "FOLD: 3, EPOCH: 13, train_loss: 0.019450302311294788\n",
      "FOLD: 3, EPOCH: 13, valid_loss: 0.016535737909949742\n",
      "FOLD: 3, EPOCH: 14, train_loss: 0.019362442774345744\n",
      "FOLD: 3, EPOCH: 14, valid_loss: 0.01638482339107073\n",
      "FOLD: 3, EPOCH: 15, train_loss: 0.019226198109823303\n",
      "FOLD: 3, EPOCH: 15, valid_loss: 0.016380797928342454\n",
      "FOLD: 3, EPOCH: 16, train_loss: 0.019114973375926148\n",
      "FOLD: 3, EPOCH: 16, valid_loss: 0.016265812043387156\n",
      "FOLD: 3, EPOCH: 17, train_loss: 0.019001393729971873\n",
      "FOLD: 3, EPOCH: 17, valid_loss: 0.01626609767285677\n",
      "FOLD: 3, EPOCH: 18, train_loss: 0.01885710621403681\n",
      "FOLD: 3, EPOCH: 18, valid_loss: 0.01619160920381546\n",
      "FOLD: 3, EPOCH: 19, train_loss: 0.018794896811045503\n",
      "FOLD: 3, EPOCH: 19, valid_loss: 0.0161485902678508\n",
      "FOLD: 3, EPOCH: 20, train_loss: 0.018702409456710558\n",
      "FOLD: 3, EPOCH: 20, valid_loss: 0.01615660895521824\n",
      "FOLD: 3, EPOCH: 21, train_loss: 0.018593905392933537\n",
      "FOLD: 3, EPOCH: 21, valid_loss: 0.01607787709396619\n",
      "FOLD: 3, EPOCH: 22, train_loss: 0.018439484785336094\n",
      "FOLD: 3, EPOCH: 22, valid_loss: 0.016035306840561904\n",
      "FOLD: 3, EPOCH: 23, train_loss: 0.018321004368968913\n",
      "FOLD: 3, EPOCH: 23, valid_loss: 0.0160721056163311\n",
      "FOLD: 3, EPOCH: 24, train_loss: 0.01826363109756966\n",
      "FOLD: 3, EPOCH: 24, valid_loss: 0.016040563081892636\n",
      "FOLD: 3, EPOCH: 25, train_loss: 0.018120127805584186\n",
      "FOLD: 3, EPOCH: 25, valid_loss: 0.01631059691023368\n",
      "FOLD: 3, EPOCH: 26, train_loss: 0.01808133882445258\n",
      "FOLD: 3, EPOCH: 26, valid_loss: 0.015999395615206316\n",
      "FOLD: 3, EPOCH: 27, train_loss: 0.017993725660080846\n",
      "FOLD: 3, EPOCH: 27, valid_loss: 0.015997047894276105\n",
      "FOLD: 3, EPOCH: 28, train_loss: 0.01787815456958236\n",
      "FOLD: 3, EPOCH: 28, valid_loss: 0.015943291525428113\n",
      "FOLD: 3, EPOCH: 29, train_loss: 0.01773295385410657\n",
      "FOLD: 3, EPOCH: 29, valid_loss: 0.015942130954219744\n",
      "FOLD: 3, EPOCH: 30, train_loss: 0.017660244141478796\n",
      "FOLD: 3, EPOCH: 30, valid_loss: 0.01592487175590717\n",
      "FOLD: 3, EPOCH: 31, train_loss: 0.01758742326165776\n",
      "FOLD: 3, EPOCH: 31, valid_loss: 0.01591587646935995\n",
      "FOLD: 3, EPOCH: 32, train_loss: 0.017461836463897616\n",
      "FOLD: 3, EPOCH: 32, valid_loss: 0.015917119999917653\n",
      "FOLD: 3, EPOCH: 33, train_loss: 0.017398396997737722\n",
      "FOLD: 3, EPOCH: 33, valid_loss: 0.01587897639435071\n",
      "FOLD: 3, EPOCH: 34, train_loss: 0.01726051338168012\n",
      "FOLD: 3, EPOCH: 34, valid_loss: 0.01590276688623887\n",
      "FOLD: 3, EPOCH: 35, train_loss: 0.01714127787665741\n",
      "FOLD: 3, EPOCH: 35, valid_loss: 0.015871949064043853\n",
      "FOLD: 3, EPOCH: 36, train_loss: 0.017082325484905694\n",
      "FOLD: 3, EPOCH: 36, valid_loss: 0.015830019345650308\n",
      "FOLD: 3, EPOCH: 37, train_loss: 0.016966496581664763\n",
      "FOLD: 3, EPOCH: 37, valid_loss: 0.0158974459538093\n",
      "FOLD: 3, EPOCH: 38, train_loss: 0.01685044227318989\n",
      "FOLD: 3, EPOCH: 38, valid_loss: 0.015912204765929625\n",
      "FOLD: 3, EPOCH: 39, train_loss: 0.016755815721242816\n",
      "FOLD: 3, EPOCH: 39, valid_loss: 0.015876038358188592\n",
      "FOLD: 3, EPOCH: 40, train_loss: 0.016674273823564116\n",
      "FOLD: 3, EPOCH: 40, valid_loss: 0.015886356695913352\n",
      "FOLD: 3, EPOCH: 41, train_loss: 0.016592574393930467\n",
      "FOLD: 3, EPOCH: 41, valid_loss: 0.015849959606734607\n",
      "FOLD: 3, EPOCH: 42, train_loss: 0.01645290952276539\n",
      "FOLD: 3, EPOCH: 42, valid_loss: 0.01587944463468515\n",
      "FOLD: 3, EPOCH: 43, train_loss: 0.016408918828174874\n",
      "FOLD: 3, EPOCH: 43, valid_loss: 0.01586096239491151\n",
      "Epoch    44: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 3, EPOCH: 44, train_loss: 0.016216246260179056\n",
      "FOLD: 3, EPOCH: 44, valid_loss: 0.015836609814029474\n",
      "FOLD: 3, EPOCH: 45, train_loss: 0.016027654118433193\n",
      "FOLD: 3, EPOCH: 45, valid_loss: 0.01582025349713289\n",
      "FOLD: 3, EPOCH: 46, train_loss: 0.015950232052017708\n",
      "FOLD: 3, EPOCH: 46, valid_loss: 0.01584057309306585\n",
      "FOLD: 3, EPOCH: 47, train_loss: 0.01584584328874543\n",
      "FOLD: 3, EPOCH: 47, valid_loss: 0.015816492673296195\n",
      "FOLD: 3, EPOCH: 48, train_loss: 0.01578485218153612\n",
      "FOLD: 3, EPOCH: 48, valid_loss: 0.01585197914391756\n",
      "FOLD: 3, EPOCH: 49, train_loss: 0.0156952851527446\n",
      "FOLD: 3, EPOCH: 49, valid_loss: 0.015848557034937236\n",
      "FOLD: 4, EPOCH: 0, train_loss: 0.04441416248477794\n",
      "FOLD: 4, EPOCH: 0, valid_loss: 0.020483499965988673\n",
      "FOLD: 4, EPOCH: 1, train_loss: 0.02275328599923366\n",
      "FOLD: 4, EPOCH: 1, valid_loss: 0.019094869780998964\n",
      "FOLD: 4, EPOCH: 2, train_loss: 0.02187072324591714\n",
      "FOLD: 4, EPOCH: 2, valid_loss: 0.01850332563313154\n",
      "FOLD: 4, EPOCH: 3, train_loss: 0.021287149070082483\n",
      "FOLD: 4, EPOCH: 3, valid_loss: 0.017865157327972926\n",
      "FOLD: 4, EPOCH: 4, train_loss: 0.021001593990100396\n",
      "FOLD: 4, EPOCH: 4, valid_loss: 0.017646303973518886\n",
      "FOLD: 4, EPOCH: 5, train_loss: 0.020737395521152665\n",
      "FOLD: 4, EPOCH: 5, valid_loss: 0.01747634892280285\n",
      "FOLD: 4, EPOCH: 6, train_loss: 0.020570565241615515\n",
      "FOLD: 4, EPOCH: 6, valid_loss: 0.017274931359749574\n",
      "FOLD: 4, EPOCH: 7, train_loss: 0.02036612337404812\n",
      "FOLD: 4, EPOCH: 7, valid_loss: 0.017104197580080766\n",
      "FOLD: 4, EPOCH: 8, train_loss: 0.02020203005019072\n",
      "FOLD: 4, EPOCH: 8, valid_loss: 0.016983738742195643\n",
      "FOLD: 4, EPOCH: 9, train_loss: 0.020050287498412905\n",
      "FOLD: 4, EPOCH: 9, valid_loss: 0.016898868032372914\n",
      "FOLD: 4, EPOCH: 10, train_loss: 0.019914251173267495\n",
      "FOLD: 4, EPOCH: 10, valid_loss: 0.016779568906013783\n",
      "FOLD: 4, EPOCH: 11, train_loss: 0.01975872052984463\n",
      "FOLD: 4, EPOCH: 11, valid_loss: 0.01676436886191368\n",
      "FOLD: 4, EPOCH: 12, train_loss: 0.019617651312335116\n",
      "FOLD: 4, EPOCH: 12, valid_loss: 0.016639956631339513\n",
      "FOLD: 4, EPOCH: 13, train_loss: 0.019478149611402203\n",
      "FOLD: 4, EPOCH: 13, valid_loss: 0.01659211989205617\n",
      "FOLD: 4, EPOCH: 14, train_loss: 0.019363422097789275\n",
      "FOLD: 4, EPOCH: 14, valid_loss: 0.016539583412500527\n",
      "FOLD: 4, EPOCH: 15, train_loss: 0.019211507603727484\n",
      "FOLD: 4, EPOCH: 15, valid_loss: 0.016413655848457262\n",
      "FOLD: 4, EPOCH: 16, train_loss: 0.01911022013204323\n",
      "FOLD: 4, EPOCH: 16, valid_loss: 0.016424489709047172\n",
      "FOLD: 4, EPOCH: 17, train_loss: 0.01899121142923832\n",
      "FOLD: 4, EPOCH: 17, valid_loss: 0.01634873550098676\n",
      "FOLD: 4, EPOCH: 18, train_loss: 0.01888691359577147\n",
      "FOLD: 4, EPOCH: 18, valid_loss: 0.016281834230400048\n",
      "FOLD: 4, EPOCH: 19, train_loss: 0.018782488145941013\n",
      "FOLD: 4, EPOCH: 19, valid_loss: 0.01628252498518962\n",
      "FOLD: 4, EPOCH: 20, train_loss: 0.01869594884683957\n",
      "FOLD: 4, EPOCH: 20, valid_loss: 0.016243508730370265\n",
      "FOLD: 4, EPOCH: 21, train_loss: 0.018632300575641362\n",
      "FOLD: 4, EPOCH: 21, valid_loss: 0.016263112927285526\n",
      "FOLD: 4, EPOCH: 22, train_loss: 0.0185005700064672\n",
      "FOLD: 4, EPOCH: 22, valid_loss: 0.016169833305936594\n",
      "FOLD: 4, EPOCH: 23, train_loss: 0.018354775610606413\n",
      "FOLD: 4, EPOCH: 23, valid_loss: 0.01614569005771325\n",
      "FOLD: 4, EPOCH: 24, train_loss: 0.01827631256467587\n",
      "FOLD: 4, EPOCH: 24, valid_loss: 0.016173553867981985\n",
      "FOLD: 4, EPOCH: 25, train_loss: 0.018137070123811026\n",
      "FOLD: 4, EPOCH: 25, valid_loss: 0.016128414525435522\n",
      "FOLD: 4, EPOCH: 26, train_loss: 0.018095503108122864\n",
      "FOLD: 4, EPOCH: 26, valid_loss: 0.016090629120858815\n",
      "FOLD: 4, EPOCH: 27, train_loss: 0.01796163437334267\n",
      "FOLD: 4, EPOCH: 27, valid_loss: 0.016066570336428974\n",
      "FOLD: 4, EPOCH: 28, train_loss: 0.017893657944089657\n",
      "FOLD: 4, EPOCH: 28, valid_loss: 0.016062828712165356\n",
      "FOLD: 4, EPOCH: 29, train_loss: 0.017811312831032114\n",
      "FOLD: 4, EPOCH: 29, valid_loss: 0.016119188270889796\n",
      "FOLD: 4, EPOCH: 30, train_loss: 0.01765984352174643\n",
      "FOLD: 4, EPOCH: 30, valid_loss: 0.01604552151492009\n",
      "FOLD: 4, EPOCH: 31, train_loss: 0.01756468294440089\n",
      "FOLD: 4, EPOCH: 31, valid_loss: 0.0160524626620687\n",
      "FOLD: 4, EPOCH: 32, train_loss: 0.01744886476395501\n",
      "FOLD: 4, EPOCH: 32, valid_loss: 0.016086456939004935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 4, EPOCH: 33, train_loss: 0.017368888432109677\n",
      "FOLD: 4, EPOCH: 33, valid_loss: 0.01602112716780259\n",
      "FOLD: 4, EPOCH: 34, train_loss: 0.017251581809407956\n",
      "FOLD: 4, EPOCH: 34, valid_loss: 0.016027843293089133\n",
      "FOLD: 4, EPOCH: 35, train_loss: 0.017163687159080763\n",
      "FOLD: 4, EPOCH: 35, valid_loss: 0.016034708358347416\n",
      "FOLD: 4, EPOCH: 36, train_loss: 0.017094769817147706\n",
      "FOLD: 4, EPOCH: 36, valid_loss: 0.016050964164046142\n",
      "FOLD: 4, EPOCH: 37, train_loss: 0.016998917858644918\n",
      "FOLD: 4, EPOCH: 37, valid_loss: 0.01603271411015437\n",
      "FOLD: 4, EPOCH: 38, train_loss: 0.016868287558994582\n",
      "FOLD: 4, EPOCH: 38, valid_loss: 0.016036424785852432\n",
      "FOLD: 4, EPOCH: 39, train_loss: 0.0167729125635044\n",
      "FOLD: 4, EPOCH: 39, valid_loss: 0.016027817502617836\n",
      "FOLD: 4, EPOCH: 40, train_loss: 0.016651472013846442\n",
      "FOLD: 4, EPOCH: 40, valid_loss: 0.015985964869077388\n",
      "FOLD: 4, EPOCH: 41, train_loss: 0.016582591786376527\n",
      "FOLD: 4, EPOCH: 41, valid_loss: 0.015979441455923594\n",
      "FOLD: 4, EPOCH: 42, train_loss: 0.016443397535162198\n",
      "FOLD: 4, EPOCH: 42, valid_loss: 0.01600993496294205\n",
      "FOLD: 4, EPOCH: 43, train_loss: 0.016369252692203264\n",
      "FOLD: 4, EPOCH: 43, valid_loss: 0.016038879537238523\n",
      "FOLD: 4, EPOCH: 44, train_loss: 0.01630028967770773\n",
      "FOLD: 4, EPOCH: 44, valid_loss: 0.01601618406577752\n",
      "FOLD: 4, EPOCH: 45, train_loss: 0.01620540874884338\n",
      "FOLD: 4, EPOCH: 45, valid_loss: 0.016008669510483742\n",
      "FOLD: 4, EPOCH: 46, train_loss: 0.016089462285005563\n",
      "FOLD: 4, EPOCH: 46, valid_loss: 0.01598859650011246\n",
      "FOLD: 4, EPOCH: 47, train_loss: 0.016032658542531567\n",
      "FOLD: 4, EPOCH: 47, valid_loss: 0.01600244612647937\n",
      "FOLD: 4, EPOCH: 48, train_loss: 0.015901848094890248\n",
      "FOLD: 4, EPOCH: 48, valid_loss: 0.016034081864815492\n",
      "Epoch    49: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 4, EPOCH: 49, train_loss: 0.015682424292773813\n",
      "FOLD: 4, EPOCH: 49, valid_loss: 0.01598643547353836\n",
      "FOLD: 5, EPOCH: 0, train_loss: 0.043051331807431335\n",
      "FOLD: 5, EPOCH: 0, valid_loss: 0.02056313707278325\n",
      "FOLD: 5, EPOCH: 1, train_loss: 0.022711857926805277\n",
      "FOLD: 5, EPOCH: 1, valid_loss: 0.01908283121883869\n",
      "FOLD: 5, EPOCH: 2, train_loss: 0.021791553829570074\n",
      "FOLD: 5, EPOCH: 2, valid_loss: 0.018170479971628923\n",
      "FOLD: 5, EPOCH: 3, train_loss: 0.02132581629966562\n",
      "FOLD: 5, EPOCH: 3, valid_loss: 0.01772266268157042\n",
      "FOLD: 5, EPOCH: 4, train_loss: 0.02099667826818453\n",
      "FOLD: 5, EPOCH: 4, valid_loss: 0.017665270142830335\n",
      "FOLD: 5, EPOCH: 5, train_loss: 0.020730073387558397\n",
      "FOLD: 5, EPOCH: 5, valid_loss: 0.017329216719819948\n",
      "FOLD: 5, EPOCH: 6, train_loss: 0.02051209723828612\n",
      "FOLD: 5, EPOCH: 6, valid_loss: 0.017174294361701377\n",
      "FOLD: 5, EPOCH: 7, train_loss: 0.02032514226094291\n",
      "FOLD: 5, EPOCH: 7, valid_loss: 0.01710655726492405\n",
      "FOLD: 5, EPOCH: 8, train_loss: 0.020165365566877095\n",
      "FOLD: 5, EPOCH: 8, valid_loss: 0.016936899521029912\n",
      "FOLD: 5, EPOCH: 9, train_loss: 0.020055264234542847\n",
      "FOLD: 5, EPOCH: 9, valid_loss: 0.016838636822425403\n",
      "FOLD: 5, EPOCH: 10, train_loss: 0.019891945443846083\n",
      "FOLD: 5, EPOCH: 10, valid_loss: 0.016797099262475967\n",
      "FOLD: 5, EPOCH: 11, train_loss: 0.01968670198442163\n",
      "FOLD: 5, EPOCH: 11, valid_loss: 0.016635833451381095\n",
      "FOLD: 5, EPOCH: 12, train_loss: 0.01963695793135746\n",
      "FOLD: 5, EPOCH: 12, valid_loss: 0.01655945160354559\n",
      "FOLD: 5, EPOCH: 13, train_loss: 0.01946058141923434\n",
      "FOLD: 5, EPOCH: 13, valid_loss: 0.016492384772461195\n",
      "FOLD: 5, EPOCH: 14, train_loss: 0.01928920578211546\n",
      "FOLD: 5, EPOCH: 14, valid_loss: 0.016460867097171452\n",
      "FOLD: 5, EPOCH: 15, train_loss: 0.019229057219785614\n",
      "FOLD: 5, EPOCH: 15, valid_loss: 0.016312265052245215\n",
      "FOLD: 5, EPOCH: 16, train_loss: 0.019096233605130297\n",
      "FOLD: 5, EPOCH: 16, valid_loss: 0.01631656038359954\n",
      "FOLD: 5, EPOCH: 17, train_loss: 0.018977483684146725\n",
      "FOLD: 5, EPOCH: 17, valid_loss: 0.016234444454312325\n",
      "FOLD: 5, EPOCH: 18, train_loss: 0.018860199337674154\n",
      "FOLD: 5, EPOCH: 18, valid_loss: 0.016173575861522786\n",
      "FOLD: 5, EPOCH: 19, train_loss: 0.01878186531767652\n",
      "FOLD: 5, EPOCH: 19, valid_loss: 0.01617574842216877\n",
      "FOLD: 5, EPOCH: 20, train_loss: 0.018636003161805706\n",
      "FOLD: 5, EPOCH: 20, valid_loss: 0.016105167352809355\n",
      "FOLD: 5, EPOCH: 21, train_loss: 0.01852884443124404\n",
      "FOLD: 5, EPOCH: 21, valid_loss: 0.016082887036296036\n",
      "FOLD: 5, EPOCH: 22, train_loss: 0.018449535442365182\n",
      "FOLD: 5, EPOCH: 22, valid_loss: 0.016119576059281826\n",
      "FOLD: 5, EPOCH: 23, train_loss: 0.018286932561848615\n",
      "FOLD: 5, EPOCH: 23, valid_loss: 0.016085422239624537\n",
      "FOLD: 5, EPOCH: 24, train_loss: 0.018216504916750097\n",
      "FOLD: 5, EPOCH: 24, valid_loss: 0.016028990825781457\n",
      "FOLD: 5, EPOCH: 25, train_loss: 0.01816014467260322\n",
      "FOLD: 5, EPOCH: 25, valid_loss: 0.016026822133706167\n",
      "FOLD: 5, EPOCH: 26, train_loss: 0.018049187756873464\n",
      "FOLD: 5, EPOCH: 26, valid_loss: 0.015945668045717936\n",
      "FOLD: 5, EPOCH: 27, train_loss: 0.01794676198246511\n",
      "FOLD: 5, EPOCH: 27, valid_loss: 0.015934307772952776\n",
      "FOLD: 5, EPOCH: 28, train_loss: 0.01784066078127236\n",
      "FOLD: 5, EPOCH: 28, valid_loss: 0.015983239747583866\n",
      "FOLD: 5, EPOCH: 29, train_loss: 0.017729731150776952\n",
      "FOLD: 5, EPOCH: 29, valid_loss: 0.01592293596611573\n",
      "FOLD: 5, EPOCH: 30, train_loss: 0.017592327845459048\n",
      "FOLD: 5, EPOCH: 30, valid_loss: 0.015969357524926845\n",
      "FOLD: 5, EPOCH: 31, train_loss: 0.01749155542032944\n",
      "FOLD: 5, EPOCH: 31, valid_loss: 0.015944954151144393\n",
      "FOLD: 5, EPOCH: 32, train_loss: 0.01742975233827491\n",
      "FOLD: 5, EPOCH: 32, valid_loss: 0.015913234378855962\n",
      "FOLD: 5, EPOCH: 33, train_loss: 0.01731962718170237\n",
      "FOLD: 5, EPOCH: 33, valid_loss: 0.015881471192607514\n",
      "FOLD: 5, EPOCH: 34, train_loss: 0.01722281934643114\n",
      "FOLD: 5, EPOCH: 34, valid_loss: 0.0158947603060649\n",
      "FOLD: 5, EPOCH: 35, train_loss: 0.01714111086786599\n",
      "FOLD: 5, EPOCH: 35, valid_loss: 0.01587452760969217\n",
      "FOLD: 5, EPOCH: 36, train_loss: 0.017043007525137148\n",
      "FOLD: 5, EPOCH: 36, valid_loss: 0.01590790506452322\n",
      "FOLD: 5, EPOCH: 37, train_loss: 0.016949758462205127\n",
      "FOLD: 5, EPOCH: 37, valid_loss: 0.015898018717192687\n",
      "FOLD: 5, EPOCH: 38, train_loss: 0.01680007829909792\n",
      "FOLD: 5, EPOCH: 38, valid_loss: 0.015875862481502388\n",
      "FOLD: 5, EPOCH: 39, train_loss: 0.016705205324231774\n",
      "FOLD: 5, EPOCH: 39, valid_loss: 0.015892231120513037\n",
      "FOLD: 5, EPOCH: 40, train_loss: 0.0165853733322709\n",
      "FOLD: 5, EPOCH: 40, valid_loss: 0.015897672766676314\n",
      "FOLD: 5, EPOCH: 41, train_loss: 0.0165005038190331\n",
      "FOLD: 5, EPOCH: 41, valid_loss: 0.015886417446801297\n",
      "FOLD: 5, EPOCH: 42, train_loss: 0.016416825328928394\n",
      "FOLD: 5, EPOCH: 42, valid_loss: 0.015924961735995915\n",
      "Epoch    43: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 5, EPOCH: 43, train_loss: 0.016216049209941883\n",
      "FOLD: 5, EPOCH: 43, valid_loss: 0.015867215294677477\n",
      "FOLD: 5, EPOCH: 44, train_loss: 0.01612160137124561\n",
      "FOLD: 5, EPOCH: 44, valid_loss: 0.01587054398483955\n",
      "FOLD: 5, EPOCH: 45, train_loss: 0.0160255746387348\n",
      "FOLD: 5, EPOCH: 45, valid_loss: 0.015866293858450193\n",
      "FOLD: 5, EPOCH: 46, train_loss: 0.015936827815666393\n",
      "FOLD: 5, EPOCH: 46, valid_loss: 0.015886234477735482\n",
      "FOLD: 5, EPOCH: 47, train_loss: 0.015810781485728315\n",
      "FOLD: 5, EPOCH: 47, valid_loss: 0.015869414863678124\n",
      "FOLD: 5, EPOCH: 48, train_loss: 0.015694631653762347\n",
      "FOLD: 5, EPOCH: 48, valid_loss: 0.015863352383558568\n",
      "FOLD: 5, EPOCH: 49, train_loss: 0.015622259004393945\n",
      "FOLD: 5, EPOCH: 49, valid_loss: 0.01587274534484515\n",
      "FOLD: 6, EPOCH: 0, train_loss: 0.045403825320504805\n",
      "FOLD: 6, EPOCH: 0, valid_loss: 0.01991379504593519\n",
      "FOLD: 6, EPOCH: 1, train_loss: 0.022732284549321677\n",
      "FOLD: 6, EPOCH: 1, valid_loss: 0.019032554557690255\n",
      "FOLD: 6, EPOCH: 2, train_loss: 0.021762847296289495\n",
      "FOLD: 6, EPOCH: 2, valid_loss: 0.017833952935269244\n",
      "FOLD: 6, EPOCH: 3, train_loss: 0.02128932881798293\n",
      "FOLD: 6, EPOCH: 3, valid_loss: 0.01779057293270643\n",
      "FOLD: 6, EPOCH: 4, train_loss: 0.02096990976683997\n",
      "FOLD: 6, EPOCH: 4, valid_loss: 0.017344899403934296\n",
      "FOLD: 6, EPOCH: 5, train_loss: 0.020724583190639276\n",
      "FOLD: 6, EPOCH: 5, valid_loss: 0.017210195127588052\n",
      "FOLD: 6, EPOCH: 6, train_loss: 0.020527656869711104\n",
      "FOLD: 6, EPOCH: 6, valid_loss: 0.017012334428727627\n",
      "FOLD: 6, EPOCH: 7, train_loss: 0.02033237914076528\n",
      "FOLD: 6, EPOCH: 7, valid_loss: 0.01685545056198652\n",
      "FOLD: 6, EPOCH: 8, train_loss: 0.020190689310028747\n",
      "FOLD: 6, EPOCH: 8, valid_loss: 0.016780500944990378\n",
      "FOLD: 6, EPOCH: 9, train_loss: 0.01998553895769087\n",
      "FOLD: 6, EPOCH: 9, valid_loss: 0.01678054020381891\n",
      "FOLD: 6, EPOCH: 10, train_loss: 0.0198833989986294\n",
      "FOLD: 6, EPOCH: 10, valid_loss: 0.016590250584368523\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 6, EPOCH: 11, train_loss: 0.019749759721594887\n",
      "FOLD: 6, EPOCH: 11, valid_loss: 0.01651351863088516\n",
      "FOLD: 6, EPOCH: 12, train_loss: 0.01962226173664267\n",
      "FOLD: 6, EPOCH: 12, valid_loss: 0.016382532480817575\n",
      "FOLD: 6, EPOCH: 13, train_loss: 0.01945939161688895\n",
      "FOLD: 6, EPOCH: 13, valid_loss: 0.016320309458443753\n",
      "FOLD: 6, EPOCH: 14, train_loss: 0.019367297482047532\n",
      "FOLD: 6, EPOCH: 14, valid_loss: 0.016261812371130172\n",
      "FOLD: 6, EPOCH: 15, train_loss: 0.019219996130748374\n",
      "FOLD: 6, EPOCH: 15, valid_loss: 0.016189617319748953\n",
      "FOLD: 6, EPOCH: 16, train_loss: 0.019106568642766088\n",
      "FOLD: 6, EPOCH: 16, valid_loss: 0.016138980594965126\n",
      "FOLD: 6, EPOCH: 17, train_loss: 0.01897832111933747\n",
      "FOLD: 6, EPOCH: 17, valid_loss: 0.016130568029788826\n",
      "FOLD: 6, EPOCH: 18, train_loss: 0.018887281669555483\n",
      "FOLD: 6, EPOCH: 18, valid_loss: 0.01614520548341366\n",
      "FOLD: 6, EPOCH: 19, train_loss: 0.01880193023464164\n",
      "FOLD: 6, EPOCH: 19, valid_loss: 0.01598673392660343\n",
      "FOLD: 6, EPOCH: 20, train_loss: 0.018673177607156134\n",
      "FOLD: 6, EPOCH: 20, valid_loss: 0.016000874913655795\n",
      "FOLD: 6, EPOCH: 21, train_loss: 0.01862392297669037\n",
      "FOLD: 6, EPOCH: 21, valid_loss: 0.015987136043035068\n",
      "FOLD: 6, EPOCH: 22, train_loss: 0.018500091231151206\n",
      "FOLD: 6, EPOCH: 22, valid_loss: 0.015955196048777837\n",
      "FOLD: 6, EPOCH: 23, train_loss: 0.01836581404849484\n",
      "FOLD: 6, EPOCH: 23, valid_loss: 0.015949816443026066\n",
      "FOLD: 6, EPOCH: 24, train_loss: 0.018283891692958975\n",
      "FOLD: 6, EPOCH: 24, valid_loss: 0.015931073289651137\n",
      "FOLD: 6, EPOCH: 25, train_loss: 0.01815384764828392\n",
      "FOLD: 6, EPOCH: 25, valid_loss: 0.01589774268750961\n",
      "FOLD: 6, EPOCH: 26, train_loss: 0.018086377782998857\n",
      "FOLD: 6, EPOCH: 26, valid_loss: 0.015885591721878603\n",
      "FOLD: 6, EPOCH: 27, train_loss: 0.01798264871074541\n",
      "FOLD: 6, EPOCH: 27, valid_loss: 0.015880585146638062\n",
      "FOLD: 6, EPOCH: 28, train_loss: 0.01785337009995773\n",
      "FOLD: 6, EPOCH: 28, valid_loss: 0.015823983730605014\n",
      "FOLD: 6, EPOCH: 29, train_loss: 0.01777604959804464\n",
      "FOLD: 6, EPOCH: 29, valid_loss: 0.015805019710499506\n",
      "FOLD: 6, EPOCH: 30, train_loss: 0.01770029269863625\n",
      "FOLD: 6, EPOCH: 30, valid_loss: 0.01580394388964543\n",
      "FOLD: 6, EPOCH: 31, train_loss: 0.01761039809600727\n",
      "FOLD: 6, EPOCH: 31, valid_loss: 0.015758506022393703\n",
      "FOLD: 6, EPOCH: 32, train_loss: 0.01745863469015505\n",
      "FOLD: 6, EPOCH: 32, valid_loss: 0.01576995190519553\n",
      "FOLD: 6, EPOCH: 33, train_loss: 0.017394064170484606\n",
      "FOLD: 6, EPOCH: 33, valid_loss: 0.015736472864563648\n",
      "FOLD: 6, EPOCH: 34, train_loss: 0.017269030439893942\n",
      "FOLD: 6, EPOCH: 34, valid_loss: 0.015735645993397787\n",
      "FOLD: 6, EPOCH: 35, train_loss: 0.017130893680292206\n",
      "FOLD: 6, EPOCH: 35, valid_loss: 0.015753197841919385\n",
      "FOLD: 6, EPOCH: 36, train_loss: 0.017043786500958173\n",
      "FOLD: 6, EPOCH: 36, valid_loss: 0.015761771955742285\n",
      "FOLD: 6, EPOCH: 37, train_loss: 0.01699313783162349\n",
      "FOLD: 6, EPOCH: 37, valid_loss: 0.015761196828232363\n",
      "FOLD: 6, EPOCH: 38, train_loss: 0.016846584634402313\n",
      "FOLD: 6, EPOCH: 38, valid_loss: 0.01576410076366021\n",
      "FOLD: 6, EPOCH: 39, train_loss: 0.0167968631092761\n",
      "FOLD: 6, EPOCH: 39, valid_loss: 0.015741497708054688\n",
      "FOLD: 6, EPOCH: 40, train_loss: 0.01668763713206391\n",
      "FOLD: 6, EPOCH: 40, valid_loss: 0.015759354528899375\n",
      "FOLD: 6, EPOCH: 41, train_loss: 0.01658803831181816\n",
      "FOLD: 6, EPOCH: 41, valid_loss: 0.01573646326477711\n",
      "Epoch    42: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 6, EPOCH: 42, train_loss: 0.01639591292100581\n",
      "FOLD: 6, EPOCH: 42, valid_loss: 0.01575156723937163\n",
      "FOLD: 6, EPOCH: 43, train_loss: 0.01628880760305234\n",
      "FOLD: 6, EPOCH: 43, valid_loss: 0.015762021621832482\n",
      "FOLD: 6, EPOCH: 44, train_loss: 0.016180422106707417\n",
      "FOLD: 6, EPOCH: 44, valid_loss: 0.015753592006289042\n",
      "FOLD: 6, EPOCH: 45, train_loss: 0.016054873619027233\n",
      "FOLD: 6, EPOCH: 45, valid_loss: 0.0157574531980432\n",
      "FOLD: 6, EPOCH: 46, train_loss: 0.01596015490390159\n",
      "FOLD: 6, EPOCH: 46, valid_loss: 0.015743884043051645\n",
      "FOLD: 6, EPOCH: 47, train_loss: 0.015907958212837175\n",
      "FOLD: 6, EPOCH: 47, valid_loss: 0.015747159862747558\n",
      "FOLD: 6, EPOCH: 48, train_loss: 0.015792242830266822\n",
      "FOLD: 6, EPOCH: 48, valid_loss: 0.01578629955362815\n",
      "Epoch    49: reducing learning rate of group 0 to 1.1288e-01.\n",
      "FOLD: 6, EPOCH: 49, train_loss: 0.015664925257599837\n",
      "FOLD: 6, EPOCH: 49, valid_loss: 0.015782340787924252\n",
      "FOLD: 0, EPOCH: 0, train_loss: 0.04385981781760583\n",
      "FOLD: 0, EPOCH: 0, valid_loss: 0.020380225892250355\n",
      "FOLD: 0, EPOCH: 1, train_loss: 0.022739557700382697\n",
      "FOLD: 0, EPOCH: 1, valid_loss: 0.019214098556683615\n",
      "FOLD: 0, EPOCH: 2, train_loss: 0.021804172671525866\n",
      "FOLD: 0, EPOCH: 2, valid_loss: 0.01801322916379342\n",
      "FOLD: 0, EPOCH: 3, train_loss: 0.02130550579042048\n",
      "FOLD: 0, EPOCH: 3, valid_loss: 0.01780743796664935\n",
      "FOLD: 0, EPOCH: 4, train_loss: 0.02101317038004463\n",
      "FOLD: 0, EPOCH: 4, valid_loss: 0.0176458704786805\n",
      "FOLD: 0, EPOCH: 5, train_loss: 0.020721488556749112\n",
      "FOLD: 0, EPOCH: 5, valid_loss: 0.017300757937706433\n",
      "FOLD: 0, EPOCH: 6, train_loss: 0.020506129474253267\n",
      "FOLD: 0, EPOCH: 6, valid_loss: 0.017194941783180602\n",
      "FOLD: 0, EPOCH: 7, train_loss: 0.020312577813259652\n",
      "FOLD: 0, EPOCH: 7, valid_loss: 0.016978430991562512\n",
      "FOLD: 0, EPOCH: 8, train_loss: 0.020139457834129397\n",
      "FOLD: 0, EPOCH: 8, valid_loss: 0.016869343530673247\n",
      "FOLD: 0, EPOCH: 9, train_loss: 0.01999457469965155\n",
      "FOLD: 0, EPOCH: 9, valid_loss: 0.016772945053302325\n",
      "FOLD: 0, EPOCH: 10, train_loss: 0.019876425431386846\n",
      "FOLD: 0, EPOCH: 10, valid_loss: 0.01668001911961115\n",
      "FOLD: 0, EPOCH: 11, train_loss: 0.019757480184371408\n",
      "FOLD: 0, EPOCH: 11, valid_loss: 0.01658279147858803\n",
      "FOLD: 0, EPOCH: 12, train_loss: 0.019583761616534478\n",
      "FOLD: 0, EPOCH: 12, valid_loss: 0.016528302660355203\n",
      "FOLD: 0, EPOCH: 13, train_loss: 0.019456508695273787\n",
      "FOLD: 0, EPOCH: 13, valid_loss: 0.01650935261008831\n",
      "FOLD: 0, EPOCH: 14, train_loss: 0.0193811218078072\n",
      "FOLD: 0, EPOCH: 14, valid_loss: 0.01641029040687359\n",
      "FOLD: 0, EPOCH: 15, train_loss: 0.019254315946553205\n",
      "FOLD: 0, EPOCH: 15, valid_loss: 0.01628803526266263\n",
      "FOLD: 0, EPOCH: 16, train_loss: 0.019100653863436467\n",
      "FOLD: 0, EPOCH: 16, valid_loss: 0.016207480086730078\n",
      "FOLD: 0, EPOCH: 17, train_loss: 0.018986637905441427\n",
      "FOLD: 0, EPOCH: 17, valid_loss: 0.01618637810819424\n",
      "FOLD: 0, EPOCH: 18, train_loss: 0.01885552367044462\n",
      "FOLD: 0, EPOCH: 18, valid_loss: 0.016138451030621163\n",
      "FOLD: 0, EPOCH: 19, train_loss: 0.018783435653391724\n",
      "FOLD: 0, EPOCH: 19, valid_loss: 0.016072459447269257\n",
      "FOLD: 0, EPOCH: 20, train_loss: 0.018681527669163974\n",
      "FOLD: 0, EPOCH: 20, valid_loss: 0.016060103877232626\n",
      "FOLD: 0, EPOCH: 21, train_loss: 0.018550657465852594\n",
      "FOLD: 0, EPOCH: 21, valid_loss: 0.016083898810813062\n",
      "FOLD: 0, EPOCH: 22, train_loss: 0.018492466895966918\n",
      "FOLD: 0, EPOCH: 22, valid_loss: 0.016001864193150632\n",
      "FOLD: 0, EPOCH: 23, train_loss: 0.018380038943645115\n",
      "FOLD: 0, EPOCH: 23, valid_loss: 0.015935157568982013\n",
      "FOLD: 0, EPOCH: 24, train_loss: 0.018289262353367097\n",
      "FOLD: 0, EPOCH: 24, valid_loss: 0.015889679511579182\n",
      "FOLD: 0, EPOCH: 25, train_loss: 0.01813445632924905\n",
      "FOLD: 0, EPOCH: 25, valid_loss: 0.015897587586480837\n",
      "FOLD: 0, EPOCH: 26, train_loss: 0.018079342975004298\n",
      "FOLD: 0, EPOCH: 26, valid_loss: 0.015852491801174786\n",
      "FOLD: 0, EPOCH: 27, train_loss: 0.017938014975673443\n",
      "FOLD: 0, EPOCH: 27, valid_loss: 0.015841097499315556\n",
      "FOLD: 0, EPOCH: 28, train_loss: 0.017836908633644517\n",
      "FOLD: 0, EPOCH: 28, valid_loss: 0.015860096264917117\n",
      "FOLD: 0, EPOCH: 29, train_loss: 0.017806382849812508\n",
      "FOLD: 0, EPOCH: 29, valid_loss: 0.015853180908239804\n",
      "FOLD: 0, EPOCH: 30, train_loss: 0.017675290211431077\n",
      "FOLD: 0, EPOCH: 30, valid_loss: 0.01586371216063316\n",
      "FOLD: 0, EPOCH: 31, train_loss: 0.017557555532737357\n",
      "FOLD: 0, EPOCH: 31, valid_loss: 0.015848332442916356\n",
      "FOLD: 0, EPOCH: 32, train_loss: 0.017465682096175245\n",
      "FOLD: 0, EPOCH: 32, valid_loss: 0.015843245415733412\n",
      "FOLD: 0, EPOCH: 33, train_loss: 0.017344983042897406\n",
      "FOLD: 0, EPOCH: 33, valid_loss: 0.015840401371511128\n",
      "FOLD: 0, EPOCH: 34, train_loss: 0.017265191616339458\n",
      "FOLD: 0, EPOCH: 34, valid_loss: 0.01582156838132785\n",
      "FOLD: 0, EPOCH: 35, train_loss: 0.017181189630079915\n",
      "FOLD: 0, EPOCH: 35, valid_loss: 0.015777858905494213\n",
      "FOLD: 0, EPOCH: 36, train_loss: 0.017037553385504195\n",
      "FOLD: 0, EPOCH: 36, valid_loss: 0.01575629527752216\n",
      "FOLD: 0, EPOCH: 37, train_loss: 0.016952562611550093\n",
      "FOLD: 0, EPOCH: 37, valid_loss: 0.01579366996884346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 0, EPOCH: 38, train_loss: 0.016850506924596186\n",
      "FOLD: 0, EPOCH: 38, valid_loss: 0.01578588396883928\n",
      "FOLD: 0, EPOCH: 39, train_loss: 0.016744282625212863\n",
      "FOLD: 0, EPOCH: 39, valid_loss: 0.015781953644294005\n",
      "FOLD: 0, EPOCH: 40, train_loss: 0.016666306497378124\n",
      "FOLD: 0, EPOCH: 40, valid_loss: 0.01581089162769226\n",
      "FOLD: 0, EPOCH: 41, train_loss: 0.016600427154854342\n",
      "FOLD: 0, EPOCH: 41, valid_loss: 0.015790745687599365\n",
      "FOLD: 0, EPOCH: 42, train_loss: 0.016467317775193904\n",
      "FOLD: 0, EPOCH: 42, valid_loss: 0.01581373531371355\n",
      "FOLD: 0, EPOCH: 43, train_loss: 0.016366051170169503\n",
      "FOLD: 0, EPOCH: 43, valid_loss: 0.01579198570778737\n",
      "Epoch    44: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 0, EPOCH: 44, train_loss: 0.01618787137841856\n",
      "FOLD: 0, EPOCH: 44, valid_loss: 0.015796546537715655\n",
      "FOLD: 0, EPOCH: 45, train_loss: 0.01601146921716832\n",
      "FOLD: 0, EPOCH: 45, valid_loss: 0.01581272676300544\n",
      "FOLD: 0, EPOCH: 46, train_loss: 0.015920038687417638\n",
      "FOLD: 0, EPOCH: 46, valid_loss: 0.015807021910754535\n",
      "FOLD: 0, EPOCH: 47, train_loss: 0.015886728727334255\n",
      "FOLD: 0, EPOCH: 47, valid_loss: 0.01581521088687273\n",
      "FOLD: 0, EPOCH: 48, train_loss: 0.01572860052456727\n",
      "FOLD: 0, EPOCH: 48, valid_loss: 0.015835121273994446\n",
      "FOLD: 0, EPOCH: 49, train_loss: 0.015649141731193743\n",
      "FOLD: 0, EPOCH: 49, valid_loss: 0.015835650766698215\n",
      "FOLD: 1, EPOCH: 0, train_loss: 0.0426767926310768\n",
      "FOLD: 1, EPOCH: 0, valid_loss: 0.0200685802847147\n",
      "FOLD: 1, EPOCH: 1, train_loss: 0.022740251183308458\n",
      "FOLD: 1, EPOCH: 1, valid_loss: 0.01886979232613857\n",
      "FOLD: 1, EPOCH: 2, train_loss: 0.021919375864436496\n",
      "FOLD: 1, EPOCH: 2, valid_loss: 0.017986199317070153\n",
      "FOLD: 1, EPOCH: 3, train_loss: 0.021374783075943187\n",
      "FOLD: 1, EPOCH: 3, valid_loss: 0.017582897240152724\n",
      "FOLD: 1, EPOCH: 4, train_loss: 0.020969491477149563\n",
      "FOLD: 1, EPOCH: 4, valid_loss: 0.01724295447079035\n",
      "FOLD: 1, EPOCH: 5, train_loss: 0.02072411319996054\n",
      "FOLD: 1, EPOCH: 5, valid_loss: 0.01706823133505308\n",
      "FOLD: 1, EPOCH: 6, train_loss: 0.020465504805985336\n",
      "FOLD: 1, EPOCH: 6, valid_loss: 0.017128540919377253\n",
      "FOLD: 1, EPOCH: 7, train_loss: 0.02033805741450271\n",
      "FOLD: 1, EPOCH: 7, valid_loss: 0.01682022328560169\n",
      "FOLD: 1, EPOCH: 8, train_loss: 0.020142613406721\n",
      "FOLD: 1, EPOCH: 8, valid_loss: 0.016670405578154784\n",
      "FOLD: 1, EPOCH: 9, train_loss: 0.019989461889742193\n",
      "FOLD: 1, EPOCH: 9, valid_loss: 0.016541125396123298\n",
      "FOLD: 1, EPOCH: 10, train_loss: 0.019859722916137527\n",
      "FOLD: 1, EPOCH: 10, valid_loss: 0.01639772027444381\n",
      "FOLD: 1, EPOCH: 11, train_loss: 0.019660476620334225\n",
      "FOLD: 1, EPOCH: 11, valid_loss: 0.01631283530822167\n",
      "FOLD: 1, EPOCH: 12, train_loss: 0.019549737457890768\n",
      "FOLD: 1, EPOCH: 12, valid_loss: 0.01623907319914836\n",
      "FOLD: 1, EPOCH: 13, train_loss: 0.01940512443213044\n",
      "FOLD: 1, EPOCH: 13, valid_loss: 0.016127426320543654\n",
      "FOLD: 1, EPOCH: 14, train_loss: 0.01929940454460479\n",
      "FOLD: 1, EPOCH: 14, valid_loss: 0.016066335141658783\n",
      "FOLD: 1, EPOCH: 15, train_loss: 0.019143246491817204\n",
      "FOLD: 1, EPOCH: 15, valid_loss: 0.016061254490453463\n",
      "FOLD: 1, EPOCH: 16, train_loss: 0.019029501775229298\n",
      "FOLD: 1, EPOCH: 16, valid_loss: 0.015941668230180558\n",
      "FOLD: 1, EPOCH: 17, train_loss: 0.018920426080758508\n",
      "FOLD: 1, EPOCH: 17, valid_loss: 0.015897503337607935\n",
      "FOLD: 1, EPOCH: 18, train_loss: 0.01879109542917561\n",
      "FOLD: 1, EPOCH: 18, valid_loss: 0.015896980937283773\n",
      "FOLD: 1, EPOCH: 19, train_loss: 0.01867916580994387\n",
      "FOLD: 1, EPOCH: 19, valid_loss: 0.015840290687405147\n",
      "FOLD: 1, EPOCH: 20, train_loss: 0.018596663089418732\n",
      "FOLD: 1, EPOCH: 20, valid_loss: 0.015801219627834283\n",
      "FOLD: 1, EPOCH: 21, train_loss: 0.01846790605702916\n",
      "FOLD: 1, EPOCH: 21, valid_loss: 0.01576705542034828\n",
      "FOLD: 1, EPOCH: 22, train_loss: 0.018383138208977273\n",
      "FOLD: 1, EPOCH: 22, valid_loss: 0.01575104512560826\n",
      "FOLD: 1, EPOCH: 23, train_loss: 0.018279008365966177\n",
      "FOLD: 1, EPOCH: 23, valid_loss: 0.015758860713014237\n",
      "FOLD: 1, EPOCH: 24, train_loss: 0.01813370593496271\n",
      "FOLD: 1, EPOCH: 24, valid_loss: 0.015666929431832753\n",
      "FOLD: 1, EPOCH: 25, train_loss: 0.018052970889855077\n",
      "FOLD: 1, EPOCH: 25, valid_loss: 0.015660332515835762\n",
      "FOLD: 1, EPOCH: 26, train_loss: 0.017964696436113602\n",
      "FOLD: 1, EPOCH: 26, valid_loss: 0.015639530709729746\n",
      "FOLD: 1, EPOCH: 27, train_loss: 0.017874699916589906\n",
      "FOLD: 1, EPOCH: 27, valid_loss: 0.015670522760886412\n",
      "FOLD: 1, EPOCH: 28, train_loss: 0.01777128456160426\n",
      "FOLD: 1, EPOCH: 28, valid_loss: 0.015641277011197347\n",
      "FOLD: 1, EPOCH: 29, train_loss: 0.01767455675714725\n",
      "FOLD: 1, EPOCH: 29, valid_loss: 0.015596108726010872\n",
      "FOLD: 1, EPOCH: 30, train_loss: 0.017536399898597517\n",
      "FOLD: 1, EPOCH: 30, valid_loss: 0.015614132396876812\n",
      "FOLD: 1, EPOCH: 31, train_loss: 0.017424974011609685\n",
      "FOLD: 1, EPOCH: 31, valid_loss: 0.015592354779633192\n",
      "FOLD: 1, EPOCH: 32, train_loss: 0.017363615774524374\n",
      "FOLD: 1, EPOCH: 32, valid_loss: 0.015592152109512916\n",
      "FOLD: 1, EPOCH: 33, train_loss: 0.01726207518446687\n",
      "FOLD: 1, EPOCH: 33, valid_loss: 0.01558160581267797\n",
      "FOLD: 1, EPOCH: 34, train_loss: 0.01711415674386395\n",
      "FOLD: 1, EPOCH: 34, valid_loss: 0.015557988045307307\n",
      "FOLD: 1, EPOCH: 35, train_loss: 0.017042955685708974\n",
      "FOLD: 1, EPOCH: 35, valid_loss: 0.015531979143046416\n",
      "FOLD: 1, EPOCH: 36, train_loss: 0.016915698397300533\n",
      "FOLD: 1, EPOCH: 36, valid_loss: 0.015568473018132724\n",
      "FOLD: 1, EPOCH: 37, train_loss: 0.01682216122847151\n",
      "FOLD: 1, EPOCH: 37, valid_loss: 0.015555405559448095\n",
      "FOLD: 1, EPOCH: 38, train_loss: 0.016787968450100034\n",
      "FOLD: 1, EPOCH: 38, valid_loss: 0.015572676936594339\n",
      "FOLD: 1, EPOCH: 39, train_loss: 0.016643030380175727\n",
      "FOLD: 1, EPOCH: 39, valid_loss: 0.015520405883972462\n",
      "FOLD: 1, EPOCH: 40, train_loss: 0.016512565490965907\n",
      "FOLD: 1, EPOCH: 40, valid_loss: 0.015565893111320643\n",
      "FOLD: 1, EPOCH: 41, train_loss: 0.01646648660755238\n",
      "FOLD: 1, EPOCH: 41, valid_loss: 0.015550340239245158\n",
      "FOLD: 1, EPOCH: 42, train_loss: 0.016311419435550232\n",
      "FOLD: 1, EPOCH: 42, valid_loss: 0.01557210123596283\n",
      "FOLD: 1, EPOCH: 43, train_loss: 0.01622605147595341\n",
      "FOLD: 1, EPOCH: 43, valid_loss: 0.015541121721840821\n",
      "FOLD: 1, EPOCH: 44, train_loss: 0.016139907523284893\n",
      "FOLD: 1, EPOCH: 44, valid_loss: 0.015559941172026671\n",
      "FOLD: 1, EPOCH: 45, train_loss: 0.01604465619233009\n",
      "FOLD: 1, EPOCH: 45, valid_loss: 0.015579205937683582\n",
      "FOLD: 1, EPOCH: 46, train_loss: 0.01591973423303382\n",
      "FOLD: 1, EPOCH: 46, valid_loss: 0.015594604783333264\n",
      "Epoch    47: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 1, EPOCH: 47, train_loss: 0.01571130971550136\n",
      "FOLD: 1, EPOCH: 47, valid_loss: 0.015583277393419009\n",
      "FOLD: 1, EPOCH: 48, train_loss: 0.015653376064791873\n",
      "FOLD: 1, EPOCH: 48, valid_loss: 0.015574249367301281\n",
      "FOLD: 1, EPOCH: 49, train_loss: 0.015526964971040553\n",
      "FOLD: 1, EPOCH: 49, valid_loss: 0.01559135217506152\n",
      "FOLD: 2, EPOCH: 0, train_loss: 0.04231602854623988\n",
      "FOLD: 2, EPOCH: 0, valid_loss: 0.020235491486696098\n",
      "FOLD: 2, EPOCH: 1, train_loss: 0.02258932137408772\n",
      "FOLD: 2, EPOCH: 1, valid_loss: 0.0187323960260703\n",
      "FOLD: 2, EPOCH: 2, train_loss: 0.021776728055163008\n",
      "FOLD: 2, EPOCH: 2, valid_loss: 0.017953644363352887\n",
      "FOLD: 2, EPOCH: 3, train_loss: 0.021206148673553724\n",
      "FOLD: 2, EPOCH: 3, valid_loss: 0.017566676538151044\n",
      "FOLD: 2, EPOCH: 4, train_loss: 0.020895775659261522\n",
      "FOLD: 2, EPOCH: 4, valid_loss: 0.017248204336143456\n",
      "FOLD: 2, EPOCH: 5, train_loss: 0.020576630769347824\n",
      "FOLD: 2, EPOCH: 5, valid_loss: 0.016971105351470984\n",
      "FOLD: 2, EPOCH: 6, train_loss: 0.020403131011974166\n",
      "FOLD: 2, EPOCH: 6, valid_loss: 0.01694316968608361\n",
      "FOLD: 2, EPOCH: 7, train_loss: 0.02026681773163177\n",
      "FOLD: 2, EPOCH: 7, valid_loss: 0.016703781241980884\n",
      "FOLD: 2, EPOCH: 8, train_loss: 0.020053337151939805\n",
      "FOLD: 2, EPOCH: 8, valid_loss: 0.016785149534161273\n",
      "FOLD: 2, EPOCH: 9, train_loss: 0.019914293888251524\n",
      "FOLD: 2, EPOCH: 9, valid_loss: 0.016629466142218847\n",
      "FOLD: 2, EPOCH: 10, train_loss: 0.019734053494962486\n",
      "FOLD: 2, EPOCH: 10, valid_loss: 0.016433129660212077\n",
      "FOLD: 2, EPOCH: 11, train_loss: 0.019599730224424117\n",
      "FOLD: 2, EPOCH: 11, valid_loss: 0.016383280117924396\n",
      "FOLD: 2, EPOCH: 12, train_loss: 0.019471354480530764\n",
      "FOLD: 2, EPOCH: 12, valid_loss: 0.016350268816145565\n",
      "FOLD: 2, EPOCH: 13, train_loss: 0.01935760724685482\n",
      "FOLD: 2, EPOCH: 13, valid_loss: 0.01622126201310983\n",
      "FOLD: 2, EPOCH: 14, train_loss: 0.01919245654465379\n",
      "FOLD: 2, EPOCH: 14, valid_loss: 0.01610459516254755\n",
      "FOLD: 2, EPOCH: 15, train_loss: 0.01910717651952763\n",
      "FOLD: 2, EPOCH: 15, valid_loss: 0.016089903835493784\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 2, EPOCH: 16, train_loss: 0.01898739207535982\n",
      "FOLD: 2, EPOCH: 16, valid_loss: 0.016091331266439877\n",
      "FOLD: 2, EPOCH: 17, train_loss: 0.01886495097062072\n",
      "FOLD: 2, EPOCH: 17, valid_loss: 0.016049676932967626\n",
      "FOLD: 2, EPOCH: 18, train_loss: 0.018767427048973134\n",
      "FOLD: 2, EPOCH: 18, valid_loss: 0.015945826657116413\n",
      "FOLD: 2, EPOCH: 19, train_loss: 0.01864168344921357\n",
      "FOLD: 2, EPOCH: 19, valid_loss: 0.015943430077571135\n",
      "FOLD: 2, EPOCH: 20, train_loss: 0.018582114194696013\n",
      "FOLD: 2, EPOCH: 20, valid_loss: 0.015912228765395973\n",
      "FOLD: 2, EPOCH: 21, train_loss: 0.01837402129092732\n",
      "FOLD: 2, EPOCH: 21, valid_loss: 0.01588512147561862\n",
      "FOLD: 2, EPOCH: 22, train_loss: 0.018314813439910475\n",
      "FOLD: 2, EPOCH: 22, valid_loss: 0.01583918255682175\n",
      "FOLD: 2, EPOCH: 23, train_loss: 0.01818231868280752\n",
      "FOLD: 2, EPOCH: 23, valid_loss: 0.015824969643010542\n",
      "FOLD: 2, EPOCH: 24, train_loss: 0.018136297377782898\n",
      "FOLD: 2, EPOCH: 24, valid_loss: 0.015826561559851352\n",
      "FOLD: 2, EPOCH: 25, train_loss: 0.01798084861523396\n",
      "FOLD: 2, EPOCH: 25, valid_loss: 0.01574753110225384\n",
      "FOLD: 2, EPOCH: 26, train_loss: 0.0179168134424332\n",
      "FOLD: 2, EPOCH: 26, valid_loss: 0.015749739984480236\n",
      "FOLD: 2, EPOCH: 27, train_loss: 0.017789890360389207\n",
      "FOLD: 2, EPOCH: 27, valid_loss: 0.015751952018875342\n",
      "FOLD: 2, EPOCH: 28, train_loss: 0.017656535350692434\n",
      "FOLD: 2, EPOCH: 28, valid_loss: 0.01572085451334715\n",
      "FOLD: 2, EPOCH: 29, train_loss: 0.01758819748018239\n",
      "FOLD: 2, EPOCH: 29, valid_loss: 0.01572459513464799\n",
      "FOLD: 2, EPOCH: 30, train_loss: 0.017528727487031673\n",
      "FOLD: 2, EPOCH: 30, valid_loss: 0.015732121295653857\n",
      "FOLD: 2, EPOCH: 31, train_loss: 0.017359706648700946\n",
      "FOLD: 2, EPOCH: 31, valid_loss: 0.01567496946797921\n",
      "FOLD: 2, EPOCH: 32, train_loss: 0.017256766054275875\n",
      "FOLD: 2, EPOCH: 32, valid_loss: 0.01570552007223551\n",
      "FOLD: 2, EPOCH: 33, train_loss: 0.017172341662886982\n",
      "FOLD: 2, EPOCH: 33, valid_loss: 0.015621005987318663\n",
      "FOLD: 2, EPOCH: 34, train_loss: 0.017050704390213296\n",
      "FOLD: 2, EPOCH: 34, valid_loss: 0.015635479026688978\n",
      "FOLD: 2, EPOCH: 35, train_loss: 0.016973355646572402\n",
      "FOLD: 2, EPOCH: 35, valid_loss: 0.015654554447302453\n",
      "FOLD: 2, EPOCH: 36, train_loss: 0.016884530856701975\n",
      "FOLD: 2, EPOCH: 36, valid_loss: 0.015664051071955606\n",
      "FOLD: 2, EPOCH: 37, train_loss: 0.016784375256581885\n",
      "FOLD: 2, EPOCH: 37, valid_loss: 0.01566533622546838\n",
      "FOLD: 2, EPOCH: 38, train_loss: 0.01667249811863577\n",
      "FOLD: 2, EPOCH: 38, valid_loss: 0.015666008067245666\n",
      "FOLD: 2, EPOCH: 39, train_loss: 0.01657623812757634\n",
      "FOLD: 2, EPOCH: 39, valid_loss: 0.015635353226501208\n",
      "FOLD: 2, EPOCH: 40, train_loss: 0.016488765366375446\n",
      "FOLD: 2, EPOCH: 40, valid_loss: 0.01566078893553752\n",
      "Epoch    41: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 2, EPOCH: 41, train_loss: 0.016275139059871435\n",
      "FOLD: 2, EPOCH: 41, valid_loss: 0.01567659369454934\n",
      "FOLD: 2, EPOCH: 42, train_loss: 0.01612444273573724\n",
      "FOLD: 2, EPOCH: 42, valid_loss: 0.01568464175439798\n",
      "FOLD: 2, EPOCH: 43, train_loss: 0.01606964518794337\n",
      "FOLD: 2, EPOCH: 43, valid_loss: 0.015672648755403664\n",
      "FOLD: 2, EPOCH: 44, train_loss: 0.01586737622179695\n",
      "FOLD: 2, EPOCH: 44, valid_loss: 0.015665602297163926\n",
      "FOLD: 2, EPOCH: 45, train_loss: 0.01584077310572202\n",
      "FOLD: 2, EPOCH: 45, valid_loss: 0.015654987154098656\n",
      "FOLD: 2, EPOCH: 46, train_loss: 0.015733045601361507\n",
      "FOLD: 2, EPOCH: 46, valid_loss: 0.01567730737420229\n",
      "FOLD: 2, EPOCH: 47, train_loss: 0.01569508825353271\n",
      "FOLD: 2, EPOCH: 47, valid_loss: 0.015664736883571513\n",
      "Epoch    48: reducing learning rate of group 0 to 1.1288e-01.\n",
      "FOLD: 2, EPOCH: 48, train_loss: 0.015484101000569156\n",
      "FOLD: 2, EPOCH: 48, valid_loss: 0.01566989368830736\n",
      "FOLD: 2, EPOCH: 49, train_loss: 0.015437112982712081\n",
      "FOLD: 2, EPOCH: 49, valid_loss: 0.015703890830851518\n",
      "FOLD: 3, EPOCH: 0, train_loss: 0.042405675844968974\n",
      "FOLD: 3, EPOCH: 0, valid_loss: 0.020029577211691782\n",
      "FOLD: 3, EPOCH: 1, train_loss: 0.022796257852098427\n",
      "FOLD: 3, EPOCH: 1, valid_loss: 0.019336105969089728\n",
      "FOLD: 3, EPOCH: 2, train_loss: 0.02195828906386285\n",
      "FOLD: 3, EPOCH: 2, valid_loss: 0.018251120041196164\n",
      "FOLD: 3, EPOCH: 3, train_loss: 0.021360393890456575\n",
      "FOLD: 3, EPOCH: 3, valid_loss: 0.01772769956061473\n",
      "FOLD: 3, EPOCH: 4, train_loss: 0.02100139984709991\n",
      "FOLD: 3, EPOCH: 4, valid_loss: 0.01766905804666189\n",
      "FOLD: 3, EPOCH: 5, train_loss: 0.02075328995045778\n",
      "FOLD: 3, EPOCH: 5, valid_loss: 0.017261046916246414\n",
      "FOLD: 3, EPOCH: 6, train_loss: 0.020474786417105713\n",
      "FOLD: 3, EPOCH: 6, valid_loss: 0.017057383647904947\n",
      "FOLD: 3, EPOCH: 7, train_loss: 0.02030330687459256\n",
      "FOLD: 3, EPOCH: 7, valid_loss: 0.017027949484495018\n",
      "FOLD: 3, EPOCH: 8, train_loss: 0.02014205864051709\n",
      "FOLD: 3, EPOCH: 8, valid_loss: 0.016900996677577496\n",
      "FOLD: 3, EPOCH: 9, train_loss: 0.01998357505008981\n",
      "FOLD: 3, EPOCH: 9, valid_loss: 0.016722144845586557\n",
      "FOLD: 3, EPOCH: 10, train_loss: 0.01986198389046901\n",
      "FOLD: 3, EPOCH: 10, valid_loss: 0.016624179668724537\n",
      "FOLD: 3, EPOCH: 11, train_loss: 0.019702964260972834\n",
      "FOLD: 3, EPOCH: 11, valid_loss: 0.01657949359371112\n",
      "FOLD: 3, EPOCH: 12, train_loss: 0.01957776057659774\n",
      "FOLD: 3, EPOCH: 12, valid_loss: 0.016468731829753287\n",
      "FOLD: 3, EPOCH: 13, train_loss: 0.019432843562114884\n",
      "FOLD: 3, EPOCH: 13, valid_loss: 0.016407079206636317\n",
      "FOLD: 3, EPOCH: 14, train_loss: 0.0192735390485944\n",
      "FOLD: 3, EPOCH: 14, valid_loss: 0.01640408865820903\n",
      "FOLD: 3, EPOCH: 15, train_loss: 0.019117195184367733\n",
      "FOLD: 3, EPOCH: 15, valid_loss: 0.01632117437055478\n",
      "FOLD: 3, EPOCH: 16, train_loss: 0.019010023943878507\n",
      "FOLD: 3, EPOCH: 16, valid_loss: 0.016220553133350153\n",
      "FOLD: 3, EPOCH: 17, train_loss: 0.01894257977806233\n",
      "FOLD: 3, EPOCH: 17, valid_loss: 0.016179178841412067\n",
      "FOLD: 3, EPOCH: 18, train_loss: 0.01881902481152399\n",
      "FOLD: 3, EPOCH: 18, valid_loss: 0.016170342954305503\n",
      "FOLD: 3, EPOCH: 19, train_loss: 0.01870090640275865\n",
      "FOLD: 3, EPOCH: 19, valid_loss: 0.016099772559335597\n",
      "FOLD: 3, EPOCH: 20, train_loss: 0.01860636832645616\n",
      "FOLD: 3, EPOCH: 20, valid_loss: 0.016369502776517317\n",
      "FOLD: 3, EPOCH: 21, train_loss: 0.018496588426264556\n",
      "FOLD: 3, EPOCH: 21, valid_loss: 0.016080978971261244\n",
      "FOLD: 3, EPOCH: 22, train_loss: 0.01836497731808875\n",
      "FOLD: 3, EPOCH: 22, valid_loss: 0.0160796415204039\n",
      "FOLD: 3, EPOCH: 23, train_loss: 0.018254118870843102\n",
      "FOLD: 3, EPOCH: 23, valid_loss: 0.01600586945334306\n",
      "FOLD: 3, EPOCH: 24, train_loss: 0.0181663460727479\n",
      "FOLD: 3, EPOCH: 24, valid_loss: 0.015998204023792192\n",
      "FOLD: 3, EPOCH: 25, train_loss: 0.018033378858219932\n",
      "FOLD: 3, EPOCH: 25, valid_loss: 0.015993183908554223\n",
      "FOLD: 3, EPOCH: 26, train_loss: 0.01793161515347861\n",
      "FOLD: 3, EPOCH: 26, valid_loss: 0.015995986616382234\n",
      "FOLD: 3, EPOCH: 27, train_loss: 0.0178162221721298\n",
      "FOLD: 3, EPOCH: 27, valid_loss: 0.01596313822441376\n",
      "FOLD: 3, EPOCH: 28, train_loss: 0.017707027822128824\n",
      "FOLD: 3, EPOCH: 28, valid_loss: 0.015930888099739186\n",
      "FOLD: 3, EPOCH: 29, train_loss: 0.017596841890465568\n",
      "FOLD: 3, EPOCH: 29, valid_loss: 0.01594773572511398\n",
      "FOLD: 3, EPOCH: 30, train_loss: 0.01752621859211374\n",
      "FOLD: 3, EPOCH: 30, valid_loss: 0.01594848049661288\n",
      "FOLD: 3, EPOCH: 31, train_loss: 0.017400903249713214\n",
      "FOLD: 3, EPOCH: 31, valid_loss: 0.015971882053865835\n",
      "FOLD: 3, EPOCH: 32, train_loss: 0.01733207654812046\n",
      "FOLD: 3, EPOCH: 32, valid_loss: 0.015911607286677912\n",
      "FOLD: 3, EPOCH: 33, train_loss: 0.017196447664016002\n",
      "FOLD: 3, EPOCH: 33, valid_loss: 0.015913308168259952\n",
      "FOLD: 3, EPOCH: 34, train_loss: 0.017156513042848657\n",
      "FOLD: 3, EPOCH: 34, valid_loss: 0.015905651192252453\n",
      "FOLD: 3, EPOCH: 35, train_loss: 0.017010483211158094\n",
      "FOLD: 3, EPOCH: 35, valid_loss: 0.01586344766502197\n",
      "FOLD: 3, EPOCH: 36, train_loss: 0.01684960546727116\n",
      "FOLD: 3, EPOCH: 36, valid_loss: 0.015883971793720357\n",
      "FOLD: 3, EPOCH: 37, train_loss: 0.01680154797646242\n",
      "FOLD: 3, EPOCH: 37, valid_loss: 0.015874632061100923\n",
      "FOLD: 3, EPOCH: 38, train_loss: 0.016700341416573204\n",
      "FOLD: 3, EPOCH: 38, valid_loss: 0.015835226155244388\n",
      "FOLD: 3, EPOCH: 39, train_loss: 0.01661929257868512\n",
      "FOLD: 3, EPOCH: 39, valid_loss: 0.0158200402959035\n",
      "FOLD: 3, EPOCH: 40, train_loss: 0.016440058340998116\n",
      "FOLD: 3, EPOCH: 40, valid_loss: 0.015893165451975968\n",
      "FOLD: 3, EPOCH: 41, train_loss: 0.016365223815916357\n",
      "FOLD: 3, EPOCH: 41, valid_loss: 0.015846956377992265\n",
      "FOLD: 3, EPOCH: 42, train_loss: 0.016265702031150058\n",
      "FOLD: 3, EPOCH: 42, valid_loss: 0.01584066952077242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 3, EPOCH: 43, train_loss: 0.016168874298297876\n",
      "FOLD: 3, EPOCH: 43, valid_loss: 0.015822183054227095\n",
      "FOLD: 3, EPOCH: 44, train_loss: 0.01605021909533723\n",
      "FOLD: 3, EPOCH: 44, valid_loss: 0.01586298759167011\n",
      "FOLD: 3, EPOCH: 45, train_loss: 0.015973625994111236\n",
      "FOLD: 3, EPOCH: 45, valid_loss: 0.015859904770667736\n",
      "FOLD: 3, EPOCH: 46, train_loss: 0.01590255881986908\n",
      "FOLD: 3, EPOCH: 46, valid_loss: 0.015890800537398227\n",
      "Epoch    47: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 3, EPOCH: 47, train_loss: 0.01566938843225708\n",
      "FOLD: 3, EPOCH: 47, valid_loss: 0.015872390152743228\n",
      "FOLD: 3, EPOCH: 48, train_loss: 0.015547844744916703\n",
      "FOLD: 3, EPOCH: 48, valid_loss: 0.015898454576157607\n",
      "FOLD: 3, EPOCH: 49, train_loss: 0.015383687789073668\n",
      "FOLD: 3, EPOCH: 49, valid_loss: 0.015894604560274344\n",
      "FOLD: 4, EPOCH: 0, train_loss: 0.04281797428690904\n",
      "FOLD: 4, EPOCH: 0, valid_loss: 0.020531180386359874\n",
      "FOLD: 4, EPOCH: 1, train_loss: 0.02262191819278775\n",
      "FOLD: 4, EPOCH: 1, valid_loss: 0.01884303714793462\n",
      "FOLD: 4, EPOCH: 2, train_loss: 0.02181230462785508\n",
      "FOLD: 4, EPOCH: 2, valid_loss: 0.018202326761988494\n",
      "FOLD: 4, EPOCH: 3, train_loss: 0.02126177873563122\n",
      "FOLD: 4, EPOCH: 3, valid_loss: 0.017844277075850047\n",
      "FOLD: 4, EPOCH: 4, train_loss: 0.020898803263097197\n",
      "FOLD: 4, EPOCH: 4, valid_loss: 0.017607707960101273\n",
      "FOLD: 4, EPOCH: 5, train_loss: 0.020738505607319845\n",
      "FOLD: 4, EPOCH: 5, valid_loss: 0.017472649136414893\n",
      "FOLD: 4, EPOCH: 6, train_loss: 0.020463712891010014\n",
      "FOLD: 4, EPOCH: 6, valid_loss: 0.017251411739450235\n",
      "FOLD: 4, EPOCH: 7, train_loss: 0.020236446698372428\n",
      "FOLD: 4, EPOCH: 7, valid_loss: 0.01713343093601557\n",
      "FOLD: 4, EPOCH: 8, train_loss: 0.02010991087032331\n",
      "FOLD: 4, EPOCH: 8, valid_loss: 0.01707139267371251\n",
      "FOLD: 4, EPOCH: 9, train_loss: 0.019968900225452474\n",
      "FOLD: 4, EPOCH: 9, valid_loss: 0.016856986599472854\n",
      "FOLD: 4, EPOCH: 10, train_loss: 0.019812358417422384\n",
      "FOLD: 4, EPOCH: 10, valid_loss: 0.016772116749332502\n",
      "FOLD: 4, EPOCH: 11, train_loss: 0.019659463442056567\n",
      "FOLD: 4, EPOCH: 11, valid_loss: 0.01663465998493708\n",
      "FOLD: 4, EPOCH: 12, train_loss: 0.01953919035558765\n",
      "FOLD: 4, EPOCH: 12, valid_loss: 0.016603382161030404\n",
      "FOLD: 4, EPOCH: 13, train_loss: 0.019372956046985614\n",
      "FOLD: 4, EPOCH: 13, valid_loss: 0.01650609219303498\n",
      "FOLD: 4, EPOCH: 14, train_loss: 0.019283817778970744\n",
      "FOLD: 4, EPOCH: 14, valid_loss: 0.016415760207634706\n",
      "FOLD: 4, EPOCH: 15, train_loss: 0.019140798271306464\n",
      "FOLD: 4, EPOCH: 15, valid_loss: 0.016390055059813537\n",
      "FOLD: 4, EPOCH: 16, train_loss: 0.0190057666670229\n",
      "FOLD: 4, EPOCH: 16, valid_loss: 0.016324051655828953\n",
      "FOLD: 4, EPOCH: 17, train_loss: 0.018901643796345673\n",
      "FOLD: 4, EPOCH: 17, valid_loss: 0.016312536425315417\n",
      "FOLD: 4, EPOCH: 18, train_loss: 0.01878923613175347\n",
      "FOLD: 4, EPOCH: 18, valid_loss: 0.01628086293259492\n",
      "FOLD: 4, EPOCH: 19, train_loss: 0.018657278999484873\n",
      "FOLD: 4, EPOCH: 19, valid_loss: 0.016203510431716077\n",
      "FOLD: 4, EPOCH: 20, train_loss: 0.01859055769101188\n",
      "FOLD: 4, EPOCH: 20, valid_loss: 0.01616229432133528\n",
      "FOLD: 4, EPOCH: 21, train_loss: 0.018473542144371045\n",
      "FOLD: 4, EPOCH: 21, valid_loss: 0.01613096076135452\n",
      "FOLD: 4, EPOCH: 22, train_loss: 0.018394434472193587\n",
      "FOLD: 4, EPOCH: 22, valid_loss: 0.01616819382000428\n",
      "FOLD: 4, EPOCH: 23, train_loss: 0.018290771976918786\n",
      "FOLD: 4, EPOCH: 23, valid_loss: 0.0161470970712029\n",
      "FOLD: 4, EPOCH: 24, train_loss: 0.018147496305204725\n",
      "FOLD: 4, EPOCH: 24, valid_loss: 0.016121656633913517\n",
      "FOLD: 4, EPOCH: 25, train_loss: 0.018065771943814046\n",
      "FOLD: 4, EPOCH: 25, valid_loss: 0.016113675056168668\n",
      "FOLD: 4, EPOCH: 26, train_loss: 0.017971578872183692\n",
      "FOLD: 4, EPOCH: 26, valid_loss: 0.016085011884570122\n",
      "FOLD: 4, EPOCH: 27, train_loss: 0.017843556691061805\n",
      "FOLD: 4, EPOCH: 27, valid_loss: 0.016102045846100036\n",
      "FOLD: 4, EPOCH: 28, train_loss: 0.017752523483658158\n",
      "FOLD: 4, EPOCH: 28, valid_loss: 0.01602433743671729\n",
      "FOLD: 4, EPOCH: 29, train_loss: 0.017638399391560942\n",
      "FOLD: 4, EPOCH: 29, valid_loss: 0.01604303180311735\n",
      "FOLD: 4, EPOCH: 30, train_loss: 0.017552958675534337\n",
      "FOLD: 4, EPOCH: 30, valid_loss: 0.016004245799894515\n",
      "FOLD: 4, EPOCH: 31, train_loss: 0.017454415459085156\n",
      "FOLD: 4, EPOCH: 31, valid_loss: 0.015972439486246843\n",
      "FOLD: 4, EPOCH: 32, train_loss: 0.017363325227051973\n",
      "FOLD: 4, EPOCH: 32, valid_loss: 0.015994555603426237\n",
      "FOLD: 4, EPOCH: 33, train_loss: 0.017228159071827256\n",
      "FOLD: 4, EPOCH: 33, valid_loss: 0.015991532387068637\n",
      "FOLD: 4, EPOCH: 34, train_loss: 0.017082320337460655\n",
      "FOLD: 4, EPOCH: 34, valid_loss: 0.015963985871237058\n",
      "FOLD: 4, EPOCH: 35, train_loss: 0.017032665389313084\n",
      "FOLD: 4, EPOCH: 35, valid_loss: 0.016016027317024194\n",
      "FOLD: 4, EPOCH: 36, train_loss: 0.016914321361361322\n",
      "FOLD: 4, EPOCH: 36, valid_loss: 0.016017950856341764\n",
      "FOLD: 4, EPOCH: 37, train_loss: 0.016838036527907527\n",
      "FOLD: 4, EPOCH: 37, valid_loss: 0.016003519153365724\n",
      "FOLD: 4, EPOCH: 38, train_loss: 0.016737752342344942\n",
      "FOLD: 4, EPOCH: 38, valid_loss: 0.016001273233156938\n",
      "FOLD: 4, EPOCH: 39, train_loss: 0.0166107014425703\n",
      "FOLD: 4, EPOCH: 39, valid_loss: 0.015963693865789816\n",
      "FOLD: 4, EPOCH: 40, train_loss: 0.01652043205811768\n",
      "FOLD: 4, EPOCH: 40, valid_loss: 0.015970906171088036\n",
      "FOLD: 4, EPOCH: 41, train_loss: 0.01641640140096078\n",
      "FOLD: 4, EPOCH: 41, valid_loss: 0.01597499489211119\n",
      "FOLD: 4, EPOCH: 42, train_loss: 0.016325252118042193\n",
      "FOLD: 4, EPOCH: 42, valid_loss: 0.01596188387618615\n",
      "FOLD: 4, EPOCH: 43, train_loss: 0.016188811548557634\n",
      "FOLD: 4, EPOCH: 43, valid_loss: 0.015980302141262934\n",
      "FOLD: 4, EPOCH: 44, train_loss: 0.016134037044704765\n",
      "FOLD: 4, EPOCH: 44, valid_loss: 0.015973368516335122\n",
      "FOLD: 4, EPOCH: 45, train_loss: 0.01602299321392501\n",
      "FOLD: 4, EPOCH: 45, valid_loss: 0.01598826043594342\n",
      "FOLD: 4, EPOCH: 46, train_loss: 0.015890220104641205\n",
      "FOLD: 4, EPOCH: 46, valid_loss: 0.015973655650248893\n",
      "FOLD: 4, EPOCH: 47, train_loss: 0.015858593220646318\n",
      "FOLD: 4, EPOCH: 47, valid_loss: 0.015970594249665737\n",
      "FOLD: 4, EPOCH: 48, train_loss: 0.015692093308914353\n",
      "FOLD: 4, EPOCH: 48, valid_loss: 0.01599093755850425\n",
      "FOLD: 4, EPOCH: 49, train_loss: 0.015616019621390748\n",
      "FOLD: 4, EPOCH: 49, valid_loss: 0.01599771701372587\n",
      "Epoch    50: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 5, EPOCH: 0, train_loss: 0.04661502618644688\n",
      "FOLD: 5, EPOCH: 0, valid_loss: 0.02069820907826607\n",
      "FOLD: 5, EPOCH: 1, train_loss: 0.022768167401286395\n",
      "FOLD: 5, EPOCH: 1, valid_loss: 0.019050252695496265\n",
      "FOLD: 5, EPOCH: 2, train_loss: 0.02189869979849538\n",
      "FOLD: 5, EPOCH: 2, valid_loss: 0.01824191542199025\n",
      "FOLD: 5, EPOCH: 3, train_loss: 0.021329576376120787\n",
      "FOLD: 5, EPOCH: 3, valid_loss: 0.017884856949631985\n",
      "FOLD: 5, EPOCH: 4, train_loss: 0.02100197963315893\n",
      "FOLD: 5, EPOCH: 4, valid_loss: 0.017631036444352224\n",
      "FOLD: 5, EPOCH: 5, train_loss: 0.020730902314991563\n",
      "FOLD: 5, EPOCH: 5, valid_loss: 0.017363661327041112\n",
      "FOLD: 5, EPOCH: 6, train_loss: 0.020542020431241474\n",
      "FOLD: 5, EPOCH: 6, valid_loss: 0.017268247472552154\n",
      "FOLD: 5, EPOCH: 7, train_loss: 0.020372708906998504\n",
      "FOLD: 5, EPOCH: 7, valid_loss: 0.01717112070092788\n",
      "FOLD: 5, EPOCH: 8, train_loss: 0.020194985098331363\n",
      "FOLD: 5, EPOCH: 8, valid_loss: 0.016940815947376765\n",
      "FOLD: 5, EPOCH: 9, train_loss: 0.020031996851635946\n",
      "FOLD: 5, EPOCH: 9, valid_loss: 0.016911126816501983\n",
      "FOLD: 5, EPOCH: 10, train_loss: 0.019916847776118164\n",
      "FOLD: 5, EPOCH: 10, valid_loss: 0.01677737865023888\n",
      "FOLD: 5, EPOCH: 11, train_loss: 0.019762450124363642\n",
      "FOLD: 5, EPOCH: 11, valid_loss: 0.016697627134048022\n",
      "FOLD: 5, EPOCH: 12, train_loss: 0.019631633157464298\n",
      "FOLD: 5, EPOCH: 12, valid_loss: 0.016553616581054833\n",
      "FOLD: 5, EPOCH: 13, train_loss: 0.019488564994488214\n",
      "FOLD: 5, EPOCH: 13, valid_loss: 0.016502238523501616\n",
      "FOLD: 5, EPOCH: 14, train_loss: 0.019385890884173883\n",
      "FOLD: 5, EPOCH: 14, valid_loss: 0.016373673740487833\n",
      "FOLD: 5, EPOCH: 15, train_loss: 0.019198307087896643\n",
      "FOLD: 5, EPOCH: 15, valid_loss: 0.01637584909510154\n",
      "FOLD: 5, EPOCH: 16, train_loss: 0.01912667489031682\n",
      "FOLD: 5, EPOCH: 16, valid_loss: 0.016373037074047785\n",
      "FOLD: 5, EPOCH: 17, train_loss: 0.019033266734835262\n",
      "FOLD: 5, EPOCH: 17, valid_loss: 0.016291596568547763\n",
      "FOLD: 5, EPOCH: 18, train_loss: 0.018923954710968444\n",
      "FOLD: 5, EPOCH: 18, valid_loss: 0.016257796006707046\n",
      "FOLD: 5, EPOCH: 19, train_loss: 0.018822432621508033\n",
      "FOLD: 5, EPOCH: 19, valid_loss: 0.016185895253259402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 5, EPOCH: 20, train_loss: 0.018689880147576332\n",
      "FOLD: 5, EPOCH: 20, valid_loss: 0.01616632343771366\n",
      "FOLD: 5, EPOCH: 21, train_loss: 0.018599385974576343\n",
      "FOLD: 5, EPOCH: 21, valid_loss: 0.016121702985121653\n",
      "FOLD: 5, EPOCH: 22, train_loss: 0.018486285003254544\n",
      "FOLD: 5, EPOCH: 22, valid_loss: 0.01611044379667594\n",
      "FOLD: 5, EPOCH: 23, train_loss: 0.01838795400249797\n",
      "FOLD: 5, EPOCH: 23, valid_loss: 0.016079285110418614\n",
      "FOLD: 5, EPOCH: 24, train_loss: 0.018273350581325388\n",
      "FOLD: 5, EPOCH: 24, valid_loss: 0.01605986409748976\n",
      "FOLD: 5, EPOCH: 25, train_loss: 0.01820840112663604\n",
      "FOLD: 5, EPOCH: 25, valid_loss: 0.01605120960336465\n",
      "FOLD: 5, EPOCH: 26, train_loss: 0.018107147283248\n",
      "FOLD: 5, EPOCH: 26, valid_loss: 0.016018008096860006\n",
      "FOLD: 5, EPOCH: 27, train_loss: 0.01798795475750356\n",
      "FOLD: 5, EPOCH: 27, valid_loss: 0.016027196310460567\n",
      "FOLD: 5, EPOCH: 28, train_loss: 0.017891556704165163\n",
      "FOLD: 5, EPOCH: 28, valid_loss: 0.015978270568526708\n",
      "FOLD: 5, EPOCH: 29, train_loss: 0.017806048026761494\n",
      "FOLD: 5, EPOCH: 29, valid_loss: 0.015910937164265376\n",
      "FOLD: 5, EPOCH: 30, train_loss: 0.017701031086412636\n",
      "FOLD: 5, EPOCH: 30, valid_loss: 0.015945711746238746\n",
      "FOLD: 5, EPOCH: 31, train_loss: 0.017584411074986327\n",
      "FOLD: 5, EPOCH: 31, valid_loss: 0.015919043539235227\n",
      "FOLD: 5, EPOCH: 32, train_loss: 0.017511987404243368\n",
      "FOLD: 5, EPOCH: 32, valid_loss: 0.01593744568526745\n",
      "FOLD: 5, EPOCH: 33, train_loss: 0.017374254385563167\n",
      "FOLD: 5, EPOCH: 33, valid_loss: 0.015882911518789254\n",
      "FOLD: 5, EPOCH: 34, train_loss: 0.017330167865430988\n",
      "FOLD: 5, EPOCH: 34, valid_loss: 0.01592940349991505\n",
      "FOLD: 5, EPOCH: 35, train_loss: 0.017174190438881114\n",
      "FOLD: 5, EPOCH: 35, valid_loss: 0.015878593549132347\n",
      "FOLD: 5, EPOCH: 36, train_loss: 0.01707672456128372\n",
      "FOLD: 5, EPOCH: 36, valid_loss: 0.015893922330668338\n",
      "FOLD: 5, EPOCH: 37, train_loss: 0.016989974632255128\n",
      "FOLD: 5, EPOCH: 37, valid_loss: 0.015867913500047647\n",
      "FOLD: 5, EPOCH: 38, train_loss: 0.016877877677010523\n",
      "FOLD: 5, EPOCH: 38, valid_loss: 0.0158467202518995\n",
      "FOLD: 5, EPOCH: 39, train_loss: 0.01677955748714708\n",
      "FOLD: 5, EPOCH: 39, valid_loss: 0.015860019753185604\n",
      "FOLD: 5, EPOCH: 40, train_loss: 0.01670894040600271\n",
      "FOLD: 5, EPOCH: 40, valid_loss: 0.015907539484592583\n",
      "FOLD: 5, EPOCH: 41, train_loss: 0.0165995820803014\n",
      "FOLD: 5, EPOCH: 41, valid_loss: 0.015857569730052583\n",
      "FOLD: 5, EPOCH: 42, train_loss: 0.01645301152775819\n",
      "FOLD: 5, EPOCH: 42, valid_loss: 0.015910259519632045\n",
      "FOLD: 5, EPOCH: 43, train_loss: 0.016398454563239136\n",
      "FOLD: 5, EPOCH: 43, valid_loss: 0.015897203236818314\n",
      "FOLD: 5, EPOCH: 44, train_loss: 0.016303677376158333\n",
      "FOLD: 5, EPOCH: 44, valid_loss: 0.015906150022951458\n",
      "FOLD: 5, EPOCH: 45, train_loss: 0.01618460242359622\n",
      "FOLD: 5, EPOCH: 45, valid_loss: 0.01591403502970934\n",
      "Epoch    46: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 5, EPOCH: 46, train_loss: 0.015969701249756524\n",
      "FOLD: 5, EPOCH: 46, valid_loss: 0.015865473578182552\n",
      "FOLD: 5, EPOCH: 47, train_loss: 0.015849359736249253\n",
      "FOLD: 5, EPOCH: 47, valid_loss: 0.01587777269574312\n",
      "FOLD: 5, EPOCH: 48, train_loss: 0.015737464500440133\n",
      "FOLD: 5, EPOCH: 48, valid_loss: 0.015899958088994026\n",
      "FOLD: 5, EPOCH: 49, train_loss: 0.015668906447653834\n",
      "FOLD: 5, EPOCH: 49, valid_loss: 0.01589112298992964\n",
      "FOLD: 6, EPOCH: 0, train_loss: 0.042341179440955855\n",
      "FOLD: 6, EPOCH: 0, valid_loss: 0.019981872003812056\n",
      "FOLD: 6, EPOCH: 1, train_loss: 0.022695447393768543\n",
      "FOLD: 6, EPOCH: 1, valid_loss: 0.01882540979064428\n",
      "FOLD: 6, EPOCH: 2, train_loss: 0.021804897542539482\n",
      "FOLD: 6, EPOCH: 2, valid_loss: 0.017804989018119298\n",
      "FOLD: 6, EPOCH: 3, train_loss: 0.021238586815023743\n",
      "FOLD: 6, EPOCH: 3, valid_loss: 0.01745871726710063\n",
      "FOLD: 6, EPOCH: 4, train_loss: 0.02088744578429976\n",
      "FOLD: 6, EPOCH: 4, valid_loss: 0.01717477349134592\n",
      "FOLD: 6, EPOCH: 5, train_loss: 0.020665490295033197\n",
      "FOLD: 6, EPOCH: 5, valid_loss: 0.017039834163509883\n",
      "FOLD: 6, EPOCH: 6, train_loss: 0.02039489258282088\n",
      "FOLD: 6, EPOCH: 6, valid_loss: 0.016796139212181933\n",
      "FOLD: 6, EPOCH: 7, train_loss: 0.020235516483316552\n",
      "FOLD: 6, EPOCH: 7, valid_loss: 0.016710107644590046\n",
      "FOLD: 6, EPOCH: 8, train_loss: 0.020099223641729034\n",
      "FOLD: 6, EPOCH: 8, valid_loss: 0.01665730244265153\n",
      "FOLD: 6, EPOCH: 9, train_loss: 0.019996970262680505\n",
      "FOLD: 6, EPOCH: 9, valid_loss: 0.016592531106792964\n",
      "FOLD: 6, EPOCH: 10, train_loss: 0.019805616044716257\n",
      "FOLD: 6, EPOCH: 10, valid_loss: 0.01639261834609967\n",
      "FOLD: 6, EPOCH: 11, train_loss: 0.019642266872766857\n",
      "FOLD: 6, EPOCH: 11, valid_loss: 0.016413638439889137\n",
      "FOLD: 6, EPOCH: 12, train_loss: 0.019488679899557215\n",
      "FOLD: 6, EPOCH: 12, valid_loss: 0.01627217333477277\n",
      "FOLD: 6, EPOCH: 13, train_loss: 0.019377559071054328\n",
      "FOLD: 6, EPOCH: 13, valid_loss: 0.01627529742052922\n",
      "FOLD: 6, EPOCH: 14, train_loss: 0.019268566138438276\n",
      "FOLD: 6, EPOCH: 14, valid_loss: 0.01617897695933397\n",
      "FOLD: 6, EPOCH: 15, train_loss: 0.019146414096089633\n",
      "FOLD: 6, EPOCH: 15, valid_loss: 0.016114633745298937\n",
      "FOLD: 6, EPOCH: 16, train_loss: 0.019040219359905332\n",
      "FOLD: 6, EPOCH: 16, valid_loss: 0.016173765851328008\n",
      "FOLD: 6, EPOCH: 17, train_loss: 0.018906585469439224\n",
      "FOLD: 6, EPOCH: 17, valid_loss: 0.01605928108955805\n",
      "FOLD: 6, EPOCH: 18, train_loss: 0.01874284421068591\n",
      "FOLD: 6, EPOCH: 18, valid_loss: 0.015992270567669317\n",
      "FOLD: 6, EPOCH: 19, train_loss: 0.01866052231776553\n",
      "FOLD: 6, EPOCH: 19, valid_loss: 0.015883517666504934\n",
      "FOLD: 6, EPOCH: 20, train_loss: 0.018554866137738164\n",
      "FOLD: 6, EPOCH: 20, valid_loss: 0.015931618113357287\n",
      "FOLD: 6, EPOCH: 21, train_loss: 0.01845494974907991\n",
      "FOLD: 6, EPOCH: 21, valid_loss: 0.015868076194937412\n",
      "FOLD: 6, EPOCH: 22, train_loss: 0.018314479623694677\n",
      "FOLD: 6, EPOCH: 22, valid_loss: 0.01582849183334754\n",
      "FOLD: 6, EPOCH: 23, train_loss: 0.01826375997247728\n",
      "FOLD: 6, EPOCH: 23, valid_loss: 0.015798332957694165\n",
      "FOLD: 6, EPOCH: 24, train_loss: 0.018115753761014423\n",
      "FOLD: 6, EPOCH: 24, valid_loss: 0.015808115140176736\n",
      "FOLD: 6, EPOCH: 25, train_loss: 0.018017146761554317\n",
      "FOLD: 6, EPOCH: 25, valid_loss: 0.01573356190839639\n",
      "FOLD: 6, EPOCH: 26, train_loss: 0.0178910723660846\n",
      "FOLD: 6, EPOCH: 26, valid_loss: 0.015747835501455344\n",
      "FOLD: 6, EPOCH: 27, train_loss: 0.01774910558015108\n",
      "FOLD: 6, EPOCH: 27, valid_loss: 0.015780290158895347\n",
      "FOLD: 6, EPOCH: 28, train_loss: 0.017633184186510137\n",
      "FOLD: 6, EPOCH: 28, valid_loss: 0.015721450774715498\n",
      "FOLD: 6, EPOCH: 29, train_loss: 0.0175517238676548\n",
      "FOLD: 6, EPOCH: 29, valid_loss: 0.015697499020741537\n",
      "FOLD: 6, EPOCH: 30, train_loss: 0.017421818615214246\n",
      "FOLD: 6, EPOCH: 30, valid_loss: 0.015693602008888356\n",
      "FOLD: 6, EPOCH: 31, train_loss: 0.01736344409653464\n",
      "FOLD: 6, EPOCH: 31, valid_loss: 0.015694310100605853\n",
      "FOLD: 6, EPOCH: 32, train_loss: 0.01724818112278307\n",
      "FOLD: 6, EPOCH: 32, valid_loss: 0.01566592023636286\n",
      "FOLD: 6, EPOCH: 33, train_loss: 0.01717392900153189\n",
      "FOLD: 6, EPOCH: 33, valid_loss: 0.01563354667562705\n",
      "FOLD: 6, EPOCH: 34, train_loss: 0.017093203055697517\n",
      "FOLD: 6, EPOCH: 34, valid_loss: 0.01567129511386156\n",
      "FOLD: 6, EPOCH: 35, train_loss: 0.016960683958353224\n",
      "FOLD: 6, EPOCH: 35, valid_loss: 0.015653367154300213\n",
      "FOLD: 6, EPOCH: 36, train_loss: 0.016876631680674648\n",
      "FOLD: 6, EPOCH: 36, valid_loss: 0.015695149867007367\n",
      "FOLD: 6, EPOCH: 37, train_loss: 0.01675558028541304\n",
      "FOLD: 6, EPOCH: 37, valid_loss: 0.015679586320542373\n",
      "FOLD: 6, EPOCH: 38, train_loss: 0.01664310397983\n",
      "FOLD: 6, EPOCH: 38, valid_loss: 0.01564388320996211\n",
      "FOLD: 6, EPOCH: 39, train_loss: 0.016524334136094596\n",
      "FOLD: 6, EPOCH: 39, valid_loss: 0.015661685154415093\n",
      "FOLD: 6, EPOCH: 40, train_loss: 0.016465801154137462\n",
      "FOLD: 6, EPOCH: 40, valid_loss: 0.015654402068601206\n",
      "Epoch    41: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 6, EPOCH: 41, train_loss: 0.016248300042305444\n",
      "FOLD: 6, EPOCH: 41, valid_loss: 0.015621694019780709\n",
      "FOLD: 6, EPOCH: 42, train_loss: 0.01610978676408932\n",
      "FOLD: 6, EPOCH: 42, valid_loss: 0.015622536580149945\n",
      "FOLD: 6, EPOCH: 43, train_loss: 0.016041618318775215\n",
      "FOLD: 6, EPOCH: 43, valid_loss: 0.015622240921052603\n",
      "FOLD: 6, EPOCH: 44, train_loss: 0.015914731533140748\n",
      "FOLD: 6, EPOCH: 44, valid_loss: 0.015636668182336368\n",
      "FOLD: 6, EPOCH: 45, train_loss: 0.015809669725697587\n",
      "FOLD: 6, EPOCH: 45, valid_loss: 0.01563744956197647\n",
      "FOLD: 6, EPOCH: 46, train_loss: 0.015697904925390676\n",
      "FOLD: 6, EPOCH: 46, valid_loss: 0.01564495201007678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 6, EPOCH: 47, train_loss: 0.015648223736600297\n",
      "FOLD: 6, EPOCH: 47, valid_loss: 0.015628588744080983\n",
      "FOLD: 6, EPOCH: 48, train_loss: 0.015551711432635784\n",
      "FOLD: 6, EPOCH: 48, valid_loss: 0.01565563614265277\n",
      "Epoch    49: reducing learning rate of group 0 to 1.1288e-01.\n",
      "FOLD: 6, EPOCH: 49, train_loss: 0.015368871944578918\n",
      "FOLD: 6, EPOCH: 49, valid_loss: 0.01564473185974818\n",
      "FOLD: 0, EPOCH: 0, train_loss: 0.04383363594879975\n",
      "FOLD: 0, EPOCH: 0, valid_loss: 0.020065714676792804\n",
      "FOLD: 0, EPOCH: 1, train_loss: 0.022672237450810703\n",
      "FOLD: 0, EPOCH: 1, valid_loss: 0.018755562603473663\n",
      "FOLD: 0, EPOCH: 2, train_loss: 0.02172003739286919\n",
      "FOLD: 0, EPOCH: 2, valid_loss: 0.018207841337873384\n",
      "FOLD: 0, EPOCH: 3, train_loss: 0.02125492022448295\n",
      "FOLD: 0, EPOCH: 3, valid_loss: 0.017824097321583673\n",
      "FOLD: 0, EPOCH: 4, train_loss: 0.020955089932760677\n",
      "FOLD: 0, EPOCH: 4, valid_loss: 0.017547360478112332\n",
      "FOLD: 0, EPOCH: 5, train_loss: 0.020702747477067483\n",
      "FOLD: 0, EPOCH: 5, valid_loss: 0.017295837832184937\n",
      "FOLD: 0, EPOCH: 6, train_loss: 0.020485313735096843\n",
      "FOLD: 0, EPOCH: 6, valid_loss: 0.01713139585290964\n",
      "FOLD: 0, EPOCH: 7, train_loss: 0.02025774579394508\n",
      "FOLD: 0, EPOCH: 7, valid_loss: 0.016990019008517265\n",
      "FOLD: 0, EPOCH: 8, train_loss: 0.020113814722847293\n",
      "FOLD: 0, EPOCH: 8, valid_loss: 0.016903746514939345\n",
      "FOLD: 0, EPOCH: 9, train_loss: 0.019965678755496\n",
      "FOLD: 0, EPOCH: 9, valid_loss: 0.01679687495701588\n",
      "FOLD: 0, EPOCH: 10, train_loss: 0.019849410154730886\n",
      "FOLD: 0, EPOCH: 10, valid_loss: 0.016667881192496188\n",
      "FOLD: 0, EPOCH: 11, train_loss: 0.019704350698235874\n",
      "FOLD: 0, EPOCH: 11, valid_loss: 0.01651211455464363\n",
      "FOLD: 0, EPOCH: 12, train_loss: 0.019557378984786367\n",
      "FOLD: 0, EPOCH: 12, valid_loss: 0.016468265380423803\n",
      "FOLD: 0, EPOCH: 13, train_loss: 0.01942964703650088\n",
      "FOLD: 0, EPOCH: 13, valid_loss: 0.01644865867610161\n",
      "FOLD: 0, EPOCH: 14, train_loss: 0.019298769533634186\n",
      "FOLD: 0, EPOCH: 14, valid_loss: 0.01635374050014294\n",
      "FOLD: 0, EPOCH: 15, train_loss: 0.019187578830767323\n",
      "FOLD: 0, EPOCH: 15, valid_loss: 0.01629296840670017\n",
      "FOLD: 0, EPOCH: 16, train_loss: 0.019082932254752598\n",
      "FOLD: 0, EPOCH: 16, valid_loss: 0.016247106787676994\n",
      "FOLD: 0, EPOCH: 17, train_loss: 0.018946548873508298\n",
      "FOLD: 0, EPOCH: 17, valid_loss: 0.016205060152480237\n",
      "FOLD: 0, EPOCH: 18, train_loss: 0.018846114568815037\n",
      "FOLD: 0, EPOCH: 18, valid_loss: 0.016110277806337062\n",
      "FOLD: 0, EPOCH: 19, train_loss: 0.018758945720824035\n",
      "FOLD: 0, EPOCH: 19, valid_loss: 0.016121306743186254\n",
      "FOLD: 0, EPOCH: 20, train_loss: 0.018614354895780217\n",
      "FOLD: 0, EPOCH: 20, valid_loss: 0.01607277846107116\n",
      "FOLD: 0, EPOCH: 21, train_loss: 0.018545175273273443\n",
      "FOLD: 0, EPOCH: 21, valid_loss: 0.016023508201424893\n",
      "FOLD: 0, EPOCH: 22, train_loss: 0.01841031712152668\n",
      "FOLD: 0, EPOCH: 22, valid_loss: 0.016025024824417554\n",
      "FOLD: 0, EPOCH: 23, train_loss: 0.01832316833472735\n",
      "FOLD: 0, EPOCH: 23, valid_loss: 0.01594122205502712\n",
      "FOLD: 0, EPOCH: 24, train_loss: 0.01821346563362592\n",
      "FOLD: 0, EPOCH: 24, valid_loss: 0.015930894905557998\n",
      "FOLD: 0, EPOCH: 25, train_loss: 0.018095986187659407\n",
      "FOLD: 0, EPOCH: 25, valid_loss: 0.01591332357090253\n",
      "FOLD: 0, EPOCH: 26, train_loss: 0.017989042064023984\n",
      "FOLD: 0, EPOCH: 26, valid_loss: 0.015883661305102017\n",
      "FOLD: 0, EPOCH: 27, train_loss: 0.017902961202167177\n",
      "FOLD: 0, EPOCH: 27, valid_loss: 0.0158664367090051\n",
      "FOLD: 0, EPOCH: 28, train_loss: 0.01782225348005021\n",
      "FOLD: 0, EPOCH: 28, valid_loss: 0.015946219603602704\n",
      "FOLD: 0, EPOCH: 29, train_loss: 0.01772452168468688\n",
      "FOLD: 0, EPOCH: 29, valid_loss: 0.015811238939372394\n",
      "FOLD: 0, EPOCH: 30, train_loss: 0.017592556913056084\n",
      "FOLD: 0, EPOCH: 30, valid_loss: 0.015817998263698358\n",
      "FOLD: 0, EPOCH: 31, train_loss: 0.01750984030297479\n",
      "FOLD: 0, EPOCH: 31, valid_loss: 0.015825968164090928\n",
      "FOLD: 0, EPOCH: 32, train_loss: 0.017411092837416643\n",
      "FOLD: 0, EPOCH: 32, valid_loss: 0.015844666685622472\n",
      "FOLD: 0, EPOCH: 33, train_loss: 0.017305916993299853\n",
      "FOLD: 0, EPOCH: 33, valid_loss: 0.01582433791974416\n",
      "FOLD: 0, EPOCH: 34, train_loss: 0.017200960538576584\n",
      "FOLD: 0, EPOCH: 34, valid_loss: 0.01578798768325494\n",
      "FOLD: 0, EPOCH: 35, train_loss: 0.017106498427991126\n",
      "FOLD: 0, EPOCH: 35, valid_loss: 0.015787795544243775\n",
      "FOLD: 0, EPOCH: 36, train_loss: 0.016994724113091424\n",
      "FOLD: 0, EPOCH: 36, valid_loss: 0.015803788645336263\n",
      "FOLD: 0, EPOCH: 37, train_loss: 0.016871769899955473\n",
      "FOLD: 0, EPOCH: 37, valid_loss: 0.015815674542234495\n",
      "FOLD: 0, EPOCH: 38, train_loss: 0.016792893384558125\n",
      "FOLD: 0, EPOCH: 38, valid_loss: 0.015798781282053545\n",
      "FOLD: 0, EPOCH: 39, train_loss: 0.016717879051292264\n",
      "FOLD: 0, EPOCH: 39, valid_loss: 0.015797037344712477\n",
      "FOLD: 0, EPOCH: 40, train_loss: 0.0166397202583785\n",
      "FOLD: 0, EPOCH: 40, valid_loss: 0.015808135414352782\n",
      "FOLD: 0, EPOCH: 41, train_loss: 0.016454185459863494\n",
      "FOLD: 0, EPOCH: 41, valid_loss: 0.015792794812184114\n",
      "FOLD: 0, EPOCH: 42, train_loss: 0.016362812470745395\n",
      "FOLD: 0, EPOCH: 42, valid_loss: 0.01580302553394666\n",
      "Epoch    43: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 0, EPOCH: 43, train_loss: 0.016162468900753034\n",
      "FOLD: 0, EPOCH: 43, valid_loss: 0.015775512975568954\n",
      "FOLD: 0, EPOCH: 44, train_loss: 0.016052789721839333\n",
      "FOLD: 0, EPOCH: 44, valid_loss: 0.015745832226597346\n",
      "FOLD: 0, EPOCH: 45, train_loss: 0.016001569195272954\n",
      "FOLD: 0, EPOCH: 45, valid_loss: 0.0157906819994633\n",
      "FOLD: 0, EPOCH: 46, train_loss: 0.015916109664013255\n",
      "FOLD: 0, EPOCH: 46, valid_loss: 0.01577350590378046\n",
      "FOLD: 0, EPOCH: 47, train_loss: 0.01575232160000785\n",
      "FOLD: 0, EPOCH: 47, valid_loss: 0.015771190994060956\n",
      "FOLD: 0, EPOCH: 48, train_loss: 0.01567851551934271\n",
      "FOLD: 0, EPOCH: 48, valid_loss: 0.015780915721104696\n",
      "FOLD: 0, EPOCH: 49, train_loss: 0.015580675073874157\n",
      "FOLD: 0, EPOCH: 49, valid_loss: 0.015803742938889906\n",
      "FOLD: 1, EPOCH: 0, train_loss: 0.044580879847745634\n",
      "FOLD: 1, EPOCH: 0, valid_loss: 0.02047784239626848\n",
      "FOLD: 1, EPOCH: 1, train_loss: 0.022717831152919178\n",
      "FOLD: 1, EPOCH: 1, valid_loss: 0.018885733989568856\n",
      "FOLD: 1, EPOCH: 2, train_loss: 0.021854092669043992\n",
      "FOLD: 1, EPOCH: 2, valid_loss: 0.018066078137892943\n",
      "FOLD: 1, EPOCH: 3, train_loss: 0.021343759336584323\n",
      "FOLD: 1, EPOCH: 3, valid_loss: 0.017663371964142874\n",
      "FOLD: 1, EPOCH: 4, train_loss: 0.021010947061350215\n",
      "FOLD: 1, EPOCH: 4, valid_loss: 0.01751126177035845\n",
      "FOLD: 1, EPOCH: 5, train_loss: 0.020739741321351077\n",
      "FOLD: 1, EPOCH: 5, valid_loss: 0.017295085753385838\n",
      "FOLD: 1, EPOCH: 6, train_loss: 0.02056519390159362\n",
      "FOLD: 1, EPOCH: 6, valid_loss: 0.01695246249437332\n",
      "FOLD: 1, EPOCH: 7, train_loss: 0.020367975497769343\n",
      "FOLD: 1, EPOCH: 7, valid_loss: 0.01676179740864497\n",
      "FOLD: 1, EPOCH: 8, train_loss: 0.02019748471777987\n",
      "FOLD: 1, EPOCH: 8, valid_loss: 0.016662492488439266\n",
      "FOLD: 1, EPOCH: 9, train_loss: 0.020108767130689043\n",
      "FOLD: 1, EPOCH: 9, valid_loss: 0.016616665973113134\n",
      "FOLD: 1, EPOCH: 10, train_loss: 0.01991650814542899\n",
      "FOLD: 1, EPOCH: 10, valid_loss: 0.01645317465926592\n",
      "FOLD: 1, EPOCH: 11, train_loss: 0.019756195915711892\n",
      "FOLD: 1, EPOCH: 11, valid_loss: 0.016385499817820694\n",
      "FOLD: 1, EPOCH: 12, train_loss: 0.019620837694084323\n",
      "FOLD: 1, EPOCH: 12, valid_loss: 0.01628716612377992\n",
      "FOLD: 1, EPOCH: 13, train_loss: 0.019536926109041716\n",
      "FOLD: 1, EPOCH: 13, valid_loss: 0.016198160055165108\n",
      "FOLD: 1, EPOCH: 14, train_loss: 0.019407401566167135\n",
      "FOLD: 1, EPOCH: 14, valid_loss: 0.01612355997069524\n",
      "FOLD: 1, EPOCH: 15, train_loss: 0.019314788382601093\n",
      "FOLD: 1, EPOCH: 15, valid_loss: 0.01606703563951529\n",
      "FOLD: 1, EPOCH: 16, train_loss: 0.019188765335727383\n",
      "FOLD: 1, EPOCH: 16, valid_loss: 0.01602225972769352\n",
      "FOLD: 1, EPOCH: 17, train_loss: 0.019044577219599002\n",
      "FOLD: 1, EPOCH: 17, valid_loss: 0.016010307420331698\n",
      "FOLD: 1, EPOCH: 18, train_loss: 0.018957233257793093\n",
      "FOLD: 1, EPOCH: 18, valid_loss: 0.01596703179753744\n",
      "FOLD: 1, EPOCH: 19, train_loss: 0.018875380550083275\n",
      "FOLD: 1, EPOCH: 19, valid_loss: 0.015871040236491423\n",
      "FOLD: 1, EPOCH: 20, train_loss: 0.018717966520705738\n",
      "FOLD: 1, EPOCH: 20, valid_loss: 0.01585237431124999\n",
      "FOLD: 1, EPOCH: 21, train_loss: 0.018635238948706036\n",
      "FOLD: 1, EPOCH: 21, valid_loss: 0.015802454203367233\n",
      "FOLD: 1, EPOCH: 22, train_loss: 0.01854822683978725\n",
      "FOLD: 1, EPOCH: 22, valid_loss: 0.01583002658131031\n",
      "FOLD: 1, EPOCH: 23, train_loss: 0.018484415813676408\n",
      "FOLD: 1, EPOCH: 23, valid_loss: 0.015767627467329685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 1, EPOCH: 24, train_loss: 0.018346878775470966\n",
      "FOLD: 1, EPOCH: 24, valid_loss: 0.015745830793793384\n",
      "FOLD: 1, EPOCH: 25, train_loss: 0.018222794087754714\n",
      "FOLD: 1, EPOCH: 25, valid_loss: 0.015743515239312098\n",
      "FOLD: 1, EPOCH: 26, train_loss: 0.018143117729876493\n",
      "FOLD: 1, EPOCH: 26, valid_loss: 0.01571254740254237\n",
      "FOLD: 1, EPOCH: 27, train_loss: 0.01801227529004619\n",
      "FOLD: 1, EPOCH: 27, valid_loss: 0.015708041807206776\n",
      "FOLD: 1, EPOCH: 28, train_loss: 0.017925742384348367\n",
      "FOLD: 1, EPOCH: 28, valid_loss: 0.015648719997933276\n",
      "FOLD: 1, EPOCH: 29, train_loss: 0.017841548986128858\n",
      "FOLD: 1, EPOCH: 29, valid_loss: 0.015612729968359837\n",
      "FOLD: 1, EPOCH: 30, train_loss: 0.01773825494220128\n",
      "FOLD: 1, EPOCH: 30, valid_loss: 0.015588730143812986\n",
      "FOLD: 1, EPOCH: 31, train_loss: 0.01764004196770288\n",
      "FOLD: 1, EPOCH: 31, valid_loss: 0.015635240608109877\n",
      "FOLD: 1, EPOCH: 32, train_loss: 0.017585947027278913\n",
      "FOLD: 1, EPOCH: 32, valid_loss: 0.015627272427082062\n",
      "FOLD: 1, EPOCH: 33, train_loss: 0.017399618200756407\n",
      "FOLD: 1, EPOCH: 33, valid_loss: 0.01560272011332787\n",
      "FOLD: 1, EPOCH: 34, train_loss: 0.017368853218048007\n",
      "FOLD: 1, EPOCH: 34, valid_loss: 0.015587112293220483\n",
      "FOLD: 1, EPOCH: 35, train_loss: 0.0172141633323721\n",
      "FOLD: 1, EPOCH: 35, valid_loss: 0.015568823481981572\n",
      "FOLD: 1, EPOCH: 36, train_loss: 0.017157048805037867\n",
      "FOLD: 1, EPOCH: 36, valid_loss: 0.015569828307399383\n",
      "FOLD: 1, EPOCH: 37, train_loss: 0.017018553574342985\n",
      "FOLD: 1, EPOCH: 37, valid_loss: 0.015557373873889446\n",
      "FOLD: 1, EPOCH: 38, train_loss: 0.01692555021695994\n",
      "FOLD: 1, EPOCH: 38, valid_loss: 0.015534509976322834\n",
      "FOLD: 1, EPOCH: 39, train_loss: 0.0168586806771723\n",
      "FOLD: 1, EPOCH: 39, valid_loss: 0.015538674850876514\n",
      "FOLD: 1, EPOCH: 40, train_loss: 0.01675055986522017\n",
      "FOLD: 1, EPOCH: 40, valid_loss: 0.015570790005417971\n",
      "FOLD: 1, EPOCH: 41, train_loss: 0.01667254289763199\n",
      "FOLD: 1, EPOCH: 41, valid_loss: 0.015543003638203327\n",
      "FOLD: 1, EPOCH: 42, train_loss: 0.016558734888865334\n",
      "FOLD: 1, EPOCH: 42, valid_loss: 0.015561034114888081\n",
      "FOLD: 1, EPOCH: 43, train_loss: 0.016395422994989802\n",
      "FOLD: 1, EPOCH: 43, valid_loss: 0.015543765818270354\n",
      "FOLD: 1, EPOCH: 44, train_loss: 0.016414117068052292\n",
      "FOLD: 1, EPOCH: 44, valid_loss: 0.0155368042536653\n",
      "FOLD: 1, EPOCH: 45, train_loss: 0.01622702253076273\n",
      "FOLD: 1, EPOCH: 45, valid_loss: 0.015557636148654498\n",
      "Epoch    46: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 1, EPOCH: 46, train_loss: 0.01604004264683337\n",
      "FOLD: 1, EPOCH: 46, valid_loss: 0.015529821483561626\n",
      "FOLD: 1, EPOCH: 47, train_loss: 0.015951299637153343\n",
      "FOLD: 1, EPOCH: 47, valid_loss: 0.015540551895705553\n",
      "FOLD: 1, EPOCH: 48, train_loss: 0.01587320742121822\n",
      "FOLD: 1, EPOCH: 48, valid_loss: 0.015545067449028675\n",
      "FOLD: 1, EPOCH: 49, train_loss: 0.015751672543685983\n",
      "FOLD: 1, EPOCH: 49, valid_loss: 0.015594613523437427\n",
      "FOLD: 2, EPOCH: 0, train_loss: 0.04591939203139093\n",
      "FOLD: 2, EPOCH: 0, valid_loss: 0.020552566848122157\n",
      "FOLD: 2, EPOCH: 1, train_loss: 0.022873213917419716\n",
      "FOLD: 2, EPOCH: 1, valid_loss: 0.018857543356716633\n",
      "FOLD: 2, EPOCH: 2, train_loss: 0.021919635476896893\n",
      "FOLD: 2, EPOCH: 2, valid_loss: 0.018169854337779377\n",
      "FOLD: 2, EPOCH: 3, train_loss: 0.021389159399109917\n",
      "FOLD: 2, EPOCH: 3, valid_loss: 0.017628727194208365\n",
      "FOLD: 2, EPOCH: 4, train_loss: 0.02099326444236008\n",
      "FOLD: 2, EPOCH: 4, valid_loss: 0.017401879724974815\n",
      "FOLD: 2, EPOCH: 5, train_loss: 0.020776767308848934\n",
      "FOLD: 2, EPOCH: 5, valid_loss: 0.017210477891449746\n",
      "FOLD: 2, EPOCH: 6, train_loss: 0.020576754031149117\n",
      "FOLD: 2, EPOCH: 6, valid_loss: 0.017055947047013503\n",
      "FOLD: 2, EPOCH: 7, train_loss: 0.020382251087072735\n",
      "FOLD: 2, EPOCH: 7, valid_loss: 0.016808791157717887\n",
      "FOLD: 2, EPOCH: 8, train_loss: 0.020230376916761335\n",
      "FOLD: 2, EPOCH: 8, valid_loss: 0.016741214463343985\n",
      "FOLD: 2, EPOCH: 9, train_loss: 0.020030267310102243\n",
      "FOLD: 2, EPOCH: 9, valid_loss: 0.01665035757021262\n",
      "FOLD: 2, EPOCH: 10, train_loss: 0.01990913589661186\n",
      "FOLD: 2, EPOCH: 10, valid_loss: 0.016591546698831595\n",
      "FOLD: 2, EPOCH: 11, train_loss: 0.019796048769274273\n",
      "FOLD: 2, EPOCH: 11, valid_loss: 0.016586940377377547\n",
      "FOLD: 2, EPOCH: 12, train_loss: 0.019648747216608073\n",
      "FOLD: 2, EPOCH: 12, valid_loss: 0.016438209595015414\n",
      "FOLD: 2, EPOCH: 13, train_loss: 0.019524060743483337\n",
      "FOLD: 2, EPOCH: 13, valid_loss: 0.01634166597460325\n",
      "FOLD: 2, EPOCH: 14, train_loss: 0.0194040130119066\n",
      "FOLD: 2, EPOCH: 14, valid_loss: 0.01629370078444481\n",
      "FOLD: 2, EPOCH: 15, train_loss: 0.019318263272981386\n",
      "FOLD: 2, EPOCH: 15, valid_loss: 0.016275126487016678\n",
      "FOLD: 2, EPOCH: 16, train_loss: 0.019184537584314477\n",
      "FOLD: 2, EPOCH: 16, valid_loss: 0.01617843722208188\n",
      "FOLD: 2, EPOCH: 17, train_loss: 0.01903973685929904\n",
      "FOLD: 2, EPOCH: 17, valid_loss: 0.01606494073684399\n",
      "FOLD: 2, EPOCH: 18, train_loss: 0.01895243795336904\n",
      "FOLD: 2, EPOCH: 18, valid_loss: 0.016035783104598522\n",
      "FOLD: 2, EPOCH: 19, train_loss: 0.01886032028375445\n",
      "FOLD: 2, EPOCH: 19, valid_loss: 0.015991154628304336\n",
      "FOLD: 2, EPOCH: 20, train_loss: 0.018736680894083268\n",
      "FOLD: 2, EPOCH: 20, valid_loss: 0.015983219759968612\n",
      "FOLD: 2, EPOCH: 21, train_loss: 0.018649807048810495\n",
      "FOLD: 2, EPOCH: 21, valid_loss: 0.015939199079114657\n",
      "FOLD: 2, EPOCH: 22, train_loss: 0.01853579990062359\n",
      "FOLD: 2, EPOCH: 22, valid_loss: 0.015921759490783397\n",
      "FOLD: 2, EPOCH: 23, train_loss: 0.01842370631827696\n",
      "FOLD: 2, EPOCH: 23, valid_loss: 0.015872094708566483\n",
      "FOLD: 2, EPOCH: 24, train_loss: 0.018327022424420795\n",
      "FOLD: 2, EPOCH: 24, valid_loss: 0.015830902096170645\n",
      "FOLD: 2, EPOCH: 25, train_loss: 0.018243858515209443\n",
      "FOLD: 2, EPOCH: 25, valid_loss: 0.015834274415213328\n",
      "FOLD: 2, EPOCH: 26, train_loss: 0.018165331157679494\n",
      "FOLD: 2, EPOCH: 26, valid_loss: 0.015808687831919927\n",
      "FOLD: 2, EPOCH: 27, train_loss: 0.018067398712643096\n",
      "FOLD: 2, EPOCH: 27, valid_loss: 0.015767232586558048\n",
      "FOLD: 2, EPOCH: 28, train_loss: 0.017985831744767523\n",
      "FOLD: 2, EPOCH: 28, valid_loss: 0.015817160789783184\n",
      "FOLD: 2, EPOCH: 29, train_loss: 0.017845769463157333\n",
      "FOLD: 2, EPOCH: 29, valid_loss: 0.015806320051734265\n",
      "FOLD: 2, EPOCH: 30, train_loss: 0.01775378258143728\n",
      "FOLD: 2, EPOCH: 30, valid_loss: 0.015762024845641393\n",
      "FOLD: 2, EPOCH: 31, train_loss: 0.01769378801455369\n",
      "FOLD: 2, EPOCH: 31, valid_loss: 0.015747751037661847\n",
      "FOLD: 2, EPOCH: 32, train_loss: 0.017521357591691857\n",
      "FOLD: 2, EPOCH: 32, valid_loss: 0.01575126900122716\n",
      "FOLD: 2, EPOCH: 33, train_loss: 0.017462436185293907\n",
      "FOLD: 2, EPOCH: 33, valid_loss: 0.015702876405647166\n",
      "FOLD: 2, EPOCH: 34, train_loss: 0.017402868138978612\n",
      "FOLD: 2, EPOCH: 34, valid_loss: 0.01569120936955397\n",
      "FOLD: 2, EPOCH: 35, train_loss: 0.017231094462142604\n",
      "FOLD: 2, EPOCH: 35, valid_loss: 0.015683566506665487\n",
      "FOLD: 2, EPOCH: 36, train_loss: 0.017176960569781227\n",
      "FOLD: 2, EPOCH: 36, valid_loss: 0.015696063781013854\n",
      "FOLD: 2, EPOCH: 37, train_loss: 0.017086870276142616\n",
      "FOLD: 2, EPOCH: 37, valid_loss: 0.015690106683625624\n",
      "FOLD: 2, EPOCH: 38, train_loss: 0.016996653310048418\n",
      "FOLD: 2, EPOCH: 38, valid_loss: 0.01571140789355223\n",
      "FOLD: 2, EPOCH: 39, train_loss: 0.016887371793288638\n",
      "FOLD: 2, EPOCH: 39, valid_loss: 0.015689346581124343\n",
      "FOLD: 2, EPOCH: 40, train_loss: 0.016775350501710497\n",
      "FOLD: 2, EPOCH: 40, valid_loss: 0.01567388132501107\n",
      "FOLD: 2, EPOCH: 41, train_loss: 0.01669261495054171\n",
      "FOLD: 2, EPOCH: 41, valid_loss: 0.015696053536465533\n",
      "FOLD: 2, EPOCH: 42, train_loss: 0.01656087990095084\n",
      "FOLD: 2, EPOCH: 42, valid_loss: 0.015694116600430928\n",
      "FOLD: 2, EPOCH: 43, train_loss: 0.01652414144041973\n",
      "FOLD: 2, EPOCH: 43, valid_loss: 0.01569288675315105\n",
      "FOLD: 2, EPOCH: 44, train_loss: 0.016363616051102006\n",
      "FOLD: 2, EPOCH: 44, valid_loss: 0.015677262455798112\n",
      "FOLD: 2, EPOCH: 45, train_loss: 0.01625713164842612\n",
      "FOLD: 2, EPOCH: 45, valid_loss: 0.015708760644953985\n",
      "FOLD: 2, EPOCH: 46, train_loss: 0.0161928312374732\n",
      "FOLD: 2, EPOCH: 46, valid_loss: 0.015674401791049883\n",
      "FOLD: 2, EPOCH: 47, train_loss: 0.016083757393062115\n",
      "FOLD: 2, EPOCH: 47, valid_loss: 0.015704787336289883\n",
      "Epoch    48: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 2, EPOCH: 48, train_loss: 0.01591617985301324\n",
      "FOLD: 2, EPOCH: 48, valid_loss: 0.015689426459945165\n",
      "FOLD: 2, EPOCH: 49, train_loss: 0.015758438425994403\n",
      "FOLD: 2, EPOCH: 49, valid_loss: 0.0156975627088776\n",
      "FOLD: 3, EPOCH: 0, train_loss: 0.04625718597624753\n",
      "FOLD: 3, EPOCH: 0, valid_loss: 0.020356024973667584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 3, EPOCH: 1, train_loss: 0.02290427133541655\n",
      "FOLD: 3, EPOCH: 1, valid_loss: 0.019068769824046355\n",
      "FOLD: 3, EPOCH: 2, train_loss: 0.021943967084626894\n",
      "FOLD: 3, EPOCH: 2, valid_loss: 0.018426450399252083\n",
      "FOLD: 3, EPOCH: 3, train_loss: 0.02140054270323064\n",
      "FOLD: 3, EPOCH: 3, valid_loss: 0.01790754869580269\n",
      "FOLD: 3, EPOCH: 4, train_loss: 0.021064626153659175\n",
      "FOLD: 3, EPOCH: 4, valid_loss: 0.0176787397895868\n",
      "FOLD: 3, EPOCH: 5, train_loss: 0.020819522062870296\n",
      "FOLD: 3, EPOCH: 5, valid_loss: 0.017347340257121965\n",
      "FOLD: 3, EPOCH: 6, train_loss: 0.02058813693253575\n",
      "FOLD: 3, EPOCH: 6, valid_loss: 0.017136017935207255\n",
      "FOLD: 3, EPOCH: 7, train_loss: 0.02036995000231105\n",
      "FOLD: 3, EPOCH: 7, valid_loss: 0.017062071066063184\n",
      "FOLD: 3, EPOCH: 8, train_loss: 0.020227911655564566\n",
      "FOLD: 3, EPOCH: 8, valid_loss: 0.016954462258861616\n",
      "FOLD: 3, EPOCH: 9, train_loss: 0.020035974756890052\n",
      "FOLD: 3, EPOCH: 9, valid_loss: 0.016828317553378068\n",
      "FOLD: 3, EPOCH: 10, train_loss: 0.0199164384220903\n",
      "FOLD: 3, EPOCH: 10, valid_loss: 0.016739976664002124\n",
      "FOLD: 3, EPOCH: 11, train_loss: 0.019806160314663035\n",
      "FOLD: 3, EPOCH: 11, valid_loss: 0.01673898143837085\n",
      "FOLD: 3, EPOCH: 12, train_loss: 0.019700573530752916\n",
      "FOLD: 3, EPOCH: 12, valid_loss: 0.016590058660277955\n",
      "FOLD: 3, EPOCH: 13, train_loss: 0.01954687510088489\n",
      "FOLD: 3, EPOCH: 13, valid_loss: 0.016536690294742584\n",
      "FOLD: 3, EPOCH: 14, train_loss: 0.0193931843229645\n",
      "FOLD: 3, EPOCH: 14, valid_loss: 0.016459650718248807\n",
      "FOLD: 3, EPOCH: 15, train_loss: 0.019324209264202696\n",
      "FOLD: 3, EPOCH: 15, valid_loss: 0.016414351761341095\n",
      "FOLD: 3, EPOCH: 16, train_loss: 0.019220098878282146\n",
      "FOLD: 3, EPOCH: 16, valid_loss: 0.01631779381288932\n",
      "FOLD: 3, EPOCH: 17, train_loss: 0.019067187718040234\n",
      "FOLD: 3, EPOCH: 17, valid_loss: 0.016339937654825356\n",
      "FOLD: 3, EPOCH: 18, train_loss: 0.01894683433646286\n",
      "FOLD: 3, EPOCH: 18, valid_loss: 0.016247691300052863\n",
      "FOLD: 3, EPOCH: 19, train_loss: 0.018888439681078936\n",
      "FOLD: 3, EPOCH: 19, valid_loss: 0.016248848360891525\n",
      "FOLD: 3, EPOCH: 20, train_loss: 0.018759148346411215\n",
      "FOLD: 3, EPOCH: 20, valid_loss: 0.016196840371076878\n",
      "FOLD: 3, EPOCH: 21, train_loss: 0.018666398384281108\n",
      "FOLD: 3, EPOCH: 21, valid_loss: 0.016199257511358995\n",
      "FOLD: 3, EPOCH: 22, train_loss: 0.018546114071599534\n",
      "FOLD: 3, EPOCH: 22, valid_loss: 0.01613084885936517\n",
      "FOLD: 3, EPOCH: 23, train_loss: 0.01846834669846135\n",
      "FOLD: 3, EPOCH: 23, valid_loss: 0.01612925106802812\n",
      "FOLD: 3, EPOCH: 24, train_loss: 0.018363978990630525\n",
      "FOLD: 3, EPOCH: 24, valid_loss: 0.016118437338333864\n",
      "FOLD: 3, EPOCH: 25, train_loss: 0.018255716038716806\n",
      "FOLD: 3, EPOCH: 25, valid_loss: 0.016060731946848907\n",
      "FOLD: 3, EPOCH: 26, train_loss: 0.018158146582946583\n",
      "FOLD: 3, EPOCH: 26, valid_loss: 0.016048041458886404\n",
      "FOLD: 3, EPOCH: 27, train_loss: 0.018064349109457957\n",
      "FOLD: 3, EPOCH: 27, valid_loss: 0.016009347083476875\n",
      "FOLD: 3, EPOCH: 28, train_loss: 0.01792820470055213\n",
      "FOLD: 3, EPOCH: 28, valid_loss: 0.016029399962952502\n",
      "FOLD: 3, EPOCH: 29, train_loss: 0.017874869203346\n",
      "FOLD: 3, EPOCH: 29, valid_loss: 0.015964895988313053\n",
      "FOLD: 3, EPOCH: 30, train_loss: 0.017774006779733544\n",
      "FOLD: 3, EPOCH: 30, valid_loss: 0.015948939352081373\n",
      "FOLD: 3, EPOCH: 31, train_loss: 0.017658632601032388\n",
      "FOLD: 3, EPOCH: 31, valid_loss: 0.01594867528631137\n",
      "FOLD: 3, EPOCH: 32, train_loss: 0.017535369490852225\n",
      "FOLD: 3, EPOCH: 32, valid_loss: 0.01596059248997615\n",
      "FOLD: 3, EPOCH: 33, train_loss: 0.017471114462996658\n",
      "FOLD: 3, EPOCH: 33, valid_loss: 0.01591708790510893\n",
      "FOLD: 3, EPOCH: 34, train_loss: 0.017347873528362125\n",
      "FOLD: 3, EPOCH: 34, valid_loss: 0.01593758989698612\n",
      "FOLD: 3, EPOCH: 35, train_loss: 0.017225013190024608\n",
      "FOLD: 3, EPOCH: 35, valid_loss: 0.015927213244140148\n",
      "FOLD: 3, EPOCH: 36, train_loss: 0.017214091872242657\n",
      "FOLD: 3, EPOCH: 36, valid_loss: 0.015919983100432616\n",
      "FOLD: 3, EPOCH: 37, train_loss: 0.01705615401167322\n",
      "FOLD: 3, EPOCH: 37, valid_loss: 0.01589536265685008\n",
      "FOLD: 3, EPOCH: 38, train_loss: 0.016967974552834355\n",
      "FOLD: 3, EPOCH: 38, valid_loss: 0.015920599492696617\n",
      "FOLD: 3, EPOCH: 39, train_loss: 0.016870361287146807\n",
      "FOLD: 3, EPOCH: 39, valid_loss: 0.015945076369322263\n",
      "FOLD: 3, EPOCH: 40, train_loss: 0.01679498765214875\n",
      "FOLD: 3, EPOCH: 40, valid_loss: 0.0159198553659595\n",
      "FOLD: 3, EPOCH: 41, train_loss: 0.01664949641437144\n",
      "FOLD: 3, EPOCH: 41, valid_loss: 0.015899484260724142\n",
      "FOLD: 3, EPOCH: 42, train_loss: 0.016598948252362175\n",
      "FOLD: 3, EPOCH: 42, valid_loss: 0.015918529950655423\n",
      "FOLD: 3, EPOCH: 43, train_loss: 0.016485010663903243\n",
      "FOLD: 3, EPOCH: 43, valid_loss: 0.015920263858368762\n",
      "FOLD: 3, EPOCH: 44, train_loss: 0.016344421958500468\n",
      "FOLD: 3, EPOCH: 44, valid_loss: 0.01591649021093662\n",
      "Epoch    45: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 3, EPOCH: 45, train_loss: 0.01617286291376159\n",
      "FOLD: 3, EPOCH: 45, valid_loss: 0.015905712086420793\n",
      "FOLD: 3, EPOCH: 46, train_loss: 0.016057430212763516\n",
      "FOLD: 3, EPOCH: 46, valid_loss: 0.015887829331824414\n",
      "FOLD: 3, EPOCH: 47, train_loss: 0.015944560788370466\n",
      "FOLD: 3, EPOCH: 47, valid_loss: 0.01590271910222677\n",
      "FOLD: 3, EPOCH: 48, train_loss: 0.01587878806969604\n",
      "FOLD: 3, EPOCH: 48, valid_loss: 0.015872186193099387\n",
      "FOLD: 3, EPOCH: 49, train_loss: 0.01574353695016455\n",
      "FOLD: 3, EPOCH: 49, valid_loss: 0.01588848175910803\n",
      "FOLD: 4, EPOCH: 0, train_loss: 0.044811551509475384\n",
      "FOLD: 4, EPOCH: 0, valid_loss: 0.020423342688725546\n",
      "FOLD: 4, EPOCH: 1, train_loss: 0.02280397428753408\n",
      "FOLD: 4, EPOCH: 1, valid_loss: 0.019429139649638764\n",
      "FOLD: 4, EPOCH: 2, train_loss: 0.02192309863764692\n",
      "FOLD: 4, EPOCH: 2, valid_loss: 0.018556886710799657\n",
      "FOLD: 4, EPOCH: 3, train_loss: 0.02141998861790509\n",
      "FOLD: 4, EPOCH: 3, valid_loss: 0.01800475040307412\n",
      "FOLD: 4, EPOCH: 4, train_loss: 0.02105645983907822\n",
      "FOLD: 4, EPOCH: 4, valid_loss: 0.0177370784087823\n",
      "FOLD: 4, EPOCH: 5, train_loss: 0.020786878652870655\n",
      "FOLD: 4, EPOCH: 5, valid_loss: 0.01754715580206651\n",
      "FOLD: 4, EPOCH: 6, train_loss: 0.020587844748956127\n",
      "FOLD: 4, EPOCH: 6, valid_loss: 0.017330813149993237\n",
      "FOLD: 4, EPOCH: 7, train_loss: 0.020369486933624423\n",
      "FOLD: 4, EPOCH: 7, valid_loss: 0.01714598287183505\n",
      "FOLD: 4, EPOCH: 8, train_loss: 0.020205784110805473\n",
      "FOLD: 4, EPOCH: 8, valid_loss: 0.017044087155507162\n",
      "FOLD: 4, EPOCH: 9, train_loss: 0.02002639328507153\n",
      "FOLD: 4, EPOCH: 9, valid_loss: 0.016984784689087134\n",
      "FOLD: 4, EPOCH: 10, train_loss: 0.019903626746019802\n",
      "FOLD: 4, EPOCH: 10, valid_loss: 0.016846764976015456\n",
      "FOLD: 4, EPOCH: 11, train_loss: 0.019759735696621844\n",
      "FOLD: 4, EPOCH: 11, valid_loss: 0.016763413754793312\n",
      "FOLD: 4, EPOCH: 12, train_loss: 0.019667403797644215\n",
      "FOLD: 4, EPOCH: 12, valid_loss: 0.016674345144285604\n",
      "FOLD: 4, EPOCH: 13, train_loss: 0.019495913356139854\n",
      "FOLD: 4, EPOCH: 13, valid_loss: 0.01659727583710964\n",
      "FOLD: 4, EPOCH: 14, train_loss: 0.019411625667802385\n",
      "FOLD: 4, EPOCH: 14, valid_loss: 0.01648121062093056\n",
      "FOLD: 4, EPOCH: 15, train_loss: 0.019254191073815565\n",
      "FOLD: 4, EPOCH: 15, valid_loss: 0.016497303230258133\n",
      "FOLD: 4, EPOCH: 16, train_loss: 0.01912879037695962\n",
      "FOLD: 4, EPOCH: 16, valid_loss: 0.016417277618669547\n",
      "FOLD: 4, EPOCH: 17, train_loss: 0.01900987417714016\n",
      "FOLD: 4, EPOCH: 17, valid_loss: 0.016396274575247213\n",
      "FOLD: 4, EPOCH: 18, train_loss: 0.01892267816976921\n",
      "FOLD: 4, EPOCH: 18, valid_loss: 0.016313200458311118\n",
      "FOLD: 4, EPOCH: 19, train_loss: 0.018850398300265945\n",
      "FOLD: 4, EPOCH: 19, valid_loss: 0.016331612705611266\n",
      "FOLD: 4, EPOCH: 20, train_loss: 0.018686466145555716\n",
      "FOLD: 4, EPOCH: 20, valid_loss: 0.016280225334832303\n",
      "FOLD: 4, EPOCH: 21, train_loss: 0.018633797789949016\n",
      "FOLD: 4, EPOCH: 21, valid_loss: 0.016285718633578375\n",
      "FOLD: 4, EPOCH: 22, train_loss: 0.018496439011918532\n",
      "FOLD: 4, EPOCH: 22, valid_loss: 0.01617590209039358\n",
      "FOLD: 4, EPOCH: 23, train_loss: 0.018403751045666838\n",
      "FOLD: 4, EPOCH: 23, valid_loss: 0.016153146154605426\n",
      "FOLD: 4, EPOCH: 24, train_loss: 0.018291728218664993\n",
      "FOLD: 4, EPOCH: 24, valid_loss: 0.01615309450202263\n",
      "FOLD: 4, EPOCH: 25, train_loss: 0.018194562252107506\n",
      "FOLD: 4, EPOCH: 25, valid_loss: 0.016130333336500022\n",
      "FOLD: 4, EPOCH: 26, train_loss: 0.01808590749026956\n",
      "FOLD: 4, EPOCH: 26, valid_loss: 0.01608155918522523\n",
      "FOLD: 4, EPOCH: 27, train_loss: 0.01799149828887469\n",
      "FOLD: 4, EPOCH: 27, valid_loss: 0.016088399534615185\n",
      "FOLD: 4, EPOCH: 28, train_loss: 0.017912523116211634\n",
      "FOLD: 4, EPOCH: 28, valid_loss: 0.016060791694774076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 4, EPOCH: 29, train_loss: 0.017784973606467247\n",
      "FOLD: 4, EPOCH: 29, valid_loss: 0.016067381733312056\n",
      "FOLD: 4, EPOCH: 30, train_loss: 0.01768366025911795\n",
      "FOLD: 4, EPOCH: 30, valid_loss: 0.016076096118642733\n",
      "FOLD: 4, EPOCH: 31, train_loss: 0.01759505141023043\n",
      "FOLD: 4, EPOCH: 31, valid_loss: 0.016020184239515893\n",
      "FOLD: 4, EPOCH: 32, train_loss: 0.017471563800968027\n",
      "FOLD: 4, EPOCH: 32, valid_loss: 0.016031510196626186\n",
      "FOLD: 4, EPOCH: 33, train_loss: 0.017382890363601414\n",
      "FOLD: 4, EPOCH: 33, valid_loss: 0.01604658107344921\n",
      "FOLD: 4, EPOCH: 34, train_loss: 0.017311119046565647\n",
      "FOLD: 4, EPOCH: 34, valid_loss: 0.016029635730844278\n",
      "FOLD: 4, EPOCH: 35, train_loss: 0.017204406520200742\n",
      "FOLD: 4, EPOCH: 35, valid_loss: 0.015993928393492333\n",
      "FOLD: 4, EPOCH: 36, train_loss: 0.017050557052464905\n",
      "FOLD: 4, EPOCH: 36, valid_loss: 0.015994800111422174\n",
      "FOLD: 4, EPOCH: 37, train_loss: 0.016981840221765073\n",
      "FOLD: 4, EPOCH: 37, valid_loss: 0.01597937812598852\n",
      "FOLD: 4, EPOCH: 38, train_loss: 0.016903993069521478\n",
      "FOLD: 4, EPOCH: 38, valid_loss: 0.015989055283940755\n",
      "FOLD: 4, EPOCH: 39, train_loss: 0.01679804889686607\n",
      "FOLD: 4, EPOCH: 39, valid_loss: 0.015992754568847325\n",
      "FOLD: 4, EPOCH: 40, train_loss: 0.0166882634968371\n",
      "FOLD: 4, EPOCH: 40, valid_loss: 0.015969025615889292\n",
      "FOLD: 4, EPOCH: 41, train_loss: 0.01656287530990871\n",
      "FOLD: 4, EPOCH: 41, valid_loss: 0.01603088040764515\n",
      "FOLD: 4, EPOCH: 42, train_loss: 0.01652116094388672\n",
      "FOLD: 4, EPOCH: 42, valid_loss: 0.01599369728221343\n",
      "FOLD: 4, EPOCH: 43, train_loss: 0.01636429292124671\n",
      "FOLD: 4, EPOCH: 43, valid_loss: 0.015943485383804027\n",
      "FOLD: 4, EPOCH: 44, train_loss: 0.016294516887314415\n",
      "FOLD: 4, EPOCH: 44, valid_loss: 0.01597488263192085\n",
      "FOLD: 4, EPOCH: 45, train_loss: 0.01620757012200114\n",
      "FOLD: 4, EPOCH: 45, valid_loss: 0.01596665446861432\n",
      "FOLD: 4, EPOCH: 46, train_loss: 0.016084914095699787\n",
      "FOLD: 4, EPOCH: 46, valid_loss: 0.015990877165817298\n",
      "FOLD: 4, EPOCH: 47, train_loss: 0.016005676277485247\n",
      "FOLD: 4, EPOCH: 47, valid_loss: 0.015968650651092712\n",
      "FOLD: 4, EPOCH: 48, train_loss: 0.015935205111934525\n",
      "FOLD: 4, EPOCH: 48, valid_loss: 0.015973552774924498\n",
      "FOLD: 4, EPOCH: 49, train_loss: 0.0158078187094951\n",
      "FOLD: 4, EPOCH: 49, valid_loss: 0.015977466192383032\n",
      "FOLD: 5, EPOCH: 0, train_loss: 0.04329543802383784\n",
      "FOLD: 5, EPOCH: 0, valid_loss: 0.021108341188384935\n",
      "FOLD: 5, EPOCH: 1, train_loss: 0.0227586042538688\n",
      "FOLD: 5, EPOCH: 1, valid_loss: 0.01936315458554488\n",
      "FOLD: 5, EPOCH: 2, train_loss: 0.021826074534171336\n",
      "FOLD: 5, EPOCH: 2, valid_loss: 0.0183858722448349\n",
      "FOLD: 5, EPOCH: 3, train_loss: 0.021310465133472067\n",
      "FOLD: 5, EPOCH: 3, valid_loss: 0.017908466120178882\n",
      "FOLD: 5, EPOCH: 4, train_loss: 0.02099719920472519\n",
      "FOLD: 5, EPOCH: 4, valid_loss: 0.017569431748527747\n",
      "FOLD: 5, EPOCH: 5, train_loss: 0.020664218083225393\n",
      "FOLD: 5, EPOCH: 5, valid_loss: 0.017332955908316832\n",
      "FOLD: 5, EPOCH: 6, train_loss: 0.02046394846527963\n",
      "FOLD: 5, EPOCH: 6, valid_loss: 0.017136473925067827\n",
      "FOLD: 5, EPOCH: 7, train_loss: 0.020298790362839762\n",
      "FOLD: 5, EPOCH: 7, valid_loss: 0.01700846191782218\n",
      "FOLD: 5, EPOCH: 8, train_loss: 0.020153359812055086\n",
      "FOLD: 5, EPOCH: 8, valid_loss: 0.017019959309926398\n",
      "FOLD: 5, EPOCH: 9, train_loss: 0.019976773752352676\n",
      "FOLD: 5, EPOCH: 9, valid_loss: 0.016878533248717967\n",
      "FOLD: 5, EPOCH: 10, train_loss: 0.01985060463886003\n",
      "FOLD: 5, EPOCH: 10, valid_loss: 0.01672829995648219\n",
      "FOLD: 5, EPOCH: 11, train_loss: 0.019730575483393024\n",
      "FOLD: 5, EPOCH: 11, valid_loss: 0.016683407414418\n",
      "FOLD: 5, EPOCH: 12, train_loss: 0.019599913191553707\n",
      "FOLD: 5, EPOCH: 12, valid_loss: 0.01659872977492901\n",
      "FOLD: 5, EPOCH: 13, train_loss: 0.01942475625892749\n",
      "FOLD: 5, EPOCH: 13, valid_loss: 0.016527874753452264\n",
      "FOLD: 5, EPOCH: 14, train_loss: 0.01931519028604836\n",
      "FOLD: 5, EPOCH: 14, valid_loss: 0.016468001386294\n",
      "FOLD: 5, EPOCH: 15, train_loss: 0.01922385877854115\n",
      "FOLD: 5, EPOCH: 15, valid_loss: 0.01641783261528382\n",
      "FOLD: 5, EPOCH: 16, train_loss: 0.019124209981512378\n",
      "FOLD: 5, EPOCH: 16, valid_loss: 0.01630878928475655\n",
      "FOLD: 5, EPOCH: 17, train_loss: 0.01898166167272909\n",
      "FOLD: 5, EPOCH: 17, valid_loss: 0.0162967755101048\n",
      "FOLD: 5, EPOCH: 18, train_loss: 0.018889913461296946\n",
      "FOLD: 5, EPOCH: 18, valid_loss: 0.016196580317157965\n",
      "FOLD: 5, EPOCH: 19, train_loss: 0.01881234355371546\n",
      "FOLD: 5, EPOCH: 19, valid_loss: 0.016151028183790352\n",
      "FOLD: 5, EPOCH: 20, train_loss: 0.01865489108840356\n",
      "FOLD: 5, EPOCH: 20, valid_loss: 0.01610743010846468\n",
      "FOLD: 5, EPOCH: 21, train_loss: 0.018595554387649975\n",
      "FOLD: 5, EPOCH: 21, valid_loss: 0.016131446625177678\n",
      "FOLD: 5, EPOCH: 22, train_loss: 0.018488718045724405\n",
      "FOLD: 5, EPOCH: 22, valid_loss: 0.01606296060176996\n",
      "FOLD: 5, EPOCH: 23, train_loss: 0.018379861564450973\n",
      "FOLD: 5, EPOCH: 23, valid_loss: 0.01604233567531292\n",
      "FOLD: 5, EPOCH: 24, train_loss: 0.01829005477043825\n",
      "FOLD: 5, EPOCH: 24, valid_loss: 0.016036653891205788\n",
      "FOLD: 5, EPOCH: 25, train_loss: 0.018165072350687272\n",
      "FOLD: 5, EPOCH: 25, valid_loss: 0.016039846321711175\n",
      "FOLD: 5, EPOCH: 26, train_loss: 0.01803370884846191\n",
      "FOLD: 5, EPOCH: 26, valid_loss: 0.015948983339162972\n",
      "FOLD: 5, EPOCH: 27, train_loss: 0.017955242379291636\n",
      "FOLD: 5, EPOCH: 27, valid_loss: 0.015976432495965406\n",
      "FOLD: 5, EPOCH: 28, train_loss: 0.017869721267473052\n",
      "FOLD: 5, EPOCH: 28, valid_loss: 0.01594785163895442\n",
      "FOLD: 5, EPOCH: 29, train_loss: 0.017803710155390406\n",
      "FOLD: 5, EPOCH: 29, valid_loss: 0.015900859107764866\n",
      "FOLD: 5, EPOCH: 30, train_loss: 0.017673063091933727\n",
      "FOLD: 5, EPOCH: 30, valid_loss: 0.015902816461255916\n",
      "FOLD: 5, EPOCH: 31, train_loss: 0.017598162203825807\n",
      "FOLD: 5, EPOCH: 31, valid_loss: 0.015893891023901794\n",
      "FOLD: 5, EPOCH: 32, train_loss: 0.017429988478889335\n",
      "FOLD: 5, EPOCH: 32, valid_loss: 0.015896556755671136\n",
      "FOLD: 5, EPOCH: 33, train_loss: 0.017307182043992186\n",
      "FOLD: 5, EPOCH: 33, valid_loss: 0.015881458010811072\n",
      "FOLD: 5, EPOCH: 34, train_loss: 0.017286083749117883\n",
      "FOLD: 5, EPOCH: 34, valid_loss: 0.015878864134160373\n",
      "FOLD: 5, EPOCH: 35, train_loss: 0.01712376590365091\n",
      "FOLD: 5, EPOCH: 35, valid_loss: 0.015862458743728124\n",
      "FOLD: 5, EPOCH: 36, train_loss: 0.01708093738636455\n",
      "FOLD: 5, EPOCH: 36, valid_loss: 0.01587054756684945\n",
      "FOLD: 5, EPOCH: 37, train_loss: 0.01696853103065813\n",
      "FOLD: 5, EPOCH: 37, valid_loss: 0.01586477795185951\n",
      "FOLD: 5, EPOCH: 38, train_loss: 0.016825992362322035\n",
      "FOLD: 5, EPOCH: 38, valid_loss: 0.015904827115054313\n",
      "FOLD: 5, EPOCH: 39, train_loss: 0.016756507618403114\n",
      "FOLD: 5, EPOCH: 39, valid_loss: 0.01587424799799919\n",
      "FOLD: 5, EPOCH: 40, train_loss: 0.016650820868341503\n",
      "FOLD: 5, EPOCH: 40, valid_loss: 0.015864551855394475\n",
      "FOLD: 5, EPOCH: 41, train_loss: 0.016584439505193685\n",
      "FOLD: 5, EPOCH: 41, valid_loss: 0.015878862629716214\n",
      "FOLD: 5, EPOCH: 42, train_loss: 0.016431362154213962\n",
      "FOLD: 5, EPOCH: 42, valid_loss: 0.01585353996891242\n",
      "FOLD: 5, EPOCH: 43, train_loss: 0.016359318953913612\n",
      "FOLD: 5, EPOCH: 43, valid_loss: 0.015885418495879725\n",
      "FOLD: 5, EPOCH: 44, train_loss: 0.016233338332558807\n",
      "FOLD: 5, EPOCH: 44, valid_loss: 0.015850818930910185\n",
      "FOLD: 5, EPOCH: 45, train_loss: 0.016154848404128005\n",
      "FOLD: 5, EPOCH: 45, valid_loss: 0.015861348607219182\n",
      "FOLD: 5, EPOCH: 46, train_loss: 0.0160489401754898\n",
      "FOLD: 5, EPOCH: 46, valid_loss: 0.015847772503128417\n",
      "FOLD: 5, EPOCH: 47, train_loss: 0.015904360375291592\n",
      "FOLD: 5, EPOCH: 47, valid_loss: 0.01587032490911392\n",
      "FOLD: 5, EPOCH: 48, train_loss: 0.015748776873019902\n",
      "FOLD: 5, EPOCH: 48, valid_loss: 0.01585554997794903\n",
      "FOLD: 5, EPOCH: 49, train_loss: 0.015717310491143853\n",
      "FOLD: 5, EPOCH: 49, valid_loss: 0.015888154435043152\n",
      "FOLD: 6, EPOCH: 0, train_loss: 0.04424444432496219\n",
      "FOLD: 6, EPOCH: 0, valid_loss: 0.0201920080356873\n",
      "FOLD: 6, EPOCH: 1, train_loss: 0.022812852837346697\n",
      "FOLD: 6, EPOCH: 1, valid_loss: 0.018847803083749917\n",
      "FOLD: 6, EPOCH: 2, train_loss: 0.021840281155262445\n",
      "FOLD: 6, EPOCH: 2, valid_loss: 0.01796443867855347\n",
      "FOLD: 6, EPOCH: 3, train_loss: 0.02133524780337875\n",
      "FOLD: 6, EPOCH: 3, valid_loss: 0.017840874023162402\n",
      "FOLD: 6, EPOCH: 4, train_loss: 0.020985418930649757\n",
      "FOLD: 6, EPOCH: 4, valid_loss: 0.01732030840447316\n",
      "FOLD: 6, EPOCH: 5, train_loss: 0.02077122815456745\n",
      "FOLD: 6, EPOCH: 5, valid_loss: 0.017175234352739956\n",
      "FOLD: 6, EPOCH: 6, train_loss: 0.020529464843708115\n",
      "FOLD: 6, EPOCH: 6, valid_loss: 0.016989576128812935\n",
      "FOLD: 6, EPOCH: 7, train_loss: 0.020348418252290907\n",
      "FOLD: 6, EPOCH: 7, valid_loss: 0.016896324088940255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 6, EPOCH: 8, train_loss: 0.02018201001290534\n",
      "FOLD: 6, EPOCH: 8, valid_loss: 0.016760201050111882\n",
      "FOLD: 6, EPOCH: 9, train_loss: 0.020082400532791745\n",
      "FOLD: 6, EPOCH: 9, valid_loss: 0.016627704366468467\n",
      "FOLD: 6, EPOCH: 10, train_loss: 0.01988331988655232\n",
      "FOLD: 6, EPOCH: 10, valid_loss: 0.016569219887829743\n",
      "FOLD: 6, EPOCH: 11, train_loss: 0.01977594551705831\n",
      "FOLD: 6, EPOCH: 11, valid_loss: 0.016490184487058565\n",
      "FOLD: 6, EPOCH: 12, train_loss: 0.019616418090221042\n",
      "FOLD: 6, EPOCH: 12, valid_loss: 0.016341122440420665\n",
      "FOLD: 6, EPOCH: 13, train_loss: 0.01949381788034697\n",
      "FOLD: 6, EPOCH: 13, valid_loss: 0.016294332006229803\n",
      "FOLD: 6, EPOCH: 14, train_loss: 0.019405210768250195\n",
      "FOLD: 6, EPOCH: 14, valid_loss: 0.0162348819610018\n",
      "FOLD: 6, EPOCH: 15, train_loss: 0.0192718199026343\n",
      "FOLD: 6, EPOCH: 15, valid_loss: 0.016224384451141723\n",
      "FOLD: 6, EPOCH: 16, train_loss: 0.019154530245105963\n",
      "FOLD: 6, EPOCH: 16, valid_loss: 0.01614917929355915\n",
      "FOLD: 6, EPOCH: 17, train_loss: 0.01904726433693557\n",
      "FOLD: 6, EPOCH: 17, valid_loss: 0.016121429606125906\n",
      "FOLD: 6, EPOCH: 18, train_loss: 0.01893907319754362\n",
      "FOLD: 6, EPOCH: 18, valid_loss: 0.01604808960109949\n",
      "FOLD: 6, EPOCH: 19, train_loss: 0.018810569761774025\n",
      "FOLD: 6, EPOCH: 19, valid_loss: 0.015994695516733024\n",
      "FOLD: 6, EPOCH: 20, train_loss: 0.018745938089449663\n",
      "FOLD: 6, EPOCH: 20, valid_loss: 0.015971093223645136\n",
      "FOLD: 6, EPOCH: 21, train_loss: 0.01863899527470002\n",
      "FOLD: 6, EPOCH: 21, valid_loss: 0.015918700812527768\n",
      "FOLD: 6, EPOCH: 22, train_loss: 0.01851268930713067\n",
      "FOLD: 6, EPOCH: 22, valid_loss: 0.01590705383569002\n",
      "FOLD: 6, EPOCH: 23, train_loss: 0.018415815901716013\n",
      "FOLD: 6, EPOCH: 23, valid_loss: 0.01593624298962263\n",
      "FOLD: 6, EPOCH: 24, train_loss: 0.01827639011615837\n",
      "FOLD: 6, EPOCH: 24, valid_loss: 0.015837900053996306\n",
      "FOLD: 6, EPOCH: 25, train_loss: 0.01820420947026562\n",
      "FOLD: 6, EPOCH: 25, valid_loss: 0.015820766584231302\n",
      "FOLD: 6, EPOCH: 26, train_loss: 0.018117704428732395\n",
      "FOLD: 6, EPOCH: 26, valid_loss: 0.015786488325550005\n",
      "FOLD: 6, EPOCH: 27, train_loss: 0.01797465469084076\n",
      "FOLD: 6, EPOCH: 27, valid_loss: 0.015783097523336228\n",
      "FOLD: 6, EPOCH: 28, train_loss: 0.017885032172843412\n",
      "FOLD: 6, EPOCH: 28, valid_loss: 0.015799490090173025\n",
      "FOLD: 6, EPOCH: 29, train_loss: 0.017756694499906654\n",
      "FOLD: 6, EPOCH: 29, valid_loss: 0.01575237842133412\n",
      "FOLD: 6, EPOCH: 30, train_loss: 0.01767101534013007\n",
      "FOLD: 6, EPOCH: 30, valid_loss: 0.01570776907297281\n",
      "FOLD: 6, EPOCH: 31, train_loss: 0.017596745485993655\n",
      "FOLD: 6, EPOCH: 31, valid_loss: 0.01567921694368124\n",
      "FOLD: 6, EPOCH: 32, train_loss: 0.01752797529302739\n",
      "FOLD: 6, EPOCH: 32, valid_loss: 0.01568505597802309\n",
      "FOLD: 6, EPOCH: 33, train_loss: 0.017422534739346924\n",
      "FOLD: 6, EPOCH: 33, valid_loss: 0.015707198387155168\n",
      "FOLD: 6, EPOCH: 34, train_loss: 0.017311309489446716\n",
      "FOLD: 6, EPOCH: 34, valid_loss: 0.015695854233434565\n",
      "FOLD: 6, EPOCH: 35, train_loss: 0.017182949590622575\n",
      "FOLD: 6, EPOCH: 35, valid_loss: 0.015713271398383837\n",
      "FOLD: 6, EPOCH: 36, train_loss: 0.017089676867062982\n",
      "FOLD: 6, EPOCH: 36, valid_loss: 0.01568678544404415\n",
      "FOLD: 6, EPOCH: 37, train_loss: 0.01700719308762534\n",
      "FOLD: 6, EPOCH: 37, valid_loss: 0.015661970497323915\n",
      "FOLD: 6, EPOCH: 38, train_loss: 0.01689525558990804\n",
      "FOLD: 6, EPOCH: 38, valid_loss: 0.015653299884154245\n",
      "FOLD: 6, EPOCH: 39, train_loss: 0.01677712916421729\n",
      "FOLD: 6, EPOCH: 39, valid_loss: 0.01567406257471213\n",
      "FOLD: 6, EPOCH: 40, train_loss: 0.01667704863624798\n",
      "FOLD: 6, EPOCH: 40, valid_loss: 0.015663797322374124\n",
      "FOLD: 6, EPOCH: 41, train_loss: 0.016604357034068654\n",
      "FOLD: 6, EPOCH: 41, valid_loss: 0.01566565975260276\n",
      "FOLD: 6, EPOCH: 42, train_loss: 0.01648229082442216\n",
      "FOLD: 6, EPOCH: 42, valid_loss: 0.015665698008468516\n",
      "FOLD: 6, EPOCH: 43, train_loss: 0.0164127233186485\n",
      "FOLD: 6, EPOCH: 43, valid_loss: 0.015651628661614198\n",
      "FOLD: 6, EPOCH: 44, train_loss: 0.01626476868231957\n",
      "FOLD: 6, EPOCH: 44, valid_loss: 0.015693561532176457\n",
      "FOLD: 6, EPOCH: 45, train_loss: 0.016200818587094545\n",
      "FOLD: 6, EPOCH: 45, valid_loss: 0.015695737244991157\n",
      "FOLD: 6, EPOCH: 46, train_loss: 0.016110765219137475\n",
      "FOLD: 6, EPOCH: 46, valid_loss: 0.01572666231256265\n",
      "FOLD: 6, EPOCH: 47, train_loss: 0.015992260689066874\n",
      "FOLD: 6, EPOCH: 47, valid_loss: 0.015712723709069766\n",
      "FOLD: 6, EPOCH: 48, train_loss: 0.01592840166208712\n",
      "FOLD: 6, EPOCH: 48, valid_loss: 0.015702895175379056\n",
      "FOLD: 6, EPOCH: 49, train_loss: 0.015832842113701878\n",
      "FOLD: 6, EPOCH: 49, valid_loss: 0.015737117268145084\n",
      "FOLD: 0, EPOCH: 0, train_loss: 0.04215368839937288\n",
      "FOLD: 0, EPOCH: 0, valid_loss: 0.020071170937556487\n",
      "FOLD: 0, EPOCH: 1, train_loss: 0.022652731931491477\n",
      "FOLD: 0, EPOCH: 1, valid_loss: 0.01875569628408322\n",
      "FOLD: 0, EPOCH: 2, train_loss: 0.021802733325072238\n",
      "FOLD: 0, EPOCH: 2, valid_loss: 0.018331446040135164\n",
      "FOLD: 0, EPOCH: 3, train_loss: 0.021306153990932414\n",
      "FOLD: 0, EPOCH: 3, valid_loss: 0.017727884034124706\n",
      "FOLD: 0, EPOCH: 4, train_loss: 0.020924728942682613\n",
      "FOLD: 0, EPOCH: 4, valid_loss: 0.017464124597609043\n",
      "FOLD: 0, EPOCH: 5, train_loss: 0.02070491760969162\n",
      "FOLD: 0, EPOCH: 5, valid_loss: 0.01727119467865962\n",
      "FOLD: 0, EPOCH: 6, train_loss: 0.02049042347415879\n",
      "FOLD: 0, EPOCH: 6, valid_loss: 0.017127965576946735\n",
      "FOLD: 0, EPOCH: 7, train_loss: 0.020259947742562037\n",
      "FOLD: 0, EPOCH: 7, valid_loss: 0.01699926252835072\n",
      "FOLD: 0, EPOCH: 8, train_loss: 0.020166666196608864\n",
      "FOLD: 0, EPOCH: 8, valid_loss: 0.016909699743756883\n",
      "FOLD: 0, EPOCH: 9, train_loss: 0.019976170758741932\n",
      "FOLD: 0, EPOCH: 9, valid_loss: 0.016783660635925256\n",
      "FOLD: 0, EPOCH: 10, train_loss: 0.019843582844210637\n",
      "FOLD: 0, EPOCH: 10, valid_loss: 0.016714946223566167\n",
      "FOLD: 0, EPOCH: 11, train_loss: 0.019741593658722734\n",
      "FOLD: 0, EPOCH: 11, valid_loss: 0.01660791591096383\n",
      "FOLD: 0, EPOCH: 12, train_loss: 0.019558355791141856\n",
      "FOLD: 0, EPOCH: 12, valid_loss: 0.01648714250096908\n",
      "FOLD: 0, EPOCH: 13, train_loss: 0.01945265694647222\n",
      "FOLD: 0, EPOCH: 13, valid_loss: 0.0163750250895436\n",
      "FOLD: 0, EPOCH: 14, train_loss: 0.019352892237539228\n",
      "FOLD: 0, EPOCH: 14, valid_loss: 0.01636354338664275\n",
      "FOLD: 0, EPOCH: 15, train_loss: 0.01917120049128661\n",
      "FOLD: 0, EPOCH: 15, valid_loss: 0.016278752913841836\n",
      "FOLD: 0, EPOCH: 16, train_loss: 0.019060798971032775\n",
      "FOLD: 0, EPOCH: 16, valid_loss: 0.016229818001962625\n",
      "FOLD: 0, EPOCH: 17, train_loss: 0.018916928964490827\n",
      "FOLD: 0, EPOCH: 17, valid_loss: 0.016120113862248566\n",
      "FOLD: 0, EPOCH: 18, train_loss: 0.01884936251853769\n",
      "FOLD: 0, EPOCH: 18, valid_loss: 0.016116616674340688\n",
      "FOLD: 0, EPOCH: 19, train_loss: 0.01871736111069048\n",
      "FOLD: 0, EPOCH: 19, valid_loss: 0.016070553459800206\n",
      "FOLD: 0, EPOCH: 20, train_loss: 0.018607996706221555\n",
      "FOLD: 0, EPOCH: 20, valid_loss: 0.016038223098103817\n",
      "FOLD: 0, EPOCH: 21, train_loss: 0.018502685316913837\n",
      "FOLD: 0, EPOCH: 21, valid_loss: 0.01601992046030668\n",
      "FOLD: 0, EPOCH: 22, train_loss: 0.018438123028121284\n",
      "FOLD: 0, EPOCH: 22, valid_loss: 0.01596790602287421\n",
      "FOLD: 0, EPOCH: 23, train_loss: 0.018304180655930494\n",
      "FOLD: 0, EPOCH: 23, valid_loss: 0.015945211411095582\n",
      "FOLD: 0, EPOCH: 24, train_loss: 0.018189212156308664\n",
      "FOLD: 0, EPOCH: 24, valid_loss: 0.01591555974804438\n",
      "FOLD: 0, EPOCH: 25, train_loss: 0.01807705284020788\n",
      "FOLD: 0, EPOCH: 25, valid_loss: 0.015907839871942997\n",
      "FOLD: 0, EPOCH: 26, train_loss: 0.017936201690620667\n",
      "FOLD: 0, EPOCH: 26, valid_loss: 0.015930076917776696\n",
      "FOLD: 0, EPOCH: 27, train_loss: 0.017853965680744197\n",
      "FOLD: 0, EPOCH: 27, valid_loss: 0.015878768136294987\n",
      "FOLD: 0, EPOCH: 28, train_loss: 0.017733523445959028\n",
      "FOLD: 0, EPOCH: 28, valid_loss: 0.015833126166119024\n",
      "FOLD: 0, EPOCH: 29, train_loss: 0.01770677271525602\n",
      "FOLD: 0, EPOCH: 29, valid_loss: 0.015868503385438368\n",
      "FOLD: 0, EPOCH: 30, train_loss: 0.017558559020226065\n",
      "FOLD: 0, EPOCH: 30, valid_loss: 0.01586349645199684\n",
      "FOLD: 0, EPOCH: 31, train_loss: 0.017463607147235324\n",
      "FOLD: 0, EPOCH: 31, valid_loss: 0.01586757593143445\n",
      "FOLD: 0, EPOCH: 32, train_loss: 0.017366765419373643\n",
      "FOLD: 0, EPOCH: 32, valid_loss: 0.015793946786568716\n",
      "FOLD: 0, EPOCH: 33, train_loss: 0.017239162824242503\n",
      "FOLD: 0, EPOCH: 33, valid_loss: 0.01579480331677657\n",
      "FOLD: 0, EPOCH: 34, train_loss: 0.01711741920459915\n",
      "FOLD: 0, EPOCH: 34, valid_loss: 0.015818169985253077\n",
      "FOLD: 0, EPOCH: 35, train_loss: 0.01705262782303868\n",
      "FOLD: 0, EPOCH: 35, valid_loss: 0.015785842059323422\n",
      "FOLD: 0, EPOCH: 36, train_loss: 0.016933502163738012\n",
      "FOLD: 0, EPOCH: 36, valid_loss: 0.01577085808206063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 0, EPOCH: 37, train_loss: 0.01684113498777151\n",
      "FOLD: 0, EPOCH: 37, valid_loss: 0.015786786348773882\n",
      "FOLD: 0, EPOCH: 38, train_loss: 0.016750604757485358\n",
      "FOLD: 0, EPOCH: 38, valid_loss: 0.01573751329515989\n",
      "FOLD: 0, EPOCH: 39, train_loss: 0.016632518189883715\n",
      "FOLD: 0, EPOCH: 39, valid_loss: 0.015801400304413758\n",
      "FOLD: 0, EPOCH: 40, train_loss: 0.016509925317321275\n",
      "FOLD: 0, EPOCH: 40, valid_loss: 0.015772120883831613\n",
      "FOLD: 0, EPOCH: 41, train_loss: 0.016406131140585686\n",
      "FOLD: 0, EPOCH: 41, valid_loss: 0.015830335422204092\n",
      "FOLD: 0, EPOCH: 42, train_loss: 0.01632992850264182\n",
      "FOLD: 0, EPOCH: 42, valid_loss: 0.015762586862995073\n",
      "FOLD: 0, EPOCH: 43, train_loss: 0.016195871829483156\n",
      "FOLD: 0, EPOCH: 43, valid_loss: 0.01578748419594306\n",
      "FOLD: 0, EPOCH: 44, train_loss: 0.016143007317205537\n",
      "FOLD: 0, EPOCH: 44, valid_loss: 0.015821033873810217\n",
      "FOLD: 0, EPOCH: 45, train_loss: 0.015987839675634295\n",
      "FOLD: 0, EPOCH: 45, valid_loss: 0.01580307876261381\n",
      "Epoch    46: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 0, EPOCH: 46, train_loss: 0.015827272049579268\n",
      "FOLD: 0, EPOCH: 46, valid_loss: 0.01576545096647281\n",
      "FOLD: 0, EPOCH: 47, train_loss: 0.015704825142952236\n",
      "FOLD: 0, EPOCH: 47, valid_loss: 0.015780868366933785\n",
      "FOLD: 0, EPOCH: 48, train_loss: 0.015565063829558927\n",
      "FOLD: 0, EPOCH: 48, valid_loss: 0.015810022273888953\n",
      "FOLD: 0, EPOCH: 49, train_loss: 0.015477930636120003\n",
      "FOLD: 0, EPOCH: 49, valid_loss: 0.015816171940129537\n",
      "FOLD: 1, EPOCH: 0, train_loss: 0.04517665500375065\n",
      "FOLD: 1, EPOCH: 0, valid_loss: 0.020086746089733563\n",
      "FOLD: 1, EPOCH: 1, train_loss: 0.02279003613905327\n",
      "FOLD: 1, EPOCH: 1, valid_loss: 0.018751674260084446\n",
      "FOLD: 1, EPOCH: 2, train_loss: 0.021909224624569353\n",
      "FOLD: 1, EPOCH: 2, valid_loss: 0.017947012272018652\n",
      "FOLD: 1, EPOCH: 3, train_loss: 0.02137048713661529\n",
      "FOLD: 1, EPOCH: 3, valid_loss: 0.017541202788169567\n",
      "FOLD: 1, EPOCH: 4, train_loss: 0.021053262507996044\n",
      "FOLD: 1, EPOCH: 4, valid_loss: 0.017362161038013604\n",
      "FOLD: 1, EPOCH: 5, train_loss: 0.02073688865513415\n",
      "FOLD: 1, EPOCH: 5, valid_loss: 0.017099811623875912\n",
      "FOLD: 1, EPOCH: 6, train_loss: 0.02060363645589835\n",
      "FOLD: 1, EPOCH: 6, valid_loss: 0.016985910299878854\n",
      "FOLD: 1, EPOCH: 7, train_loss: 0.020408513150303752\n",
      "FOLD: 1, EPOCH: 7, valid_loss: 0.016801109824043054\n",
      "FOLD: 1, EPOCH: 8, train_loss: 0.020206964398558076\n",
      "FOLD: 1, EPOCH: 8, valid_loss: 0.016656730897151507\n",
      "FOLD: 1, EPOCH: 9, train_loss: 0.020059184976727575\n",
      "FOLD: 1, EPOCH: 9, valid_loss: 0.016596893994854048\n",
      "FOLD: 1, EPOCH: 10, train_loss: 0.019935192867509416\n",
      "FOLD: 1, EPOCH: 10, valid_loss: 0.01645358157559083\n",
      "FOLD: 1, EPOCH: 11, train_loss: 0.019810746423900127\n",
      "FOLD: 1, EPOCH: 11, valid_loss: 0.016418335816034905\n",
      "FOLD: 1, EPOCH: 12, train_loss: 0.019624473174681533\n",
      "FOLD: 1, EPOCH: 12, valid_loss: 0.01629228961582367\n",
      "FOLD: 1, EPOCH: 13, train_loss: 0.019510994161907082\n",
      "FOLD: 1, EPOCH: 13, valid_loss: 0.016281519586650226\n",
      "FOLD: 1, EPOCH: 14, train_loss: 0.01941789165642616\n",
      "FOLD: 1, EPOCH: 14, valid_loss: 0.01616273533839446\n",
      "FOLD: 1, EPOCH: 15, train_loss: 0.019290284882928874\n",
      "FOLD: 1, EPOCH: 15, valid_loss: 0.016084222337947443\n",
      "FOLD: 1, EPOCH: 16, train_loss: 0.01913889484933099\n",
      "FOLD: 1, EPOCH: 16, valid_loss: 0.016040126363245342\n",
      "FOLD: 1, EPOCH: 17, train_loss: 0.01906459398466993\n",
      "FOLD: 1, EPOCH: 17, valid_loss: 0.01599677265263521\n",
      "FOLD: 1, EPOCH: 18, train_loss: 0.01890297739993076\n",
      "FOLD: 1, EPOCH: 18, valid_loss: 0.01593982822333391\n",
      "FOLD: 1, EPOCH: 19, train_loss: 0.018840458294427074\n",
      "FOLD: 1, EPOCH: 19, valid_loss: 0.015914502911842786\n",
      "FOLD: 1, EPOCH: 20, train_loss: 0.018744038292080968\n",
      "FOLD: 1, EPOCH: 20, valid_loss: 0.015937004095086686\n",
      "FOLD: 1, EPOCH: 21, train_loss: 0.01860581320785993\n",
      "FOLD: 1, EPOCH: 21, valid_loss: 0.015877921993915852\n",
      "FOLD: 1, EPOCH: 22, train_loss: 0.0185253372466242\n",
      "FOLD: 1, EPOCH: 22, valid_loss: 0.015850925746445473\n",
      "FOLD: 1, EPOCH: 23, train_loss: 0.01842569506953697\n",
      "FOLD: 1, EPOCH: 23, valid_loss: 0.01579175380846629\n",
      "FOLD: 1, EPOCH: 24, train_loss: 0.01836465553355378\n",
      "FOLD: 1, EPOCH: 24, valid_loss: 0.015788409644021437\n",
      "FOLD: 1, EPOCH: 25, train_loss: 0.01821975815235763\n",
      "FOLD: 1, EPOCH: 25, valid_loss: 0.015762696114297096\n",
      "FOLD: 1, EPOCH: 26, train_loss: 0.018100640333786205\n",
      "FOLD: 1, EPOCH: 26, valid_loss: 0.01572515392819276\n",
      "FOLD: 1, EPOCH: 27, train_loss: 0.018060054529357602\n",
      "FOLD: 1, EPOCH: 27, valid_loss: 0.015721100525787242\n",
      "FOLD: 1, EPOCH: 28, train_loss: 0.01796118640718428\n",
      "FOLD: 1, EPOCH: 28, valid_loss: 0.01568248996940943\n",
      "FOLD: 1, EPOCH: 29, train_loss: 0.017809535578094626\n",
      "FOLD: 1, EPOCH: 29, valid_loss: 0.01569366168517333\n",
      "FOLD: 1, EPOCH: 30, train_loss: 0.01770137430395226\n",
      "FOLD: 1, EPOCH: 30, valid_loss: 0.01567042712122202\n",
      "FOLD: 1, EPOCH: 31, train_loss: 0.01761542526199608\n",
      "FOLD: 1, EPOCH: 31, valid_loss: 0.015682375989854336\n",
      "FOLD: 1, EPOCH: 32, train_loss: 0.017526795445765193\n",
      "FOLD: 1, EPOCH: 32, valid_loss: 0.01564553129271819\n",
      "FOLD: 1, EPOCH: 33, train_loss: 0.017386317454479838\n",
      "FOLD: 1, EPOCH: 33, valid_loss: 0.015643419411319952\n",
      "FOLD: 1, EPOCH: 34, train_loss: 0.01734521277752277\n",
      "FOLD: 1, EPOCH: 34, valid_loss: 0.01562612231534261\n",
      "FOLD: 1, EPOCH: 35, train_loss: 0.017208875484160474\n",
      "FOLD: 1, EPOCH: 35, valid_loss: 0.015616257746632282\n",
      "FOLD: 1, EPOCH: 36, train_loss: 0.017139825441345975\n",
      "FOLD: 1, EPOCH: 36, valid_loss: 0.015670458427988566\n",
      "FOLD: 1, EPOCH: 37, train_loss: 0.016975712182151305\n",
      "FOLD: 1, EPOCH: 37, valid_loss: 0.015623042431588356\n",
      "FOLD: 1, EPOCH: 38, train_loss: 0.016889060243360093\n",
      "FOLD: 1, EPOCH: 38, valid_loss: 0.015616956453483839\n",
      "FOLD: 1, EPOCH: 39, train_loss: 0.016832510512825603\n",
      "FOLD: 1, EPOCH: 39, valid_loss: 0.015646247193217278\n",
      "FOLD: 1, EPOCH: 40, train_loss: 0.016739422128208586\n",
      "FOLD: 1, EPOCH: 40, valid_loss: 0.01569022574963478\n",
      "FOLD: 1, EPOCH: 41, train_loss: 0.01664850426636435\n",
      "FOLD: 1, EPOCH: 41, valid_loss: 0.015632441697212365\n",
      "FOLD: 1, EPOCH: 42, train_loss: 0.01659770672385757\n",
      "FOLD: 1, EPOCH: 42, valid_loss: 0.01564789806994108\n",
      "Epoch    43: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 1, EPOCH: 43, train_loss: 0.016371123014471016\n",
      "FOLD: 1, EPOCH: 43, valid_loss: 0.01567525638697239\n",
      "FOLD: 1, EPOCH: 44, train_loss: 0.01622579206486006\n",
      "FOLD: 1, EPOCH: 44, valid_loss: 0.015615092375530647\n",
      "FOLD: 1, EPOCH: 45, train_loss: 0.016074218145395454\n",
      "FOLD: 1, EPOCH: 45, valid_loss: 0.015620582880308995\n",
      "FOLD: 1, EPOCH: 46, train_loss: 0.016000987068322058\n",
      "FOLD: 1, EPOCH: 46, valid_loss: 0.015638961313435666\n",
      "FOLD: 1, EPOCH: 47, train_loss: 0.01592772880669784\n",
      "FOLD: 1, EPOCH: 47, valid_loss: 0.015644350518973973\n",
      "FOLD: 1, EPOCH: 48, train_loss: 0.015868079571707827\n",
      "FOLD: 1, EPOCH: 48, valid_loss: 0.01563652568998245\n",
      "FOLD: 1, EPOCH: 49, train_loss: 0.015763981116784585\n",
      "FOLD: 1, EPOCH: 49, valid_loss: 0.01565561078202266\n",
      "FOLD: 2, EPOCH: 0, train_loss: 0.042815789555174275\n",
      "FOLD: 2, EPOCH: 0, valid_loss: 0.020246196824770708\n",
      "FOLD: 2, EPOCH: 1, train_loss: 0.022650403524371417\n",
      "FOLD: 2, EPOCH: 1, valid_loss: 0.018985082108813982\n",
      "FOLD: 2, EPOCH: 2, train_loss: 0.021870219727625716\n",
      "FOLD: 2, EPOCH: 2, valid_loss: 0.018346899618896153\n",
      "FOLD: 2, EPOCH: 3, train_loss: 0.0212960468756186\n",
      "FOLD: 2, EPOCH: 3, valid_loss: 0.017510331594027005\n",
      "FOLD: 2, EPOCH: 4, train_loss: 0.020915714998704357\n",
      "FOLD: 2, EPOCH: 4, valid_loss: 0.01721173696793043\n",
      "FOLD: 2, EPOCH: 5, train_loss: 0.020648582964330107\n",
      "FOLD: 2, EPOCH: 5, valid_loss: 0.0170882844294493\n",
      "FOLD: 2, EPOCH: 6, train_loss: 0.0204211789107806\n",
      "FOLD: 2, EPOCH: 6, valid_loss: 0.016916059100857146\n",
      "FOLD: 2, EPOCH: 7, train_loss: 0.02022691149063207\n",
      "FOLD: 2, EPOCH: 7, valid_loss: 0.016714730371649448\n",
      "FOLD: 2, EPOCH: 8, train_loss: 0.02009543653174832\n",
      "FOLD: 2, EPOCH: 8, valid_loss: 0.01664208907347459\n",
      "FOLD: 2, EPOCH: 9, train_loss: 0.01992092153208481\n",
      "FOLD: 2, EPOCH: 9, valid_loss: 0.016547525946337443\n",
      "FOLD: 2, EPOCH: 10, train_loss: 0.0198125518808091\n",
      "FOLD: 2, EPOCH: 10, valid_loss: 0.01649736799299717\n",
      "FOLD: 2, EPOCH: 11, train_loss: 0.019671980895706126\n",
      "FOLD: 2, EPOCH: 11, valid_loss: 0.01643365155905485\n",
      "FOLD: 2, EPOCH: 12, train_loss: 0.019563888703045006\n",
      "FOLD: 2, EPOCH: 12, valid_loss: 0.01632219538665735\n",
      "FOLD: 2, EPOCH: 13, train_loss: 0.01941094731257574\n",
      "FOLD: 2, EPOCH: 13, valid_loss: 0.016303592074949008\n",
      "FOLD: 2, EPOCH: 14, train_loss: 0.01931539993431117\n",
      "FOLD: 2, EPOCH: 14, valid_loss: 0.01622593753899519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 2, EPOCH: 15, train_loss: 0.01914093298585834\n",
      "FOLD: 2, EPOCH: 15, valid_loss: 0.016118547306037866\n",
      "FOLD: 2, EPOCH: 16, train_loss: 0.01901620482975567\n",
      "FOLD: 2, EPOCH: 16, valid_loss: 0.016062326800937835\n",
      "FOLD: 2, EPOCH: 17, train_loss: 0.018939809395453415\n",
      "FOLD: 2, EPOCH: 17, valid_loss: 0.01597159792884038\n",
      "FOLD: 2, EPOCH: 18, train_loss: 0.018779619647240318\n",
      "FOLD: 2, EPOCH: 18, valid_loss: 0.01598059242734542\n",
      "FOLD: 2, EPOCH: 19, train_loss: 0.01868926512228476\n",
      "FOLD: 2, EPOCH: 19, valid_loss: 0.01590699372956386\n",
      "FOLD: 2, EPOCH: 20, train_loss: 0.018590990026053543\n",
      "FOLD: 2, EPOCH: 20, valid_loss: 0.015879642719832752\n",
      "FOLD: 2, EPOCH: 21, train_loss: 0.01850477823786236\n",
      "FOLD: 2, EPOCH: 21, valid_loss: 0.015899841602032002\n",
      "FOLD: 2, EPOCH: 22, train_loss: 0.01838536190523489\n",
      "FOLD: 2, EPOCH: 22, valid_loss: 0.015875505283474922\n",
      "FOLD: 2, EPOCH: 23, train_loss: 0.018295156366720394\n",
      "FOLD: 2, EPOCH: 23, valid_loss: 0.01585120542977865\n",
      "FOLD: 2, EPOCH: 24, train_loss: 0.01816325672474262\n",
      "FOLD: 2, EPOCH: 24, valid_loss: 0.01580578411141267\n",
      "FOLD: 2, EPOCH: 25, train_loss: 0.01802547716510457\n",
      "FOLD: 2, EPOCH: 25, valid_loss: 0.015762794762849808\n",
      "FOLD: 2, EPOCH: 26, train_loss: 0.01797994427584313\n",
      "FOLD: 2, EPOCH: 26, valid_loss: 0.01577386797334139\n",
      "FOLD: 2, EPOCH: 27, train_loss: 0.017861891764442663\n",
      "FOLD: 2, EPOCH: 27, valid_loss: 0.015739219191555794\n",
      "FOLD: 2, EPOCH: 28, train_loss: 0.017756419231158657\n",
      "FOLD: 2, EPOCH: 28, valid_loss: 0.015746052376925945\n",
      "FOLD: 2, EPOCH: 29, train_loss: 0.017637456565893984\n",
      "FOLD: 2, EPOCH: 29, valid_loss: 0.015722785933086507\n",
      "FOLD: 2, EPOCH: 30, train_loss: 0.017518529693621235\n",
      "FOLD: 2, EPOCH: 30, valid_loss: 0.01570664718747139\n",
      "FOLD: 2, EPOCH: 31, train_loss: 0.01743066348638889\n",
      "FOLD: 2, EPOCH: 31, valid_loss: 0.015733293186013516\n",
      "FOLD: 2, EPOCH: 32, train_loss: 0.01740323500456037\n",
      "FOLD: 2, EPOCH: 32, valid_loss: 0.0156961576296733\n",
      "FOLD: 2, EPOCH: 33, train_loss: 0.017222430154278472\n",
      "FOLD: 2, EPOCH: 33, valid_loss: 0.015659064914171513\n",
      "FOLD: 2, EPOCH: 34, train_loss: 0.017169062992104807\n",
      "FOLD: 2, EPOCH: 34, valid_loss: 0.01568578491703822\n",
      "FOLD: 2, EPOCH: 35, train_loss: 0.01706293982931891\n",
      "FOLD: 2, EPOCH: 35, valid_loss: 0.015681573977837197\n",
      "FOLD: 2, EPOCH: 36, train_loss: 0.016933097113930696\n",
      "FOLD: 2, EPOCH: 36, valid_loss: 0.015684248592991095\n",
      "FOLD: 2, EPOCH: 37, train_loss: 0.016845171465664298\n",
      "FOLD: 2, EPOCH: 37, valid_loss: 0.015712010960739393\n",
      "FOLD: 2, EPOCH: 38, train_loss: 0.016717403900583048\n",
      "FOLD: 2, EPOCH: 38, valid_loss: 0.01571032075354686\n",
      "FOLD: 2, EPOCH: 39, train_loss: 0.01667037991353789\n",
      "FOLD: 2, EPOCH: 39, valid_loss: 0.01568967240074506\n",
      "FOLD: 2, EPOCH: 40, train_loss: 0.01649549218347749\n",
      "FOLD: 2, EPOCH: 40, valid_loss: 0.015684472110409003\n",
      "Epoch    41: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 2, EPOCH: 41, train_loss: 0.016319239474329596\n",
      "FOLD: 2, EPOCH: 41, valid_loss: 0.015703354102487747\n",
      "FOLD: 2, EPOCH: 42, train_loss: 0.016171783800362736\n",
      "FOLD: 2, EPOCH: 42, valid_loss: 0.015675148425193932\n",
      "FOLD: 2, EPOCH: 43, train_loss: 0.016045611175532278\n",
      "FOLD: 2, EPOCH: 43, valid_loss: 0.015684398894126598\n",
      "FOLD: 2, EPOCH: 44, train_loss: 0.015963756139516026\n",
      "FOLD: 2, EPOCH: 44, valid_loss: 0.015714379385686837\n",
      "FOLD: 2, EPOCH: 45, train_loss: 0.015894229196616122\n",
      "FOLD: 2, EPOCH: 45, valid_loss: 0.015705645872423284\n",
      "FOLD: 2, EPOCH: 46, train_loss: 0.015824731351253955\n",
      "FOLD: 2, EPOCH: 46, valid_loss: 0.01567990554926487\n",
      "FOLD: 2, EPOCH: 47, train_loss: 0.015700196935417684\n",
      "FOLD: 2, EPOCH: 47, valid_loss: 0.01568465629735818\n",
      "Epoch    48: reducing learning rate of group 0 to 1.1288e-01.\n",
      "FOLD: 2, EPOCH: 48, train_loss: 0.015531431845817211\n",
      "FOLD: 2, EPOCH: 48, valid_loss: 0.01567073417111085\n",
      "FOLD: 2, EPOCH: 49, train_loss: 0.015427375489191429\n",
      "FOLD: 2, EPOCH: 49, valid_loss: 0.015699553231780346\n",
      "FOLD: 3, EPOCH: 0, train_loss: 0.04370261270653557\n",
      "FOLD: 3, EPOCH: 0, valid_loss: 0.020227097977812473\n",
      "FOLD: 3, EPOCH: 1, train_loss: 0.02272452912419229\n",
      "FOLD: 3, EPOCH: 1, valid_loss: 0.018794485582755163\n",
      "FOLD: 3, EPOCH: 2, train_loss: 0.021846484493564914\n",
      "FOLD: 3, EPOCH: 2, valid_loss: 0.017999753499260314\n",
      "FOLD: 3, EPOCH: 3, train_loss: 0.02129092495385054\n",
      "FOLD: 3, EPOCH: 3, valid_loss: 0.017708345531271055\n",
      "FOLD: 3, EPOCH: 4, train_loss: 0.02090396100302806\n",
      "FOLD: 3, EPOCH: 4, valid_loss: 0.017401729782040302\n",
      "FOLD: 3, EPOCH: 5, train_loss: 0.020663082750665175\n",
      "FOLD: 3, EPOCH: 5, valid_loss: 0.01721634493710903\n",
      "FOLD: 3, EPOCH: 6, train_loss: 0.020469719141318992\n",
      "FOLD: 3, EPOCH: 6, valid_loss: 0.017074994241388943\n",
      "FOLD: 3, EPOCH: 7, train_loss: 0.02023456499886674\n",
      "FOLD: 3, EPOCH: 7, valid_loss: 0.016939711613723867\n",
      "FOLD: 3, EPOCH: 8, train_loss: 0.02012759545264212\n",
      "FOLD: 3, EPOCH: 8, valid_loss: 0.016899519886534948\n",
      "FOLD: 3, EPOCH: 9, train_loss: 0.019940786315379915\n",
      "FOLD: 3, EPOCH: 9, valid_loss: 0.01670925856496279\n",
      "FOLD: 3, EPOCH: 10, train_loss: 0.019778215804615536\n",
      "FOLD: 3, EPOCH: 10, valid_loss: 0.016721947763401728\n",
      "FOLD: 3, EPOCH: 11, train_loss: 0.019656460077778715\n",
      "FOLD: 3, EPOCH: 11, valid_loss: 0.01657858046774681\n",
      "FOLD: 3, EPOCH: 12, train_loss: 0.01955719443189131\n",
      "FOLD: 3, EPOCH: 12, valid_loss: 0.016541534963135537\n",
      "FOLD: 3, EPOCH: 13, train_loss: 0.019398230732091376\n",
      "FOLD: 3, EPOCH: 13, valid_loss: 0.01647329294624237\n",
      "FOLD: 3, EPOCH: 14, train_loss: 0.019303124750385415\n",
      "FOLD: 3, EPOCH: 14, valid_loss: 0.016423007401709374\n",
      "FOLD: 3, EPOCH: 15, train_loss: 0.019164951543348865\n",
      "FOLD: 3, EPOCH: 15, valid_loss: 0.016331699748451892\n",
      "FOLD: 3, EPOCH: 16, train_loss: 0.019085888322946186\n",
      "FOLD: 3, EPOCH: 16, valid_loss: 0.01629549852357461\n",
      "FOLD: 3, EPOCH: 17, train_loss: 0.018954754907738517\n",
      "FOLD: 3, EPOCH: 17, valid_loss: 0.01622692817965379\n",
      "FOLD: 3, EPOCH: 18, train_loss: 0.018808915330147422\n",
      "FOLD: 3, EPOCH: 18, valid_loss: 0.016225892763871413\n",
      "FOLD: 3, EPOCH: 19, train_loss: 0.018659743140577466\n",
      "FOLD: 3, EPOCH: 19, valid_loss: 0.016145464032888412\n",
      "FOLD: 3, EPOCH: 20, train_loss: 0.01866006833576673\n",
      "FOLD: 3, EPOCH: 20, valid_loss: 0.016115376654152688\n",
      "FOLD: 3, EPOCH: 21, train_loss: 0.018528944913398574\n",
      "FOLD: 3, EPOCH: 21, valid_loss: 0.016082046911693536\n",
      "FOLD: 3, EPOCH: 22, train_loss: 0.018399144598358386\n",
      "FOLD: 3, EPOCH: 22, valid_loss: 0.016052000439510897\n",
      "FOLD: 3, EPOCH: 23, train_loss: 0.018264414843272517\n",
      "FOLD: 3, EPOCH: 23, valid_loss: 0.016039618864082374\n",
      "FOLD: 3, EPOCH: 24, train_loss: 0.0181695116633499\n",
      "FOLD: 3, EPOCH: 24, valid_loss: 0.0160119474077454\n",
      "FOLD: 3, EPOCH: 25, train_loss: 0.018104695287105198\n",
      "FOLD: 3, EPOCH: 25, valid_loss: 0.016076144905617602\n",
      "FOLD: 3, EPOCH: 26, train_loss: 0.017962765779245545\n",
      "FOLD: 3, EPOCH: 26, valid_loss: 0.015996557015639085\n",
      "FOLD: 3, EPOCH: 27, train_loss: 0.01781517790781485\n",
      "FOLD: 3, EPOCH: 27, valid_loss: 0.015958270559517238\n",
      "FOLD: 3, EPOCH: 28, train_loss: 0.017761191252518346\n",
      "FOLD: 3, EPOCH: 28, valid_loss: 0.01593429072258564\n",
      "FOLD: 3, EPOCH: 29, train_loss: 0.017648812307900674\n",
      "FOLD: 3, EPOCH: 29, valid_loss: 0.01597224620099251\n",
      "FOLD: 3, EPOCH: 30, train_loss: 0.017590532482073113\n",
      "FOLD: 3, EPOCH: 30, valid_loss: 0.01596236049842376\n",
      "FOLD: 3, EPOCH: 31, train_loss: 0.017452693015739724\n",
      "FOLD: 3, EPOCH: 31, valid_loss: 0.01592846465511964\n",
      "FOLD: 3, EPOCH: 32, train_loss: 0.01735720273761733\n",
      "FOLD: 3, EPOCH: 32, valid_loss: 0.015939219926412288\n",
      "FOLD: 3, EPOCH: 33, train_loss: 0.017230400714922597\n",
      "FOLD: 3, EPOCH: 33, valid_loss: 0.015903171796638232\n",
      "FOLD: 3, EPOCH: 34, train_loss: 0.017203376137626333\n",
      "FOLD: 3, EPOCH: 34, valid_loss: 0.015919730210533507\n",
      "FOLD: 3, EPOCH: 35, train_loss: 0.01708200286973167\n",
      "FOLD: 3, EPOCH: 35, valid_loss: 0.01589785423129797\n",
      "FOLD: 3, EPOCH: 36, train_loss: 0.016950680345699593\n",
      "FOLD: 3, EPOCH: 36, valid_loss: 0.015937989864211816\n",
      "FOLD: 3, EPOCH: 37, train_loss: 0.016897572858913523\n",
      "FOLD: 3, EPOCH: 37, valid_loss: 0.015928406196718033\n",
      "FOLD: 3, EPOCH: 38, train_loss: 0.01674550010294125\n",
      "FOLD: 3, EPOCH: 38, valid_loss: 0.01586242005802118\n",
      "FOLD: 3, EPOCH: 39, train_loss: 0.01668796568756571\n",
      "FOLD: 3, EPOCH: 39, valid_loss: 0.015856129547151234\n",
      "FOLD: 3, EPOCH: 40, train_loss: 0.01656236724827338\n",
      "FOLD: 3, EPOCH: 40, valid_loss: 0.015853078247836\n",
      "FOLD: 3, EPOCH: 41, train_loss: 0.016515262022211746\n",
      "FOLD: 3, EPOCH: 41, valid_loss: 0.015916952791695412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 3, EPOCH: 42, train_loss: 0.016338746529072523\n",
      "FOLD: 3, EPOCH: 42, valid_loss: 0.015856394687524207\n",
      "FOLD: 3, EPOCH: 43, train_loss: 0.016300321896434634\n",
      "FOLD: 3, EPOCH: 43, valid_loss: 0.01589892045236551\n",
      "FOLD: 3, EPOCH: 44, train_loss: 0.01623148555439469\n",
      "FOLD: 3, EPOCH: 44, valid_loss: 0.01589785874463045\n",
      "FOLD: 3, EPOCH: 45, train_loss: 0.01609619358252432\n",
      "FOLD: 3, EPOCH: 45, valid_loss: 0.01591293879139882\n",
      "FOLD: 3, EPOCH: 46, train_loss: 0.01596982720484202\n",
      "FOLD: 3, EPOCH: 46, valid_loss: 0.015914214846606437\n",
      "FOLD: 3, EPOCH: 47, train_loss: 0.01592936336591437\n",
      "FOLD: 3, EPOCH: 47, valid_loss: 0.015888767531858042\n",
      "Epoch    48: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 3, EPOCH: 48, train_loss: 0.015725172766660515\n",
      "FOLD: 3, EPOCH: 48, valid_loss: 0.015883595682680607\n",
      "FOLD: 3, EPOCH: 49, train_loss: 0.015576615375844208\n",
      "FOLD: 3, EPOCH: 49, valid_loss: 0.01588254629705961\n",
      "FOLD: 4, EPOCH: 0, train_loss: 0.04485300872978326\n",
      "FOLD: 4, EPOCH: 0, valid_loss: 0.021099407512408037\n",
      "FOLD: 4, EPOCH: 1, train_loss: 0.02280641075324368\n",
      "FOLD: 4, EPOCH: 1, valid_loss: 0.01916649092275363\n",
      "FOLD: 4, EPOCH: 2, train_loss: 0.021899270246157777\n",
      "FOLD: 4, EPOCH: 2, valid_loss: 0.01826110424903723\n",
      "FOLD: 4, EPOCH: 3, train_loss: 0.0212957600534365\n",
      "FOLD: 4, EPOCH: 3, valid_loss: 0.01784234751875584\n",
      "FOLD: 4, EPOCH: 4, train_loss: 0.020967857660474005\n",
      "FOLD: 4, EPOCH: 4, valid_loss: 0.017633153698765315\n",
      "FOLD: 4, EPOCH: 5, train_loss: 0.020725103774787607\n",
      "FOLD: 4, EPOCH: 5, valid_loss: 0.01741258456156804\n",
      "FOLD: 4, EPOCH: 6, train_loss: 0.020508415065705776\n",
      "FOLD: 4, EPOCH: 6, valid_loss: 0.017247013174570523\n",
      "FOLD: 4, EPOCH: 7, train_loss: 0.02031087875366211\n",
      "FOLD: 4, EPOCH: 7, valid_loss: 0.017098370079810802\n",
      "FOLD: 4, EPOCH: 8, train_loss: 0.020147513270982215\n",
      "FOLD: 4, EPOCH: 8, valid_loss: 0.0169278964973413\n",
      "FOLD: 4, EPOCH: 9, train_loss: 0.019990893911469628\n",
      "FOLD: 4, EPOCH: 9, valid_loss: 0.01695071991819602\n",
      "FOLD: 4, EPOCH: 10, train_loss: 0.01981439323139352\n",
      "FOLD: 4, EPOCH: 10, valid_loss: 0.016777912727915324\n",
      "FOLD: 4, EPOCH: 11, train_loss: 0.019697149787601585\n",
      "FOLD: 4, EPOCH: 11, valid_loss: 0.016689790699344415\n",
      "FOLD: 4, EPOCH: 12, train_loss: 0.019552764180745627\n",
      "FOLD: 4, EPOCH: 12, valid_loss: 0.016584503249480173\n",
      "FOLD: 4, EPOCH: 13, train_loss: 0.019404663200918083\n",
      "FOLD: 4, EPOCH: 13, valid_loss: 0.016563385581740968\n",
      "FOLD: 4, EPOCH: 14, train_loss: 0.01930181440469381\n",
      "FOLD: 4, EPOCH: 14, valid_loss: 0.01644467290204305\n",
      "FOLD: 4, EPOCH: 15, train_loss: 0.019209103054694227\n",
      "FOLD: 4, EPOCH: 15, valid_loss: 0.016474266178332843\n",
      "FOLD: 4, EPOCH: 16, train_loss: 0.019114067626966012\n",
      "FOLD: 4, EPOCH: 16, valid_loss: 0.016371827787504747\n",
      "FOLD: 4, EPOCH: 17, train_loss: 0.018981752036189712\n",
      "FOLD: 4, EPOCH: 17, valid_loss: 0.01634111341375571\n",
      "FOLD: 4, EPOCH: 18, train_loss: 0.018860927531243982\n",
      "FOLD: 4, EPOCH: 18, valid_loss: 0.016331813728006985\n",
      "FOLD: 4, EPOCH: 19, train_loss: 0.018757315553926134\n",
      "FOLD: 4, EPOCH: 19, valid_loss: 0.01624921179161622\n",
      "FOLD: 4, EPOCH: 20, train_loss: 0.01863398095844565\n",
      "FOLD: 4, EPOCH: 20, valid_loss: 0.016251504349593934\n",
      "FOLD: 4, EPOCH: 21, train_loss: 0.018526544593073225\n",
      "FOLD: 4, EPOCH: 21, valid_loss: 0.016236957377539232\n",
      "FOLD: 4, EPOCH: 22, train_loss: 0.018459006162309968\n",
      "FOLD: 4, EPOCH: 22, valid_loss: 0.016244537411974028\n",
      "FOLD: 4, EPOCH: 23, train_loss: 0.018371292843005142\n",
      "FOLD: 4, EPOCH: 23, valid_loss: 0.01620042187949786\n",
      "FOLD: 4, EPOCH: 24, train_loss: 0.018197172422062705\n",
      "FOLD: 4, EPOCH: 24, valid_loss: 0.01620227420845857\n",
      "FOLD: 4, EPOCH: 25, train_loss: 0.018094963419276314\n",
      "FOLD: 4, EPOCH: 25, valid_loss: 0.016134208927933987\n",
      "FOLD: 4, EPOCH: 26, train_loss: 0.018068919587578322\n",
      "FOLD: 4, EPOCH: 26, valid_loss: 0.016148111496407252\n",
      "FOLD: 4, EPOCH: 27, train_loss: 0.017912346693510946\n",
      "FOLD: 4, EPOCH: 27, valid_loss: 0.016109796742407177\n",
      "FOLD: 4, EPOCH: 28, train_loss: 0.01782671903335565\n",
      "FOLD: 4, EPOCH: 28, valid_loss: 0.0160896391966022\n",
      "FOLD: 4, EPOCH: 29, train_loss: 0.017710876651108265\n",
      "FOLD: 4, EPOCH: 29, valid_loss: 0.01607211557431863\n",
      "FOLD: 4, EPOCH: 30, train_loss: 0.017624375838282948\n",
      "FOLD: 4, EPOCH: 30, valid_loss: 0.016095108495881923\n",
      "FOLD: 4, EPOCH: 31, train_loss: 0.017519451702970104\n",
      "FOLD: 4, EPOCH: 31, valid_loss: 0.016044444046341456\n",
      "FOLD: 4, EPOCH: 32, train_loss: 0.017482462930618912\n",
      "FOLD: 4, EPOCH: 32, valid_loss: 0.01603170742209141\n",
      "FOLD: 4, EPOCH: 33, train_loss: 0.017324266451838856\n",
      "FOLD: 4, EPOCH: 33, valid_loss: 0.016037261543365624\n",
      "FOLD: 4, EPOCH: 34, train_loss: 0.017223753236435557\n",
      "FOLD: 4, EPOCH: 34, valid_loss: 0.016023651625101384\n",
      "FOLD: 4, EPOCH: 35, train_loss: 0.017111527444039647\n",
      "FOLD: 4, EPOCH: 35, valid_loss: 0.016041592264977787\n",
      "FOLD: 4, EPOCH: 36, train_loss: 0.017034073322507982\n",
      "FOLD: 4, EPOCH: 36, valid_loss: 0.016068187900460683\n",
      "FOLD: 4, EPOCH: 37, train_loss: 0.016958656594962686\n",
      "FOLD: 4, EPOCH: 37, valid_loss: 0.01605042242086851\n",
      "FOLD: 4, EPOCH: 38, train_loss: 0.016811901577622503\n",
      "FOLD: 4, EPOCH: 38, valid_loss: 0.016038724794410743\n",
      "FOLD: 4, EPOCH: 39, train_loss: 0.016767757315490697\n",
      "FOLD: 4, EPOCH: 39, valid_loss: 0.016050524794711515\n",
      "FOLD: 4, EPOCH: 40, train_loss: 0.01663829289326394\n",
      "FOLD: 4, EPOCH: 40, valid_loss: 0.016022152339036647\n",
      "FOLD: 4, EPOCH: 41, train_loss: 0.016579255625303532\n",
      "FOLD: 4, EPOCH: 41, valid_loss: 0.01605265050266798\n",
      "FOLD: 4, EPOCH: 42, train_loss: 0.016450897488441016\n",
      "FOLD: 4, EPOCH: 42, valid_loss: 0.016030025596802052\n",
      "FOLD: 4, EPOCH: 43, train_loss: 0.016375436384633586\n",
      "FOLD: 4, EPOCH: 43, valid_loss: 0.016015351463395815\n",
      "FOLD: 4, EPOCH: 44, train_loss: 0.016205474029521685\n",
      "FOLD: 4, EPOCH: 44, valid_loss: 0.01603854626703721\n",
      "FOLD: 4, EPOCH: 45, train_loss: 0.01618765721198272\n",
      "FOLD: 4, EPOCH: 45, valid_loss: 0.01602725677478772\n",
      "FOLD: 4, EPOCH: 46, train_loss: 0.016090502207343642\n",
      "FOLD: 4, EPOCH: 46, valid_loss: 0.016040022771518964\n",
      "FOLD: 4, EPOCH: 47, train_loss: 0.015964311774115305\n",
      "FOLD: 4, EPOCH: 47, valid_loss: 0.016084546151642617\n",
      "FOLD: 4, EPOCH: 48, train_loss: 0.015839428187826195\n",
      "FOLD: 4, EPOCH: 48, valid_loss: 0.016042520578664083\n",
      "FOLD: 4, EPOCH: 49, train_loss: 0.015764002247738676\n",
      "FOLD: 4, EPOCH: 49, valid_loss: 0.01606033706607727\n",
      "FOLD: 5, EPOCH: 0, train_loss: 0.04519159266272107\n",
      "FOLD: 5, EPOCH: 0, valid_loss: 0.0211896811826871\n",
      "FOLD: 5, EPOCH: 1, train_loss: 0.022711058172422485\n",
      "FOLD: 5, EPOCH: 1, valid_loss: 0.019118954355900105\n",
      "FOLD: 5, EPOCH: 2, train_loss: 0.021898731866197008\n",
      "FOLD: 5, EPOCH: 2, valid_loss: 0.01821978476185065\n",
      "FOLD: 5, EPOCH: 3, train_loss: 0.021363968130301784\n",
      "FOLD: 5, EPOCH: 3, valid_loss: 0.017915667966008186\n",
      "FOLD: 5, EPOCH: 4, train_loss: 0.02103224507457501\n",
      "FOLD: 5, EPOCH: 4, valid_loss: 0.017542619544726152\n",
      "FOLD: 5, EPOCH: 5, train_loss: 0.020771816121162596\n",
      "FOLD: 5, EPOCH: 5, valid_loss: 0.017360195517539978\n",
      "FOLD: 5, EPOCH: 6, train_loss: 0.020549785144425726\n",
      "FOLD: 5, EPOCH: 6, valid_loss: 0.017249974350516614\n",
      "FOLD: 5, EPOCH: 7, train_loss: 0.02034225916439617\n",
      "FOLD: 5, EPOCH: 7, valid_loss: 0.017148260170450576\n",
      "FOLD: 5, EPOCH: 8, train_loss: 0.020183613171448577\n",
      "FOLD: 5, EPOCH: 8, valid_loss: 0.017101597900574025\n",
      "FOLD: 5, EPOCH: 9, train_loss: 0.020045352621457062\n",
      "FOLD: 5, EPOCH: 9, valid_loss: 0.01692734085596525\n",
      "FOLD: 5, EPOCH: 10, train_loss: 0.01987785886268358\n",
      "FOLD: 5, EPOCH: 10, valid_loss: 0.01682622214922538\n",
      "FOLD: 5, EPOCH: 11, train_loss: 0.019769822775914863\n",
      "FOLD: 5, EPOCH: 11, valid_loss: 0.016759971013435952\n",
      "FOLD: 5, EPOCH: 12, train_loss: 0.019642886000911932\n",
      "FOLD: 5, EPOCH: 12, valid_loss: 0.016671352733213168\n",
      "FOLD: 5, EPOCH: 13, train_loss: 0.019520974692863388\n",
      "FOLD: 5, EPOCH: 13, valid_loss: 0.016614367254078388\n",
      "FOLD: 5, EPOCH: 14, train_loss: 0.019414203191125714\n",
      "FOLD: 5, EPOCH: 14, valid_loss: 0.016512617038992736\n",
      "FOLD: 5, EPOCH: 15, train_loss: 0.019255478136442804\n",
      "FOLD: 5, EPOCH: 15, valid_loss: 0.016390688717365265\n",
      "FOLD: 5, EPOCH: 16, train_loss: 0.019180802200492973\n",
      "FOLD: 5, EPOCH: 16, valid_loss: 0.01641420189004678\n",
      "FOLD: 5, EPOCH: 17, train_loss: 0.019049380704559183\n",
      "FOLD: 5, EPOCH: 17, valid_loss: 0.016336436813267376\n",
      "FOLD: 5, EPOCH: 18, train_loss: 0.018927730116489773\n",
      "FOLD: 5, EPOCH: 18, valid_loss: 0.016270025203434322\n",
      "FOLD: 5, EPOCH: 19, train_loss: 0.018838002497481333\n",
      "FOLD: 5, EPOCH: 19, valid_loss: 0.0162451144021291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 5, EPOCH: 20, train_loss: 0.01876668055617326\n",
      "FOLD: 5, EPOCH: 20, valid_loss: 0.016187313442619946\n",
      "FOLD: 5, EPOCH: 21, train_loss: 0.018679194051671674\n",
      "FOLD: 5, EPOCH: 21, valid_loss: 0.016178363504318092\n",
      "FOLD: 5, EPOCH: 22, train_loss: 0.018535998322673747\n",
      "FOLD: 5, EPOCH: 22, valid_loss: 0.01614727509709505\n",
      "FOLD: 5, EPOCH: 23, train_loss: 0.018460578461353842\n",
      "FOLD: 5, EPOCH: 23, valid_loss: 0.016131090573393382\n",
      "FOLD: 5, EPOCH: 24, train_loss: 0.018311377463711274\n",
      "FOLD: 5, EPOCH: 24, valid_loss: 0.016069673001766205\n",
      "FOLD: 5, EPOCH: 25, train_loss: 0.018233122933353926\n",
      "FOLD: 5, EPOCH: 25, valid_loss: 0.016090036942981757\n",
      "FOLD: 5, EPOCH: 26, train_loss: 0.018133151747689053\n",
      "FOLD: 5, EPOCH: 26, valid_loss: 0.016102236623947438\n",
      "FOLD: 5, EPOCH: 27, train_loss: 0.018022926146718295\n",
      "FOLD: 5, EPOCH: 27, valid_loss: 0.016024512525361318\n",
      "FOLD: 5, EPOCH: 28, train_loss: 0.017908355145639664\n",
      "FOLD: 5, EPOCH: 28, valid_loss: 0.016039254143834114\n",
      "FOLD: 5, EPOCH: 29, train_loss: 0.017822930438292993\n",
      "FOLD: 5, EPOCH: 29, valid_loss: 0.016012765610447295\n",
      "FOLD: 5, EPOCH: 30, train_loss: 0.017759959439973574\n",
      "FOLD: 5, EPOCH: 30, valid_loss: 0.015972514995015584\n",
      "FOLD: 5, EPOCH: 31, train_loss: 0.017615130850793543\n",
      "FOLD: 5, EPOCH: 31, valid_loss: 0.01598003091147313\n",
      "FOLD: 5, EPOCH: 32, train_loss: 0.01758039410452585\n",
      "FOLD: 5, EPOCH: 32, valid_loss: 0.015959963775598086\n",
      "FOLD: 5, EPOCH: 33, train_loss: 0.017432218039962085\n",
      "FOLD: 5, EPOCH: 33, valid_loss: 0.015978758080074422\n",
      "FOLD: 5, EPOCH: 34, train_loss: 0.0173152437735651\n",
      "FOLD: 5, EPOCH: 34, valid_loss: 0.015912029820565995\n",
      "FOLD: 5, EPOCH: 35, train_loss: 0.017210376763565315\n",
      "FOLD: 5, EPOCH: 35, valid_loss: 0.015917051010406934\n",
      "FOLD: 5, EPOCH: 36, train_loss: 0.01714068092405796\n",
      "FOLD: 5, EPOCH: 36, valid_loss: 0.01591378514869855\n",
      "FOLD: 5, EPOCH: 37, train_loss: 0.017042540265498934\n",
      "FOLD: 5, EPOCH: 37, valid_loss: 0.015908575186935756\n",
      "FOLD: 5, EPOCH: 38, train_loss: 0.016912146194561106\n",
      "FOLD: 5, EPOCH: 38, valid_loss: 0.01595564766858633\n",
      "FOLD: 5, EPOCH: 39, train_loss: 0.016861316534010944\n",
      "FOLD: 5, EPOCH: 39, valid_loss: 0.015951598564592693\n",
      "FOLD: 5, EPOCH: 40, train_loss: 0.016788409909585845\n",
      "FOLD: 5, EPOCH: 40, valid_loss: 0.015929636903680287\n",
      "FOLD: 5, EPOCH: 41, train_loss: 0.01662944379338139\n",
      "FOLD: 5, EPOCH: 41, valid_loss: 0.015941457607998297\n",
      "FOLD: 5, EPOCH: 42, train_loss: 0.016570965746870718\n",
      "FOLD: 5, EPOCH: 42, valid_loss: 0.015970363926429015\n",
      "FOLD: 5, EPOCH: 43, train_loss: 0.016477833174773165\n",
      "FOLD: 5, EPOCH: 43, valid_loss: 0.0159741580629578\n",
      "FOLD: 5, EPOCH: 44, train_loss: 0.016394662318398822\n",
      "FOLD: 5, EPOCH: 44, valid_loss: 0.015940009114833977\n",
      "Epoch    45: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 5, EPOCH: 45, train_loss: 0.016159150195685593\n",
      "FOLD: 5, EPOCH: 45, valid_loss: 0.015900417374303706\n",
      "FOLD: 5, EPOCH: 46, train_loss: 0.016081199402341973\n",
      "FOLD: 5, EPOCH: 46, valid_loss: 0.015928623911279898\n",
      "FOLD: 5, EPOCH: 47, train_loss: 0.015943214033000374\n",
      "FOLD: 5, EPOCH: 47, valid_loss: 0.015939412280344047\n",
      "FOLD: 5, EPOCH: 48, train_loss: 0.015832608792226057\n",
      "FOLD: 5, EPOCH: 48, valid_loss: 0.015954730960612114\n",
      "FOLD: 5, EPOCH: 49, train_loss: 0.015756286315720628\n",
      "FOLD: 5, EPOCH: 49, valid_loss: 0.01598914325810396\n",
      "FOLD: 6, EPOCH: 0, train_loss: 0.04410877327963307\n",
      "FOLD: 6, EPOCH: 0, valid_loss: 0.02099211743244758\n",
      "FOLD: 6, EPOCH: 1, train_loss: 0.022734682251875464\n",
      "FOLD: 6, EPOCH: 1, valid_loss: 0.018694862293509338\n",
      "FOLD: 6, EPOCH: 2, train_loss: 0.021845169189209875\n",
      "FOLD: 6, EPOCH: 2, valid_loss: 0.01787840059170356\n",
      "FOLD: 6, EPOCH: 3, train_loss: 0.021284071602732747\n",
      "FOLD: 6, EPOCH: 3, valid_loss: 0.017608497220163163\n",
      "FOLD: 6, EPOCH: 4, train_loss: 0.020966505631804466\n",
      "FOLD: 6, EPOCH: 4, valid_loss: 0.017333422429286517\n",
      "FOLD: 6, EPOCH: 5, train_loss: 0.02075476775801665\n",
      "FOLD: 6, EPOCH: 5, valid_loss: 0.01711397675367502\n",
      "FOLD: 6, EPOCH: 6, train_loss: 0.02050080517860683\n",
      "FOLD: 6, EPOCH: 6, valid_loss: 0.016941579130406562\n",
      "FOLD: 6, EPOCH: 7, train_loss: 0.020333584926619724\n",
      "FOLD: 6, EPOCH: 7, valid_loss: 0.016718084064240638\n",
      "FOLD: 6, EPOCH: 8, train_loss: 0.020156574133481528\n",
      "FOLD: 6, EPOCH: 8, valid_loss: 0.016661288145069893\n",
      "FOLD: 6, EPOCH: 9, train_loss: 0.019992052980170056\n",
      "FOLD: 6, EPOCH: 9, valid_loss: 0.01652684535544652\n",
      "FOLD: 6, EPOCH: 10, train_loss: 0.019873666199477943\n",
      "FOLD: 6, EPOCH: 10, valid_loss: 0.01645951123478321\n",
      "FOLD: 6, EPOCH: 11, train_loss: 0.019754895588030685\n",
      "FOLD: 6, EPOCH: 11, valid_loss: 0.016471154128129665\n",
      "FOLD: 6, EPOCH: 12, train_loss: 0.01962512598146458\n",
      "FOLD: 6, EPOCH: 12, valid_loss: 0.016361592552409723\n",
      "FOLD: 6, EPOCH: 13, train_loss: 0.01944750940074792\n",
      "FOLD: 6, EPOCH: 13, valid_loss: 0.01624803374019953\n",
      "FOLD: 6, EPOCH: 14, train_loss: 0.019310930819326156\n",
      "FOLD: 6, EPOCH: 14, valid_loss: 0.016277792720267407\n",
      "FOLD: 6, EPOCH: 15, train_loss: 0.01920409080245205\n",
      "FOLD: 6, EPOCH: 15, valid_loss: 0.01614992915151211\n",
      "FOLD: 6, EPOCH: 16, train_loss: 0.019076594654974098\n",
      "FOLD: 6, EPOCH: 16, valid_loss: 0.016075415536761284\n",
      "FOLD: 6, EPOCH: 17, train_loss: 0.01895411086042185\n",
      "FOLD: 6, EPOCH: 17, valid_loss: 0.016045662502829846\n",
      "FOLD: 6, EPOCH: 18, train_loss: 0.018902935993832512\n",
      "FOLD: 6, EPOCH: 18, valid_loss: 0.01599892107053445\n",
      "FOLD: 6, EPOCH: 19, train_loss: 0.018752382413761037\n",
      "FOLD: 6, EPOCH: 19, valid_loss: 0.01597940054937051\n",
      "FOLD: 6, EPOCH: 20, train_loss: 0.01866441237664706\n",
      "FOLD: 6, EPOCH: 20, valid_loss: 0.015929512966137666\n",
      "FOLD: 6, EPOCH: 21, train_loss: 0.01858856964453652\n",
      "FOLD: 6, EPOCH: 21, valid_loss: 0.015873250264960986\n",
      "FOLD: 6, EPOCH: 22, train_loss: 0.018467577048451513\n",
      "FOLD: 6, EPOCH: 22, valid_loss: 0.015870763848607358\n",
      "FOLD: 6, EPOCH: 23, train_loss: 0.01834059290185168\n",
      "FOLD: 6, EPOCH: 23, valid_loss: 0.01583216883815252\n",
      "FOLD: 6, EPOCH: 24, train_loss: 0.01823332455210589\n",
      "FOLD: 6, EPOCH: 24, valid_loss: 0.01577900514866297\n",
      "FOLD: 6, EPOCH: 25, train_loss: 0.01810449004374646\n",
      "FOLD: 6, EPOCH: 25, valid_loss: 0.015778719877394345\n",
      "FOLD: 6, EPOCH: 26, train_loss: 0.01798341312521213\n",
      "FOLD: 6, EPOCH: 26, valid_loss: 0.015723863401665136\n",
      "FOLD: 6, EPOCH: 27, train_loss: 0.01792387810309191\n",
      "FOLD: 6, EPOCH: 27, valid_loss: 0.015746242366731167\n",
      "FOLD: 6, EPOCH: 28, train_loss: 0.01783359717779063\n",
      "FOLD: 6, EPOCH: 28, valid_loss: 0.015706931670697834\n",
      "FOLD: 6, EPOCH: 29, train_loss: 0.01767651704014153\n",
      "FOLD: 6, EPOCH: 29, valid_loss: 0.01571532066624898\n",
      "FOLD: 6, EPOCH: 30, train_loss: 0.017600236951398687\n",
      "FOLD: 6, EPOCH: 30, valid_loss: 0.015752119442018177\n",
      "FOLD: 6, EPOCH: 31, train_loss: 0.017530547266171592\n",
      "FOLD: 6, EPOCH: 31, valid_loss: 0.0157265907440048\n",
      "FOLD: 6, EPOCH: 32, train_loss: 0.0174155097731666\n",
      "FOLD: 6, EPOCH: 32, valid_loss: 0.015707341595911063\n",
      "FOLD: 6, EPOCH: 33, train_loss: 0.017274640097811416\n",
      "FOLD: 6, EPOCH: 33, valid_loss: 0.01567002937484246\n",
      "FOLD: 6, EPOCH: 34, train_loss: 0.017155364029914945\n",
      "FOLD: 6, EPOCH: 34, valid_loss: 0.015625873365654394\n",
      "FOLD: 6, EPOCH: 35, train_loss: 0.01708574716404483\n",
      "FOLD: 6, EPOCH: 35, valid_loss: 0.015647744186795674\n",
      "FOLD: 6, EPOCH: 36, train_loss: 0.016971166925253096\n",
      "FOLD: 6, EPOCH: 36, valid_loss: 0.015638174345860116\n",
      "FOLD: 6, EPOCH: 37, train_loss: 0.016876571522270505\n",
      "FOLD: 6, EPOCH: 37, valid_loss: 0.015674282080278948\n",
      "FOLD: 6, EPOCH: 38, train_loss: 0.01677944789313384\n",
      "FOLD: 6, EPOCH: 38, valid_loss: 0.015621079346881462\n",
      "FOLD: 6, EPOCH: 39, train_loss: 0.016703579839117622\n",
      "FOLD: 6, EPOCH: 39, valid_loss: 0.015673261852218554\n",
      "FOLD: 6, EPOCH: 40, train_loss: 0.016580869078736852\n",
      "FOLD: 6, EPOCH: 40, valid_loss: 0.015653781019724332\n",
      "FOLD: 6, EPOCH: 41, train_loss: 0.016459050563138886\n",
      "FOLD: 6, EPOCH: 41, valid_loss: 0.015674446279612873\n",
      "FOLD: 6, EPOCH: 42, train_loss: 0.016359907512024447\n",
      "FOLD: 6, EPOCH: 42, valid_loss: 0.015649523872595567\n",
      "FOLD: 6, EPOCH: 43, train_loss: 0.01625615793808892\n",
      "FOLD: 6, EPOCH: 43, valid_loss: 0.0156908340465564\n",
      "FOLD: 6, EPOCH: 44, train_loss: 0.016105329882152176\n",
      "FOLD: 6, EPOCH: 44, valid_loss: 0.015658673973610766\n",
      "FOLD: 6, EPOCH: 45, train_loss: 0.016116859403917112\n",
      "FOLD: 6, EPOCH: 45, valid_loss: 0.01564238750590728\n",
      "Epoch    46: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 6, EPOCH: 46, train_loss: 0.015898745783881563\n",
      "FOLD: 6, EPOCH: 46, valid_loss: 0.01564675891915193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 6, EPOCH: 47, train_loss: 0.015743235264577576\n",
      "FOLD: 6, EPOCH: 47, valid_loss: 0.01565410211109198\n",
      "FOLD: 6, EPOCH: 48, train_loss: 0.015642994863761438\n",
      "FOLD: 6, EPOCH: 48, valid_loss: 0.015672298363195017\n",
      "FOLD: 6, EPOCH: 49, train_loss: 0.015598701715872094\n",
      "FOLD: 6, EPOCH: 49, valid_loss: 0.015665337729912538\n",
      "FOLD: 0, EPOCH: 0, train_loss: 0.043967811088706996\n",
      "FOLD: 0, EPOCH: 0, valid_loss: 0.02026640552167709\n",
      "FOLD: 0, EPOCH: 1, train_loss: 0.022901997473594303\n",
      "FOLD: 0, EPOCH: 1, valid_loss: 0.018945199652360037\n",
      "FOLD: 0, EPOCH: 2, train_loss: 0.021934829425771494\n",
      "FOLD: 0, EPOCH: 2, valid_loss: 0.01829454403084058\n",
      "FOLD: 0, EPOCH: 3, train_loss: 0.021339312397144938\n",
      "FOLD: 0, EPOCH: 3, valid_loss: 0.017914365977048874\n",
      "FOLD: 0, EPOCH: 4, train_loss: 0.020942116861005087\n",
      "FOLD: 0, EPOCH: 4, valid_loss: 0.01751746738759371\n",
      "FOLD: 0, EPOCH: 5, train_loss: 0.020676881679006526\n",
      "FOLD: 0, EPOCH: 5, valid_loss: 0.017311551321584445\n",
      "FOLD: 0, EPOCH: 6, train_loss: 0.02050801195405625\n",
      "FOLD: 0, EPOCH: 6, valid_loss: 0.017556427548137996\n",
      "FOLD: 0, EPOCH: 7, train_loss: 0.020287940404503733\n",
      "FOLD: 0, EPOCH: 7, valid_loss: 0.016938551185795896\n",
      "FOLD: 0, EPOCH: 8, train_loss: 0.02013703906354872\n",
      "FOLD: 0, EPOCH: 8, valid_loss: 0.016903271038944904\n",
      "FOLD: 0, EPOCH: 9, train_loss: 0.01998790378707486\n",
      "FOLD: 0, EPOCH: 9, valid_loss: 0.016741465418957747\n",
      "FOLD: 0, EPOCH: 10, train_loss: 0.019797840583566075\n",
      "FOLD: 0, EPOCH: 10, valid_loss: 0.01667682468318022\n",
      "FOLD: 0, EPOCH: 11, train_loss: 0.019689393204611702\n",
      "FOLD: 0, EPOCH: 11, valid_loss: 0.016581974708690092\n",
      "FOLD: 0, EPOCH: 12, train_loss: 0.019537436851375812\n",
      "FOLD: 0, EPOCH: 12, valid_loss: 0.01647555584517809\n",
      "FOLD: 0, EPOCH: 13, train_loss: 0.019436204982166354\n",
      "FOLD: 0, EPOCH: 13, valid_loss: 0.016424026985007983\n",
      "FOLD: 0, EPOCH: 14, train_loss: 0.01923728990997817\n",
      "FOLD: 0, EPOCH: 14, valid_loss: 0.016305031899649363\n",
      "FOLD: 0, EPOCH: 15, train_loss: 0.01915693192465885\n",
      "FOLD: 0, EPOCH: 15, valid_loss: 0.0162004093424632\n",
      "FOLD: 0, EPOCH: 16, train_loss: 0.019004949595074396\n",
      "FOLD: 0, EPOCH: 16, valid_loss: 0.01617990484317908\n",
      "FOLD: 0, EPOCH: 17, train_loss: 0.018924642480104358\n",
      "FOLD: 0, EPOCH: 17, valid_loss: 0.01612782972649886\n",
      "FOLD: 0, EPOCH: 18, train_loss: 0.01885685042755024\n",
      "FOLD: 0, EPOCH: 18, valid_loss: 0.016090918260698136\n",
      "FOLD: 0, EPOCH: 19, train_loss: 0.01870002431442609\n",
      "FOLD: 0, EPOCH: 19, valid_loss: 0.01605719614487428\n",
      "FOLD: 0, EPOCH: 20, train_loss: 0.01857040701685725\n",
      "FOLD: 0, EPOCH: 20, valid_loss: 0.016083702659950808\n",
      "FOLD: 0, EPOCH: 21, train_loss: 0.018496999215032603\n",
      "FOLD: 0, EPOCH: 21, valid_loss: 0.016007587528572634\n",
      "FOLD: 0, EPOCH: 22, train_loss: 0.018374263258600556\n",
      "FOLD: 0, EPOCH: 22, valid_loss: 0.016010172521838777\n",
      "FOLD: 0, EPOCH: 23, train_loss: 0.018238491479408096\n",
      "FOLD: 0, EPOCH: 23, valid_loss: 0.016006270781732522\n",
      "FOLD: 0, EPOCH: 24, train_loss: 0.018224582076072693\n",
      "FOLD: 0, EPOCH: 24, valid_loss: 0.015969049686995838\n",
      "FOLD: 0, EPOCH: 25, train_loss: 0.018073886934969877\n",
      "FOLD: 0, EPOCH: 25, valid_loss: 0.015908651555386875\n",
      "FOLD: 0, EPOCH: 26, train_loss: 0.017947267866819293\n",
      "FOLD: 0, EPOCH: 26, valid_loss: 0.01588704788054411\n",
      "FOLD: 0, EPOCH: 27, train_loss: 0.01786103336190855\n",
      "FOLD: 0, EPOCH: 27, valid_loss: 0.015874619380785868\n",
      "FOLD: 0, EPOCH: 28, train_loss: 0.01773833211612057\n",
      "FOLD: 0, EPOCH: 28, valid_loss: 0.015858707662958365\n",
      "FOLD: 0, EPOCH: 29, train_loss: 0.017631034240932077\n",
      "FOLD: 0, EPOCH: 29, valid_loss: 0.0158293738674659\n",
      "FOLD: 0, EPOCH: 30, train_loss: 0.017526394687592983\n",
      "FOLD: 0, EPOCH: 30, valid_loss: 0.015828107340404622\n",
      "FOLD: 0, EPOCH: 31, train_loss: 0.01746073830872774\n",
      "FOLD: 0, EPOCH: 31, valid_loss: 0.015826789017480154\n",
      "FOLD: 0, EPOCH: 32, train_loss: 0.017277063361394243\n",
      "FOLD: 0, EPOCH: 32, valid_loss: 0.01586159082272878\n",
      "FOLD: 0, EPOCH: 33, train_loss: 0.017180594728907216\n",
      "FOLD: 0, EPOCH: 33, valid_loss: 0.015806313532476243\n",
      "FOLD: 0, EPOCH: 34, train_loss: 0.01706776111009153\n",
      "FOLD: 0, EPOCH: 34, valid_loss: 0.015782656362996653\n",
      "FOLD: 0, EPOCH: 35, train_loss: 0.01698346801001478\n",
      "FOLD: 0, EPOCH: 35, valid_loss: 0.01578068661575134\n",
      "FOLD: 0, EPOCH: 36, train_loss: 0.016884878844123433\n",
      "FOLD: 0, EPOCH: 36, valid_loss: 0.015811338017766293\n",
      "FOLD: 0, EPOCH: 37, train_loss: 0.016771372684554475\n",
      "FOLD: 0, EPOCH: 37, valid_loss: 0.015799969219817564\n",
      "FOLD: 0, EPOCH: 38, train_loss: 0.016647890323420633\n",
      "FOLD: 0, EPOCH: 38, valid_loss: 0.01576298740334236\n",
      "FOLD: 0, EPOCH: 39, train_loss: 0.016546659536559035\n",
      "FOLD: 0, EPOCH: 39, valid_loss: 0.01583929230960516\n",
      "FOLD: 0, EPOCH: 40, train_loss: 0.016469225564317125\n",
      "FOLD: 0, EPOCH: 40, valid_loss: 0.015805785759137228\n",
      "FOLD: 0, EPOCH: 41, train_loss: 0.01633611015623083\n",
      "FOLD: 0, EPOCH: 41, valid_loss: 0.015813843418772403\n",
      "FOLD: 0, EPOCH: 42, train_loss: 0.016272538442265342\n",
      "FOLD: 0, EPOCH: 42, valid_loss: 0.01581403211905406\n",
      "FOLD: 0, EPOCH: 43, train_loss: 0.016133529071167513\n",
      "FOLD: 0, EPOCH: 43, valid_loss: 0.01582816193023553\n",
      "FOLD: 0, EPOCH: 44, train_loss: 0.016080139368470456\n",
      "FOLD: 0, EPOCH: 44, valid_loss: 0.015858050650702074\n",
      "FOLD: 0, EPOCH: 45, train_loss: 0.015912433985520055\n",
      "FOLD: 0, EPOCH: 45, valid_loss: 0.01586598322655146\n",
      "Epoch    46: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 0, EPOCH: 46, train_loss: 0.01573673524969333\n",
      "FOLD: 0, EPOCH: 46, valid_loss: 0.015830635236432918\n",
      "FOLD: 0, EPOCH: 47, train_loss: 0.015623999356820777\n",
      "FOLD: 0, EPOCH: 47, valid_loss: 0.01582312533775201\n",
      "FOLD: 0, EPOCH: 48, train_loss: 0.015497948118561023\n",
      "FOLD: 0, EPOCH: 48, valid_loss: 0.01583373353171807\n",
      "FOLD: 0, EPOCH: 49, train_loss: 0.015441109715784723\n",
      "FOLD: 0, EPOCH: 49, valid_loss: 0.015837281154325374\n",
      "FOLD: 1, EPOCH: 0, train_loss: 0.04235252954468534\n",
      "FOLD: 1, EPOCH: 0, valid_loss: 0.020657250514397256\n",
      "FOLD: 1, EPOCH: 1, train_loss: 0.02273635148397974\n",
      "FOLD: 1, EPOCH: 1, valid_loss: 0.018713739342414416\n",
      "FOLD: 1, EPOCH: 2, train_loss: 0.021846461688747276\n",
      "FOLD: 1, EPOCH: 2, valid_loss: 0.01795942006775966\n",
      "FOLD: 1, EPOCH: 3, train_loss: 0.021335144728623533\n",
      "FOLD: 1, EPOCH: 3, valid_loss: 0.01748081683539427\n",
      "FOLD: 1, EPOCH: 4, train_loss: 0.02099939503681821\n",
      "FOLD: 1, EPOCH: 4, valid_loss: 0.01720835755650814\n",
      "FOLD: 1, EPOCH: 5, train_loss: 0.020682510089229892\n",
      "FOLD: 1, EPOCH: 5, valid_loss: 0.01703665376855777\n",
      "FOLD: 1, EPOCH: 6, train_loss: 0.020531790079297247\n",
      "FOLD: 1, EPOCH: 6, valid_loss: 0.016995176672935486\n",
      "FOLD: 1, EPOCH: 7, train_loss: 0.02032469351448723\n",
      "FOLD: 1, EPOCH: 7, valid_loss: 0.016716542868660048\n",
      "FOLD: 1, EPOCH: 8, train_loss: 0.02015639277728828\n",
      "FOLD: 1, EPOCH: 8, valid_loss: 0.016703680157661438\n",
      "FOLD: 1, EPOCH: 9, train_loss: 0.020030269348943554\n",
      "FOLD: 1, EPOCH: 9, valid_loss: 0.016524643565599736\n",
      "FOLD: 1, EPOCH: 10, train_loss: 0.019842472028087924\n",
      "FOLD: 1, EPOCH: 10, valid_loss: 0.01641280411814268\n",
      "FOLD: 1, EPOCH: 11, train_loss: 0.019683315997590888\n",
      "FOLD: 1, EPOCH: 11, valid_loss: 0.016289846398509465\n",
      "FOLD: 1, EPOCH: 12, train_loss: 0.019570548062187595\n",
      "FOLD: 1, EPOCH: 12, valid_loss: 0.016262761173913114\n",
      "FOLD: 1, EPOCH: 13, train_loss: 0.019396332444975507\n",
      "FOLD: 1, EPOCH: 13, valid_loss: 0.0161871500313282\n",
      "FOLD: 1, EPOCH: 14, train_loss: 0.019307297855816984\n",
      "FOLD: 1, EPOCH: 14, valid_loss: 0.01608311735953276\n",
      "FOLD: 1, EPOCH: 15, train_loss: 0.019187503720860224\n",
      "FOLD: 1, EPOCH: 15, valid_loss: 0.016003779852046415\n",
      "FOLD: 1, EPOCH: 16, train_loss: 0.019072671672581015\n",
      "FOLD: 1, EPOCH: 16, valid_loss: 0.015967813678658925\n",
      "FOLD: 1, EPOCH: 17, train_loss: 0.018957172394604295\n",
      "FOLD: 1, EPOCH: 17, valid_loss: 0.01591632658472428\n",
      "FOLD: 1, EPOCH: 18, train_loss: 0.01886261408091397\n",
      "FOLD: 1, EPOCH: 18, valid_loss: 0.01592556372858011\n",
      "FOLD: 1, EPOCH: 19, train_loss: 0.018737932792990596\n",
      "FOLD: 1, EPOCH: 19, valid_loss: 0.015841630932230216\n",
      "FOLD: 1, EPOCH: 20, train_loss: 0.01860424327487881\n",
      "FOLD: 1, EPOCH: 20, valid_loss: 0.01577557960095314\n",
      "FOLD: 1, EPOCH: 21, train_loss: 0.01854955465406985\n",
      "FOLD: 1, EPOCH: 21, valid_loss: 0.015838465868280485\n",
      "FOLD: 1, EPOCH: 22, train_loss: 0.018463316247672647\n",
      "FOLD: 1, EPOCH: 22, valid_loss: 0.01576246306873285\n",
      "FOLD: 1, EPOCH: 23, train_loss: 0.01832725842659538\n",
      "FOLD: 1, EPOCH: 23, valid_loss: 0.01577067754876155\n",
      "FOLD: 1, EPOCH: 24, train_loss: 0.018197659126206023\n",
      "FOLD: 1, EPOCH: 24, valid_loss: 0.01576738417721712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 1, EPOCH: 25, train_loss: 0.018058400424952444\n",
      "FOLD: 1, EPOCH: 25, valid_loss: 0.015726066409395292\n",
      "FOLD: 1, EPOCH: 26, train_loss: 0.017988257462511193\n",
      "FOLD: 1, EPOCH: 26, valid_loss: 0.015704805676180583\n",
      "FOLD: 1, EPOCH: 27, train_loss: 0.017886939458549023\n",
      "FOLD: 1, EPOCH: 27, valid_loss: 0.015672199714642305\n",
      "FOLD: 1, EPOCH: 28, train_loss: 0.017783876030227623\n",
      "FOLD: 1, EPOCH: 28, valid_loss: 0.015632070386065886\n",
      "FOLD: 1, EPOCH: 29, train_loss: 0.01768762671162148\n",
      "FOLD: 1, EPOCH: 29, valid_loss: 0.015621637567304648\n",
      "FOLD: 1, EPOCH: 30, train_loss: 0.017562359382675308\n",
      "FOLD: 1, EPOCH: 30, valid_loss: 0.015606552434082214\n",
      "FOLD: 1, EPOCH: 31, train_loss: 0.017452847791483272\n",
      "FOLD: 1, EPOCH: 31, valid_loss: 0.015613885524754342\n",
      "FOLD: 1, EPOCH: 32, train_loss: 0.017385322223039897\n",
      "FOLD: 1, EPOCH: 32, valid_loss: 0.01560141726468618\n",
      "FOLD: 1, EPOCH: 33, train_loss: 0.01732496903403788\n",
      "FOLD: 1, EPOCH: 33, valid_loss: 0.015575916721270634\n",
      "FOLD: 1, EPOCH: 34, train_loss: 0.017156003219251696\n",
      "FOLD: 1, EPOCH: 34, valid_loss: 0.015578806615219666\n",
      "FOLD: 1, EPOCH: 35, train_loss: 0.0170469437725842\n",
      "FOLD: 1, EPOCH: 35, valid_loss: 0.015602215479772825\n",
      "FOLD: 1, EPOCH: 36, train_loss: 0.016957637488942693\n",
      "FOLD: 1, EPOCH: 36, valid_loss: 0.015547277119297247\n",
      "FOLD: 1, EPOCH: 37, train_loss: 0.016856780187604396\n",
      "FOLD: 1, EPOCH: 37, valid_loss: 0.015567613335756155\n",
      "FOLD: 1, EPOCH: 38, train_loss: 0.01673648307553014\n",
      "FOLD: 1, EPOCH: 38, valid_loss: 0.015528986158852395\n",
      "FOLD: 1, EPOCH: 39, train_loss: 0.016592029318515514\n",
      "FOLD: 1, EPOCH: 39, valid_loss: 0.0155809442870892\n",
      "FOLD: 1, EPOCH: 40, train_loss: 0.01649946888052934\n",
      "FOLD: 1, EPOCH: 40, valid_loss: 0.015553603808467206\n",
      "FOLD: 1, EPOCH: 41, train_loss: 0.01641196244069048\n",
      "FOLD: 1, EPOCH: 41, valid_loss: 0.01553718100946683\n",
      "FOLD: 1, EPOCH: 42, train_loss: 0.016321073991019983\n",
      "FOLD: 1, EPOCH: 42, valid_loss: 0.015582919407349367\n",
      "FOLD: 1, EPOCH: 43, train_loss: 0.016248237731791026\n",
      "FOLD: 1, EPOCH: 43, valid_loss: 0.015556252633149806\n",
      "FOLD: 1, EPOCH: 44, train_loss: 0.016152244766016264\n",
      "FOLD: 1, EPOCH: 44, valid_loss: 0.015565919188352732\n",
      "FOLD: 1, EPOCH: 45, train_loss: 0.016046308836824185\n",
      "FOLD: 1, EPOCH: 45, valid_loss: 0.015558411438877765\n",
      "Epoch    46: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 1, EPOCH: 46, train_loss: 0.015839454088662122\n",
      "FOLD: 1, EPOCH: 46, valid_loss: 0.015516019283005824\n",
      "FOLD: 1, EPOCH: 47, train_loss: 0.015702750445721118\n",
      "FOLD: 1, EPOCH: 47, valid_loss: 0.015534508901719864\n",
      "FOLD: 1, EPOCH: 48, train_loss: 0.015585581621004117\n",
      "FOLD: 1, EPOCH: 48, valid_loss: 0.015525422918681916\n",
      "FOLD: 1, EPOCH: 49, train_loss: 0.015500265651860752\n",
      "FOLD: 1, EPOCH: 49, valid_loss: 0.01549440106520286\n",
      "FOLD: 2, EPOCH: 0, train_loss: 0.04637563427457133\n",
      "FOLD: 2, EPOCH: 0, valid_loss: 0.02063819508139904\n",
      "FOLD: 2, EPOCH: 1, train_loss: 0.022890216670930386\n",
      "FOLD: 2, EPOCH: 1, valid_loss: 0.01927495260651295\n",
      "FOLD: 2, EPOCH: 2, train_loss: 0.021935639626069647\n",
      "FOLD: 2, EPOCH: 2, valid_loss: 0.018240592800653897\n",
      "FOLD: 2, EPOCH: 3, train_loss: 0.02134801759510427\n",
      "FOLD: 2, EPOCH: 3, valid_loss: 0.017677278257906437\n",
      "FOLD: 2, EPOCH: 4, train_loss: 0.021071695596785157\n",
      "FOLD: 2, EPOCH: 4, valid_loss: 0.01738434607306352\n",
      "FOLD: 2, EPOCH: 5, train_loss: 0.020748137949487648\n",
      "FOLD: 2, EPOCH: 5, valid_loss: 0.017125004186080053\n",
      "FOLD: 2, EPOCH: 6, train_loss: 0.020545172630935103\n",
      "FOLD: 2, EPOCH: 6, valid_loss: 0.017005625180900097\n",
      "FOLD: 2, EPOCH: 7, train_loss: 0.02035939449293388\n",
      "FOLD: 2, EPOCH: 7, valid_loss: 0.016937539626199465\n",
      "FOLD: 2, EPOCH: 8, train_loss: 0.02017569720644403\n",
      "FOLD: 2, EPOCH: 8, valid_loss: 0.016862609710257787\n",
      "FOLD: 2, EPOCH: 9, train_loss: 0.02007037806450515\n",
      "FOLD: 2, EPOCH: 9, valid_loss: 0.016713760005166896\n",
      "FOLD: 2, EPOCH: 10, train_loss: 0.019901516570432765\n",
      "FOLD: 2, EPOCH: 10, valid_loss: 0.01660327720814026\n",
      "FOLD: 2, EPOCH: 11, train_loss: 0.019769159145653248\n",
      "FOLD: 2, EPOCH: 11, valid_loss: 0.016523951592926796\n",
      "FOLD: 2, EPOCH: 12, train_loss: 0.019651568117173942\n",
      "FOLD: 2, EPOCH: 12, valid_loss: 0.016392026741344195\n",
      "FOLD: 2, EPOCH: 13, train_loss: 0.019526227201158937\n",
      "FOLD: 2, EPOCH: 13, valid_loss: 0.016307011891442996\n",
      "FOLD: 2, EPOCH: 14, train_loss: 0.019395367745813484\n",
      "FOLD: 2, EPOCH: 14, valid_loss: 0.01627668093603391\n",
      "FOLD: 2, EPOCH: 15, train_loss: 0.01925348543335457\n",
      "FOLD: 2, EPOCH: 15, valid_loss: 0.016212590038776398\n",
      "FOLD: 2, EPOCH: 16, train_loss: 0.01916073267725674\n",
      "FOLD: 2, EPOCH: 16, valid_loss: 0.01614629326818081\n",
      "FOLD: 2, EPOCH: 17, train_loss: 0.019025267126995163\n",
      "FOLD: 2, EPOCH: 17, valid_loss: 0.01604425312521366\n",
      "FOLD: 2, EPOCH: 18, train_loss: 0.018911187964919453\n",
      "FOLD: 2, EPOCH: 18, valid_loss: 0.016079778424822368\n",
      "FOLD: 2, EPOCH: 19, train_loss: 0.01881755963974708\n",
      "FOLD: 2, EPOCH: 19, valid_loss: 0.016041560313449457\n",
      "FOLD: 2, EPOCH: 20, train_loss: 0.018712691383788716\n",
      "FOLD: 2, EPOCH: 20, valid_loss: 0.015961206159912623\n",
      "FOLD: 2, EPOCH: 21, train_loss: 0.01863414222827634\n",
      "FOLD: 2, EPOCH: 21, valid_loss: 0.015939203807367727\n",
      "FOLD: 2, EPOCH: 22, train_loss: 0.018532546337794612\n",
      "FOLD: 2, EPOCH: 22, valid_loss: 0.01593700244736213\n",
      "FOLD: 2, EPOCH: 23, train_loss: 0.01840270610173812\n",
      "FOLD: 2, EPOCH: 23, valid_loss: 0.015861593258495513\n",
      "FOLD: 2, EPOCH: 24, train_loss: 0.01831756265380898\n",
      "FOLD: 2, EPOCH: 24, valid_loss: 0.015846034082082603\n",
      "FOLD: 2, EPOCH: 25, train_loss: 0.018237283780566743\n",
      "FOLD: 2, EPOCH: 25, valid_loss: 0.015798718238679264\n",
      "FOLD: 2, EPOCH: 26, train_loss: 0.018167664045216265\n",
      "FOLD: 2, EPOCH: 26, valid_loss: 0.015832768108409185\n",
      "FOLD: 2, EPOCH: 27, train_loss: 0.01800068571056063\n",
      "FOLD: 2, EPOCH: 27, valid_loss: 0.015760215930640697\n",
      "FOLD: 2, EPOCH: 28, train_loss: 0.01794031962148241\n",
      "FOLD: 2, EPOCH: 28, valid_loss: 0.015722813944403943\n",
      "FOLD: 2, EPOCH: 29, train_loss: 0.017813863446684303\n",
      "FOLD: 2, EPOCH: 29, valid_loss: 0.01572449405032855\n",
      "FOLD: 2, EPOCH: 30, train_loss: 0.01774030438045392\n",
      "FOLD: 2, EPOCH: 30, valid_loss: 0.0157524929023706\n",
      "FOLD: 2, EPOCH: 31, train_loss: 0.017635819477003975\n",
      "FOLD: 2, EPOCH: 31, valid_loss: 0.015719746740964744\n",
      "FOLD: 2, EPOCH: 32, train_loss: 0.01751505908229061\n",
      "FOLD: 2, EPOCH: 32, valid_loss: 0.015699230779248934\n",
      "FOLD: 2, EPOCH: 33, train_loss: 0.017436234947495365\n",
      "FOLD: 2, EPOCH: 33, valid_loss: 0.015707281561425097\n",
      "FOLD: 2, EPOCH: 34, train_loss: 0.01733862773540455\n",
      "FOLD: 2, EPOCH: 34, valid_loss: 0.015702606823581915\n",
      "FOLD: 2, EPOCH: 35, train_loss: 0.01724810920957778\n",
      "FOLD: 2, EPOCH: 35, valid_loss: 0.01569972839206457\n",
      "FOLD: 2, EPOCH: 36, train_loss: 0.01713929558172822\n",
      "FOLD: 2, EPOCH: 36, valid_loss: 0.015680066524789885\n",
      "FOLD: 2, EPOCH: 37, train_loss: 0.017002270708011614\n",
      "FOLD: 2, EPOCH: 37, valid_loss: 0.015703889827888746\n",
      "FOLD: 2, EPOCH: 38, train_loss: 0.016946892103029264\n",
      "FOLD: 2, EPOCH: 38, valid_loss: 0.015687075945047233\n",
      "FOLD: 2, EPOCH: 39, train_loss: 0.01684823333965363\n",
      "FOLD: 2, EPOCH: 39, valid_loss: 0.01570346550299571\n",
      "FOLD: 2, EPOCH: 40, train_loss: 0.01675062790210988\n",
      "FOLD: 2, EPOCH: 40, valid_loss: 0.015674235442510016\n",
      "FOLD: 2, EPOCH: 41, train_loss: 0.01668241369613522\n",
      "FOLD: 2, EPOCH: 41, valid_loss: 0.01569592594527281\n",
      "FOLD: 2, EPOCH: 42, train_loss: 0.01651633545957707\n",
      "FOLD: 2, EPOCH: 42, valid_loss: 0.015683537993866663\n",
      "FOLD: 2, EPOCH: 43, train_loss: 0.016451430871982028\n",
      "FOLD: 2, EPOCH: 43, valid_loss: 0.015658443936934836\n",
      "FOLD: 2, EPOCH: 44, train_loss: 0.016375102329294424\n",
      "FOLD: 2, EPOCH: 44, valid_loss: 0.01567663152057391\n",
      "FOLD: 2, EPOCH: 45, train_loss: 0.016311739382610935\n",
      "FOLD: 2, EPOCH: 45, valid_loss: 0.01567168727230567\n",
      "FOLD: 2, EPOCH: 46, train_loss: 0.016131801090228395\n",
      "FOLD: 2, EPOCH: 46, valid_loss: 0.01567051624162839\n",
      "FOLD: 2, EPOCH: 47, train_loss: 0.016013450115113646\n",
      "FOLD: 2, EPOCH: 47, valid_loss: 0.01568451337516308\n",
      "FOLD: 2, EPOCH: 48, train_loss: 0.01592067552327707\n",
      "FOLD: 2, EPOCH: 48, valid_loss: 0.01569575587144265\n",
      "FOLD: 2, EPOCH: 49, train_loss: 0.015879124755392205\n",
      "FOLD: 2, EPOCH: 49, valid_loss: 0.015702999340227015\n",
      "FOLD: 3, EPOCH: 0, train_loss: 0.043820227194275405\n",
      "FOLD: 3, EPOCH: 0, valid_loss: 0.020288885212861575\n",
      "FOLD: 3, EPOCH: 1, train_loss: 0.0228076367261442\n",
      "FOLD: 3, EPOCH: 1, valid_loss: 0.018946581448499974\n",
      "FOLD: 3, EPOCH: 2, train_loss: 0.021790855035588547\n",
      "FOLD: 3, EPOCH: 2, valid_loss: 0.01808651450734872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 3, EPOCH: 3, train_loss: 0.021275768182366282\n",
      "FOLD: 3, EPOCH: 3, valid_loss: 0.01778358665223305\n",
      "FOLD: 3, EPOCH: 4, train_loss: 0.020918725184290797\n",
      "FOLD: 3, EPOCH: 4, valid_loss: 0.017418536357581615\n",
      "FOLD: 3, EPOCH: 5, train_loss: 0.02064981311559677\n",
      "FOLD: 3, EPOCH: 5, valid_loss: 0.017311836664493267\n",
      "FOLD: 3, EPOCH: 6, train_loss: 0.020444920362048858\n",
      "FOLD: 3, EPOCH: 6, valid_loss: 0.01721440112361541\n",
      "FOLD: 3, EPOCH: 7, train_loss: 0.02029644446195783\n",
      "FOLD: 3, EPOCH: 7, valid_loss: 0.016952629989156358\n",
      "FOLD: 3, EPOCH: 8, train_loss: 0.020106138334282347\n",
      "FOLD: 3, EPOCH: 8, valid_loss: 0.016870733851996753\n",
      "FOLD: 3, EPOCH: 9, train_loss: 0.01998839313416062\n",
      "FOLD: 3, EPOCH: 9, valid_loss: 0.016750455762331303\n",
      "FOLD: 3, EPOCH: 10, train_loss: 0.019778782350791467\n",
      "FOLD: 3, EPOCH: 10, valid_loss: 0.016749982363902606\n",
      "FOLD: 3, EPOCH: 11, train_loss: 0.01966955462420309\n",
      "FOLD: 3, EPOCH: 11, valid_loss: 0.01656205500834263\n",
      "FOLD: 3, EPOCH: 12, train_loss: 0.019556278539066378\n",
      "FOLD: 3, EPOCH: 12, valid_loss: 0.016536041377828672\n",
      "FOLD: 3, EPOCH: 13, train_loss: 0.019381584750639426\n",
      "FOLD: 3, EPOCH: 13, valid_loss: 0.016429693796313725\n",
      "FOLD: 3, EPOCH: 14, train_loss: 0.01929297371188531\n",
      "FOLD: 3, EPOCH: 14, valid_loss: 0.01641437619064863\n",
      "FOLD: 3, EPOCH: 15, train_loss: 0.01916898661167235\n",
      "FOLD: 3, EPOCH: 15, valid_loss: 0.016400316300300453\n",
      "FOLD: 3, EPOCH: 16, train_loss: 0.019070691378736817\n",
      "FOLD: 3, EPOCH: 16, valid_loss: 0.016279662529436443\n",
      "FOLD: 3, EPOCH: 17, train_loss: 0.018947044991560885\n",
      "FOLD: 3, EPOCH: 17, valid_loss: 0.016282665399977796\n",
      "FOLD: 3, EPOCH: 18, train_loss: 0.01885026502045425\n",
      "FOLD: 3, EPOCH: 18, valid_loss: 0.016239203799229402\n",
      "FOLD: 3, EPOCH: 19, train_loss: 0.018764219322317355\n",
      "FOLD: 3, EPOCH: 19, valid_loss: 0.01614952631867849\n",
      "FOLD: 3, EPOCH: 20, train_loss: 0.018595684646956018\n",
      "FOLD: 3, EPOCH: 20, valid_loss: 0.016164406990775697\n",
      "FOLD: 3, EPOCH: 21, train_loss: 0.01850804515384339\n",
      "FOLD: 3, EPOCH: 21, valid_loss: 0.01611133894095054\n",
      "FOLD: 3, EPOCH: 22, train_loss: 0.018362827422852453\n",
      "FOLD: 3, EPOCH: 22, valid_loss: 0.01607206664406336\n",
      "FOLD: 3, EPOCH: 23, train_loss: 0.018336881455537433\n",
      "FOLD: 3, EPOCH: 23, valid_loss: 0.016112645371602133\n",
      "FOLD: 3, EPOCH: 24, train_loss: 0.018237487060596812\n",
      "FOLD: 3, EPOCH: 24, valid_loss: 0.016041285859850738\n",
      "FOLD: 3, EPOCH: 25, train_loss: 0.018115405421200638\n",
      "FOLD: 3, EPOCH: 25, valid_loss: 0.016028386898911916\n",
      "FOLD: 3, EPOCH: 26, train_loss: 0.018001462911834586\n",
      "FOLD: 3, EPOCH: 26, valid_loss: 0.015962241002573416\n",
      "FOLD: 3, EPOCH: 27, train_loss: 0.017931047738907305\n",
      "FOLD: 3, EPOCH: 27, valid_loss: 0.015967184462799475\n",
      "FOLD: 3, EPOCH: 28, train_loss: 0.017847718293401035\n",
      "FOLD: 3, EPOCH: 28, valid_loss: 0.015921702321905356\n",
      "FOLD: 3, EPOCH: 29, train_loss: 0.017704420571995748\n",
      "FOLD: 3, EPOCH: 29, valid_loss: 0.015909772366285324\n",
      "FOLD: 3, EPOCH: 30, train_loss: 0.01758400330672393\n",
      "FOLD: 3, EPOCH: 30, valid_loss: 0.016019894025073603\n",
      "FOLD: 3, EPOCH: 31, train_loss: 0.017536887269769166\n",
      "FOLD: 3, EPOCH: 31, valid_loss: 0.01592591505211133\n",
      "FOLD: 3, EPOCH: 32, train_loss: 0.017426364048308617\n",
      "FOLD: 3, EPOCH: 32, valid_loss: 0.015872428551889382\n",
      "FOLD: 3, EPOCH: 33, train_loss: 0.017290740198380238\n",
      "FOLD: 3, EPOCH: 33, valid_loss: 0.015899464058188293\n",
      "FOLD: 3, EPOCH: 34, train_loss: 0.017187192721443402\n",
      "FOLD: 3, EPOCH: 34, valid_loss: 0.015861060183781844\n",
      "FOLD: 3, EPOCH: 35, train_loss: 0.0171530926725953\n",
      "FOLD: 3, EPOCH: 35, valid_loss: 0.015878561239403028\n",
      "FOLD: 3, EPOCH: 36, train_loss: 0.017018926418008836\n",
      "FOLD: 3, EPOCH: 36, valid_loss: 0.015872062613757756\n",
      "FOLD: 3, EPOCH: 37, train_loss: 0.01690140541491879\n",
      "FOLD: 3, EPOCH: 37, valid_loss: 0.015854432820700683\n",
      "FOLD: 3, EPOCH: 38, train_loss: 0.016822370486585674\n",
      "FOLD: 3, EPOCH: 38, valid_loss: 0.015877537787533723\n",
      "FOLD: 3, EPOCH: 39, train_loss: 0.01669967159427501\n",
      "FOLD: 3, EPOCH: 39, valid_loss: 0.015837187234025735\n",
      "FOLD: 3, EPOCH: 40, train_loss: 0.01662121937182304\n",
      "FOLD: 3, EPOCH: 40, valid_loss: 0.015898893300730448\n",
      "FOLD: 3, EPOCH: 41, train_loss: 0.016553836031439336\n",
      "FOLD: 3, EPOCH: 41, valid_loss: 0.015904350421176508\n",
      "FOLD: 3, EPOCH: 42, train_loss: 0.01644870694223288\n",
      "FOLD: 3, EPOCH: 42, valid_loss: 0.015877431760040615\n",
      "FOLD: 3, EPOCH: 43, train_loss: 0.01629482622484903\n",
      "FOLD: 3, EPOCH: 43, valid_loss: 0.015868222197661035\n",
      "FOLD: 3, EPOCH: 44, train_loss: 0.016198358775393384\n",
      "FOLD: 3, EPOCH: 44, valid_loss: 0.015909749656342544\n",
      "FOLD: 3, EPOCH: 45, train_loss: 0.016067391173360316\n",
      "FOLD: 3, EPOCH: 45, valid_loss: 0.015867905404705267\n",
      "FOLD: 3, EPOCH: 46, train_loss: 0.016006221592023567\n",
      "FOLD: 3, EPOCH: 46, valid_loss: 0.015874946060088966\n",
      "Epoch    47: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 3, EPOCH: 47, train_loss: 0.015779467387678655\n",
      "FOLD: 3, EPOCH: 47, valid_loss: 0.01584034298474972\n",
      "FOLD: 3, EPOCH: 48, train_loss: 0.01570812671923557\n",
      "FOLD: 3, EPOCH: 48, valid_loss: 0.015825648720447835\n",
      "FOLD: 3, EPOCH: 49, train_loss: 0.015619535980796491\n",
      "FOLD: 3, EPOCH: 49, valid_loss: 0.01587718015966507\n",
      "FOLD: 4, EPOCH: 0, train_loss: 0.04300626228890709\n",
      "FOLD: 4, EPOCH: 0, valid_loss: 0.020243730539312728\n",
      "FOLD: 4, EPOCH: 1, train_loss: 0.022748862292516877\n",
      "FOLD: 4, EPOCH: 1, valid_loss: 0.019006474659993097\n",
      "FOLD: 4, EPOCH: 2, train_loss: 0.021891532365132024\n",
      "FOLD: 4, EPOCH: 2, valid_loss: 0.018542889935465958\n",
      "FOLD: 4, EPOCH: 3, train_loss: 0.021342134707280108\n",
      "FOLD: 4, EPOCH: 3, valid_loss: 0.017873445669045813\n",
      "FOLD: 4, EPOCH: 4, train_loss: 0.02095581352005939\n",
      "FOLD: 4, EPOCH: 4, valid_loss: 0.017616027822861306\n",
      "FOLD: 4, EPOCH: 5, train_loss: 0.020713038717371388\n",
      "FOLD: 4, EPOCH: 5, valid_loss: 0.017487715786466233\n",
      "FOLD: 4, EPOCH: 6, train_loss: 0.020458596758544445\n",
      "FOLD: 4, EPOCH: 6, valid_loss: 0.017241752634827908\n",
      "FOLD: 4, EPOCH: 7, train_loss: 0.020316064231903165\n",
      "FOLD: 4, EPOCH: 7, valid_loss: 0.017202053792201556\n",
      "FOLD: 4, EPOCH: 8, train_loss: 0.0201049285714288\n",
      "FOLD: 4, EPOCH: 8, valid_loss: 0.01697410413852105\n",
      "FOLD: 4, EPOCH: 9, train_loss: 0.019974502760010795\n",
      "FOLD: 4, EPOCH: 9, valid_loss: 0.016856207870520078\n",
      "FOLD: 4, EPOCH: 10, train_loss: 0.019846136782419036\n",
      "FOLD: 4, EPOCH: 10, valid_loss: 0.01672839201413668\n",
      "FOLD: 4, EPOCH: 11, train_loss: 0.01964704868559902\n",
      "FOLD: 4, EPOCH: 11, valid_loss: 0.016753573400469925\n",
      "FOLD: 4, EPOCH: 12, train_loss: 0.019564559783887218\n",
      "FOLD: 4, EPOCH: 12, valid_loss: 0.016650744785483066\n",
      "FOLD: 4, EPOCH: 13, train_loss: 0.019433976704808505\n",
      "FOLD: 4, EPOCH: 13, valid_loss: 0.016501402052549217\n",
      "FOLD: 4, EPOCH: 14, train_loss: 0.019292215137062845\n",
      "FOLD: 4, EPOCH: 14, valid_loss: 0.01641601023192589\n",
      "FOLD: 4, EPOCH: 15, train_loss: 0.019151396470496785\n",
      "FOLD: 4, EPOCH: 15, valid_loss: 0.016392107838048384\n",
      "FOLD: 4, EPOCH: 16, train_loss: 0.019007258947837998\n",
      "FOLD: 4, EPOCH: 16, valid_loss: 0.016335076222626064\n",
      "FOLD: 4, EPOCH: 17, train_loss: 0.018916007433388685\n",
      "FOLD: 4, EPOCH: 17, valid_loss: 0.016290973083904155\n",
      "FOLD: 4, EPOCH: 18, train_loss: 0.01881092172619459\n",
      "FOLD: 4, EPOCH: 18, valid_loss: 0.016273249227267046\n",
      "FOLD: 4, EPOCH: 19, train_loss: 0.018651398150502024\n",
      "FOLD: 4, EPOCH: 19, valid_loss: 0.016263798667261235\n",
      "FOLD: 4, EPOCH: 20, train_loss: 0.018563028802541463\n",
      "FOLD: 4, EPOCH: 20, valid_loss: 0.01620323683779973\n",
      "FOLD: 4, EPOCH: 21, train_loss: 0.018427308799849974\n",
      "FOLD: 4, EPOCH: 21, valid_loss: 0.0161857339911736\n",
      "FOLD: 4, EPOCH: 22, train_loss: 0.018368880440657202\n",
      "FOLD: 4, EPOCH: 22, valid_loss: 0.01613158954737278\n",
      "FOLD: 4, EPOCH: 23, train_loss: 0.018252535521782732\n",
      "FOLD: 4, EPOCH: 23, valid_loss: 0.016099215485155582\n",
      "FOLD: 4, EPOCH: 24, train_loss: 0.018138024024665356\n",
      "FOLD: 4, EPOCH: 24, valid_loss: 0.01614177320152521\n",
      "FOLD: 4, EPOCH: 25, train_loss: 0.01803520555935196\n",
      "FOLD: 4, EPOCH: 25, valid_loss: 0.016033270826133397\n",
      "FOLD: 4, EPOCH: 26, train_loss: 0.017939146708797763\n",
      "FOLD: 4, EPOCH: 26, valid_loss: 0.016038839203807023\n",
      "FOLD: 4, EPOCH: 27, train_loss: 0.017804429740519136\n",
      "FOLD: 4, EPOCH: 27, valid_loss: 0.01602345726524408\n",
      "FOLD: 4, EPOCH: 28, train_loss: 0.017711781242207902\n",
      "FOLD: 4, EPOCH: 28, valid_loss: 0.01595293666021182\n",
      "FOLD: 4, EPOCH: 29, train_loss: 0.017619156303840713\n",
      "FOLD: 4, EPOCH: 29, valid_loss: 0.015994990674348977\n",
      "FOLD: 4, EPOCH: 30, train_loss: 0.01751401980180998\n",
      "FOLD: 4, EPOCH: 30, valid_loss: 0.016038234632175703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 4, EPOCH: 31, train_loss: 0.017405827731095457\n",
      "FOLD: 4, EPOCH: 31, valid_loss: 0.015958107219865687\n",
      "FOLD: 4, EPOCH: 32, train_loss: 0.017339119423382187\n",
      "FOLD: 4, EPOCH: 32, valid_loss: 0.015980290678831246\n",
      "FOLD: 4, EPOCH: 33, train_loss: 0.017237257892014208\n",
      "FOLD: 4, EPOCH: 33, valid_loss: 0.015993374184920237\n",
      "FOLD: 4, EPOCH: 34, train_loss: 0.017123570011274236\n",
      "FOLD: 4, EPOCH: 34, valid_loss: 0.015966418055960767\n",
      "FOLD: 4, EPOCH: 35, train_loss: 0.017026424319860903\n",
      "FOLD: 4, EPOCH: 35, valid_loss: 0.015982786694971416\n",
      "Epoch    36: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 4, EPOCH: 36, train_loss: 0.016795167649114453\n",
      "FOLD: 4, EPOCH: 36, valid_loss: 0.015932804618317347\n",
      "FOLD: 4, EPOCH: 37, train_loss: 0.016682972716217912\n",
      "FOLD: 4, EPOCH: 37, valid_loss: 0.015915152473518483\n",
      "FOLD: 4, EPOCH: 38, train_loss: 0.01654561440384871\n",
      "FOLD: 4, EPOCH: 38, valid_loss: 0.0159423746741735\n",
      "FOLD: 4, EPOCH: 39, train_loss: 0.016493618475726328\n",
      "FOLD: 4, EPOCH: 39, valid_loss: 0.015919195488095284\n",
      "FOLD: 4, EPOCH: 40, train_loss: 0.016409043247836666\n",
      "FOLD: 4, EPOCH: 40, valid_loss: 0.015942346233014878\n",
      "FOLD: 4, EPOCH: 41, train_loss: 0.01630231794724996\n",
      "FOLD: 4, EPOCH: 41, valid_loss: 0.015946993604302406\n",
      "FOLD: 4, EPOCH: 42, train_loss: 0.016155534889549017\n",
      "FOLD: 4, EPOCH: 42, valid_loss: 0.01593781463228739\n",
      "FOLD: 4, EPOCH: 43, train_loss: 0.01607213487747956\n",
      "FOLD: 4, EPOCH: 43, valid_loss: 0.01596454495134262\n",
      "FOLD: 4, EPOCH: 44, train_loss: 0.016033221073952074\n",
      "FOLD: 4, EPOCH: 44, valid_loss: 0.015951695565420847\n",
      "Epoch    45: reducing learning rate of group 0 to 1.1288e-01.\n",
      "FOLD: 4, EPOCH: 45, train_loss: 0.015882203632311243\n",
      "FOLD: 4, EPOCH: 45, valid_loss: 0.01593720432944023\n",
      "FOLD: 4, EPOCH: 46, train_loss: 0.01579928513918374\n",
      "FOLD: 4, EPOCH: 46, valid_loss: 0.0159196645164719\n",
      "FOLD: 4, EPOCH: 47, train_loss: 0.015607165902651645\n",
      "FOLD: 4, EPOCH: 47, valid_loss: 0.01594187469723133\n",
      "FOLD: 4, EPOCH: 48, train_loss: 0.01558887674095663\n",
      "FOLD: 4, EPOCH: 48, valid_loss: 0.01596517810741296\n",
      "FOLD: 4, EPOCH: 49, train_loss: 0.015506030664452025\n",
      "FOLD: 4, EPOCH: 49, valid_loss: 0.015953638447591893\n",
      "FOLD: 5, EPOCH: 0, train_loss: 0.04316358978079783\n",
      "FOLD: 5, EPOCH: 0, valid_loss: 0.020650779255307637\n",
      "FOLD: 5, EPOCH: 1, train_loss: 0.02264972881892243\n",
      "FOLD: 5, EPOCH: 1, valid_loss: 0.018943507224321365\n",
      "FOLD: 5, EPOCH: 2, train_loss: 0.02181022601655206\n",
      "FOLD: 5, EPOCH: 2, valid_loss: 0.018673009454057768\n",
      "FOLD: 5, EPOCH: 3, train_loss: 0.021290499565971864\n",
      "FOLD: 5, EPOCH: 3, valid_loss: 0.017867996715582334\n",
      "FOLD: 5, EPOCH: 4, train_loss: 0.020945514225073764\n",
      "FOLD: 5, EPOCH: 4, valid_loss: 0.017623093265753526\n",
      "FOLD: 5, EPOCH: 5, train_loss: 0.020665974909993442\n",
      "FOLD: 5, EPOCH: 5, valid_loss: 0.017429049198444072\n",
      "FOLD: 5, EPOCH: 6, train_loss: 0.02051758529568041\n",
      "FOLD: 5, EPOCH: 6, valid_loss: 0.017219390720129013\n",
      "FOLD: 5, EPOCH: 7, train_loss: 0.020307248080703052\n",
      "FOLD: 5, EPOCH: 7, valid_loss: 0.01701318988433251\n",
      "FOLD: 5, EPOCH: 8, train_loss: 0.020106677721078332\n",
      "FOLD: 5, EPOCH: 8, valid_loss: 0.016952256313883342\n",
      "FOLD: 5, EPOCH: 9, train_loss: 0.019969192358690338\n",
      "FOLD: 5, EPOCH: 9, valid_loss: 0.016820635503301255\n",
      "FOLD: 5, EPOCH: 10, train_loss: 0.019843407428345165\n",
      "FOLD: 5, EPOCH: 10, valid_loss: 0.016759576562505502\n",
      "FOLD: 5, EPOCH: 11, train_loss: 0.019707992144331738\n",
      "FOLD: 5, EPOCH: 11, valid_loss: 0.016880452489623658\n",
      "FOLD: 5, EPOCH: 12, train_loss: 0.01957593873344563\n",
      "FOLD: 5, EPOCH: 12, valid_loss: 0.016548123855430346\n",
      "FOLD: 5, EPOCH: 13, train_loss: 0.019449797811339032\n",
      "FOLD: 5, EPOCH: 13, valid_loss: 0.0164861181894174\n",
      "FOLD: 5, EPOCH: 14, train_loss: 0.019296427634922234\n",
      "FOLD: 5, EPOCH: 14, valid_loss: 0.016386917434059657\n",
      "FOLD: 5, EPOCH: 15, train_loss: 0.01916747643436129\n",
      "FOLD: 5, EPOCH: 15, valid_loss: 0.016389585816516325\n",
      "FOLD: 5, EPOCH: 16, train_loss: 0.019067899424683402\n",
      "FOLD: 5, EPOCH: 16, valid_loss: 0.0163486056173077\n",
      "FOLD: 5, EPOCH: 17, train_loss: 0.01892709221086792\n",
      "FOLD: 5, EPOCH: 17, valid_loss: 0.016274247461786635\n",
      "FOLD: 5, EPOCH: 18, train_loss: 0.018859274030939954\n",
      "FOLD: 5, EPOCH: 18, valid_loss: 0.016195562166663315\n",
      "FOLD: 5, EPOCH: 19, train_loss: 0.018707052477308223\n",
      "FOLD: 5, EPOCH: 19, valid_loss: 0.016132057501146428\n",
      "FOLD: 5, EPOCH: 20, train_loss: 0.01864043187752769\n",
      "FOLD: 5, EPOCH: 20, valid_loss: 0.016177024334095992\n",
      "FOLD: 5, EPOCH: 21, train_loss: 0.018563806985479755\n",
      "FOLD: 5, EPOCH: 21, valid_loss: 0.016084610269619867\n",
      "FOLD: 5, EPOCH: 22, train_loss: 0.018393100113482087\n",
      "FOLD: 5, EPOCH: 22, valid_loss: 0.016086511313915253\n",
      "FOLD: 5, EPOCH: 23, train_loss: 0.01826165568687626\n",
      "FOLD: 5, EPOCH: 23, valid_loss: 0.016041468757276352\n",
      "FOLD: 5, EPOCH: 24, train_loss: 0.01819960856055086\n",
      "FOLD: 5, EPOCH: 24, valid_loss: 0.016001836396753788\n",
      "FOLD: 5, EPOCH: 25, train_loss: 0.018112966564257402\n",
      "FOLD: 5, EPOCH: 25, valid_loss: 0.015992445226472158\n",
      "FOLD: 5, EPOCH: 26, train_loss: 0.017977807356195676\n",
      "FOLD: 5, EPOCH: 26, valid_loss: 0.015974230849399015\n",
      "FOLD: 5, EPOCH: 27, train_loss: 0.017862279710636753\n",
      "FOLD: 5, EPOCH: 27, valid_loss: 0.015952756485113732\n",
      "FOLD: 5, EPOCH: 28, train_loss: 0.017768753214261017\n",
      "FOLD: 5, EPOCH: 28, valid_loss: 0.016004624346701\n",
      "FOLD: 5, EPOCH: 29, train_loss: 0.017730516457074397\n",
      "FOLD: 5, EPOCH: 29, valid_loss: 0.015920994230187856\n",
      "FOLD: 5, EPOCH: 30, train_loss: 0.01757381486429556\n",
      "FOLD: 5, EPOCH: 30, valid_loss: 0.01590653179356685\n",
      "FOLD: 5, EPOCH: 31, train_loss: 0.017499343743799505\n",
      "FOLD: 5, EPOCH: 31, valid_loss: 0.015895207340900715\n",
      "FOLD: 5, EPOCH: 32, train_loss: 0.017382456090401958\n",
      "FOLD: 5, EPOCH: 32, valid_loss: 0.01588718328051842\n",
      "FOLD: 5, EPOCH: 33, train_loss: 0.017309405362686596\n",
      "FOLD: 5, EPOCH: 33, valid_loss: 0.015901290883238498\n",
      "FOLD: 5, EPOCH: 34, train_loss: 0.017159635553488862\n",
      "FOLD: 5, EPOCH: 34, valid_loss: 0.015910937522466365\n",
      "FOLD: 5, EPOCH: 35, train_loss: 0.017105405080459407\n",
      "FOLD: 5, EPOCH: 35, valid_loss: 0.015925961045118477\n",
      "FOLD: 5, EPOCH: 36, train_loss: 0.016947022979021877\n",
      "FOLD: 5, EPOCH: 36, valid_loss: 0.015898660470086794\n",
      "FOLD: 5, EPOCH: 37, train_loss: 0.016913242034009984\n",
      "FOLD: 5, EPOCH: 37, valid_loss: 0.015851627820386335\n",
      "FOLD: 5, EPOCH: 38, train_loss: 0.01681673777214176\n",
      "FOLD: 5, EPOCH: 38, valid_loss: 0.015868676683077447\n",
      "FOLD: 5, EPOCH: 39, train_loss: 0.0166731728995974\n",
      "FOLD: 5, EPOCH: 39, valid_loss: 0.01586111506017355\n",
      "FOLD: 5, EPOCH: 40, train_loss: 0.016611351102993294\n",
      "FOLD: 5, EPOCH: 40, valid_loss: 0.01604640491020221\n",
      "FOLD: 5, EPOCH: 41, train_loss: 0.016499897389597184\n",
      "FOLD: 5, EPOCH: 41, valid_loss: 0.01588326534972741\n",
      "FOLD: 5, EPOCH: 42, train_loss: 0.016393408582017228\n",
      "FOLD: 5, EPOCH: 42, valid_loss: 0.015879986234582387\n",
      "FOLD: 5, EPOCH: 43, train_loss: 0.016314401064772863\n",
      "FOLD: 5, EPOCH: 43, valid_loss: 0.0158100133188642\n",
      "FOLD: 5, EPOCH: 44, train_loss: 0.016199083935872122\n",
      "FOLD: 5, EPOCH: 44, valid_loss: 0.015870019793510437\n",
      "FOLD: 5, EPOCH: 45, train_loss: 0.01607432626691219\n",
      "FOLD: 5, EPOCH: 45, valid_loss: 0.0158179049881605\n",
      "FOLD: 5, EPOCH: 46, train_loss: 0.01600761336551325\n",
      "FOLD: 5, EPOCH: 46, valid_loss: 0.015839999398359887\n",
      "FOLD: 5, EPOCH: 47, train_loss: 0.0159259842255631\n",
      "FOLD: 5, EPOCH: 47, valid_loss: 0.0158668769380221\n",
      "FOLD: 5, EPOCH: 48, train_loss: 0.01576681448952169\n",
      "FOLD: 5, EPOCH: 48, valid_loss: 0.015871939106056325\n",
      "FOLD: 5, EPOCH: 49, train_loss: 0.015658639333638792\n",
      "FOLD: 5, EPOCH: 49, valid_loss: 0.01588236053402607\n",
      "FOLD: 6, EPOCH: 0, train_loss: 0.04486847417177381\n",
      "FOLD: 6, EPOCH: 0, valid_loss: 0.02008190220938279\n",
      "FOLD: 6, EPOCH: 1, train_loss: 0.022918218067167578\n",
      "FOLD: 6, EPOCH: 1, valid_loss: 0.018888987529163178\n",
      "FOLD: 6, EPOCH: 2, train_loss: 0.02193994930869824\n",
      "FOLD: 6, EPOCH: 2, valid_loss: 0.018106702285317276\n",
      "FOLD: 6, EPOCH: 3, train_loss: 0.021393130306859274\n",
      "FOLD: 6, EPOCH: 3, valid_loss: 0.017640018764023598\n",
      "FOLD: 6, EPOCH: 4, train_loss: 0.021060757339000702\n",
      "FOLD: 6, EPOCH: 4, valid_loss: 0.017280115029559687\n",
      "FOLD: 6, EPOCH: 5, train_loss: 0.020762206558641548\n",
      "FOLD: 6, EPOCH: 5, valid_loss: 0.017153664921911862\n",
      "FOLD: 6, EPOCH: 6, train_loss: 0.02052752786894908\n",
      "FOLD: 6, EPOCH: 6, valid_loss: 0.01693716301367833\n",
      "FOLD: 6, EPOCH: 7, train_loss: 0.020376396617172537\n",
      "FOLD: 6, EPOCH: 7, valid_loss: 0.016835070645006802\n",
      "FOLD: 6, EPOCH: 8, train_loss: 0.02017280935133631\n",
      "FOLD: 6, EPOCH: 8, valid_loss: 0.01669146457257179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 6, EPOCH: 9, train_loss: 0.019993781590381184\n",
      "FOLD: 6, EPOCH: 9, valid_loss: 0.016560936346650124\n",
      "FOLD: 6, EPOCH: 10, train_loss: 0.019877779373043292\n",
      "FOLD: 6, EPOCH: 10, valid_loss: 0.016494812730413217\n",
      "FOLD: 6, EPOCH: 11, train_loss: 0.01974983702136858\n",
      "FOLD: 6, EPOCH: 11, valid_loss: 0.016406491685372133\n",
      "FOLD: 6, EPOCH: 12, train_loss: 0.01959798950701952\n",
      "FOLD: 6, EPOCH: 12, valid_loss: 0.016397329333883066\n",
      "FOLD: 6, EPOCH: 13, train_loss: 0.019459616292167355\n",
      "FOLD: 6, EPOCH: 13, valid_loss: 0.016286679185353793\n",
      "FOLD: 6, EPOCH: 14, train_loss: 0.019318095055987704\n",
      "FOLD: 6, EPOCH: 14, valid_loss: 0.01615478084064447\n",
      "FOLD: 6, EPOCH: 15, train_loss: 0.019239468927923088\n",
      "FOLD: 6, EPOCH: 15, valid_loss: 0.016165853406374272\n",
      "FOLD: 6, EPOCH: 16, train_loss: 0.019114989787340164\n",
      "FOLD: 6, EPOCH: 16, valid_loss: 0.016119879168959763\n",
      "FOLD: 6, EPOCH: 17, train_loss: 0.019012950587312918\n",
      "FOLD: 6, EPOCH: 17, valid_loss: 0.016050857276870653\n",
      "FOLD: 6, EPOCH: 18, train_loss: 0.018891852248359372\n",
      "FOLD: 6, EPOCH: 18, valid_loss: 0.016006668098270893\n",
      "FOLD: 6, EPOCH: 19, train_loss: 0.018779950241583424\n",
      "FOLD: 6, EPOCH: 19, valid_loss: 0.015970667824149132\n",
      "FOLD: 6, EPOCH: 20, train_loss: 0.018674469905326497\n",
      "FOLD: 6, EPOCH: 20, valid_loss: 0.016006319497067194\n",
      "FOLD: 6, EPOCH: 21, train_loss: 0.018573509302695055\n",
      "FOLD: 6, EPOCH: 21, valid_loss: 0.01594583632854315\n",
      "FOLD: 6, EPOCH: 22, train_loss: 0.018463956871749582\n",
      "FOLD: 6, EPOCH: 22, valid_loss: 0.01587340973604184\n",
      "FOLD: 6, EPOCH: 23, train_loss: 0.018347233634542773\n",
      "FOLD: 6, EPOCH: 23, valid_loss: 0.015879158217173357\n",
      "FOLD: 6, EPOCH: 24, train_loss: 0.018275005742907524\n",
      "FOLD: 6, EPOCH: 24, valid_loss: 0.015869892130677517\n",
      "FOLD: 6, EPOCH: 25, train_loss: 0.018182320042035065\n",
      "FOLD: 6, EPOCH: 25, valid_loss: 0.015823944829977475\n",
      "FOLD: 6, EPOCH: 26, train_loss: 0.018038572819047683\n",
      "FOLD: 6, EPOCH: 26, valid_loss: 0.015790908525769528\n",
      "FOLD: 6, EPOCH: 27, train_loss: 0.017981290339014015\n",
      "FOLD: 6, EPOCH: 27, valid_loss: 0.015785454414211787\n",
      "FOLD: 6, EPOCH: 28, train_loss: 0.0178531934129628\n",
      "FOLD: 6, EPOCH: 28, valid_loss: 0.01578086342376012\n",
      "FOLD: 6, EPOCH: 29, train_loss: 0.017776761808105418\n",
      "FOLD: 6, EPOCH: 29, valid_loss: 0.015738285075013455\n",
      "FOLD: 6, EPOCH: 30, train_loss: 0.01767799721376316\n",
      "FOLD: 6, EPOCH: 30, valid_loss: 0.015737206388551455\n",
      "FOLD: 6, EPOCH: 31, train_loss: 0.017568320035934448\n",
      "FOLD: 6, EPOCH: 31, valid_loss: 0.015732616615983155\n",
      "FOLD: 6, EPOCH: 32, train_loss: 0.017447392053499416\n",
      "FOLD: 6, EPOCH: 32, valid_loss: 0.015721947742769353\n",
      "FOLD: 6, EPOCH: 33, train_loss: 0.017348730307374452\n",
      "FOLD: 6, EPOCH: 33, valid_loss: 0.015733005908819344\n",
      "FOLD: 6, EPOCH: 34, train_loss: 0.017178681553215593\n",
      "FOLD: 6, EPOCH: 34, valid_loss: 0.01567288488149643\n",
      "FOLD: 6, EPOCH: 35, train_loss: 0.017103561791717202\n",
      "FOLD: 6, EPOCH: 35, valid_loss: 0.015699223042107545\n",
      "FOLD: 6, EPOCH: 36, train_loss: 0.01706012639191908\n",
      "FOLD: 6, EPOCH: 36, valid_loss: 0.015713865510546245\n",
      "FOLD: 6, EPOCH: 37, train_loss: 0.016922509650120866\n",
      "FOLD: 6, EPOCH: 37, valid_loss: 0.01569261050854738\n",
      "FOLD: 6, EPOCH: 38, train_loss: 0.016800661093077145\n",
      "FOLD: 6, EPOCH: 38, valid_loss: 0.015668305066915657\n",
      "FOLD: 6, EPOCH: 39, train_loss: 0.016738009915964025\n",
      "FOLD: 6, EPOCH: 39, valid_loss: 0.015692411921918392\n",
      "FOLD: 6, EPOCH: 40, train_loss: 0.01661794414945148\n",
      "FOLD: 6, EPOCH: 40, valid_loss: 0.015711049979122784\n",
      "FOLD: 6, EPOCH: 41, train_loss: 0.01652695270053841\n",
      "FOLD: 6, EPOCH: 41, valid_loss: 0.01574579826914347\n",
      "FOLD: 6, EPOCH: 42, train_loss: 0.016453262091287085\n",
      "FOLD: 6, EPOCH: 42, valid_loss: 0.015716421131331187\n",
      "FOLD: 6, EPOCH: 43, train_loss: 0.016311205697019358\n",
      "FOLD: 6, EPOCH: 43, valid_loss: 0.015742124774708197\n",
      "FOLD: 6, EPOCH: 44, train_loss: 0.016236355012232386\n",
      "FOLD: 6, EPOCH: 44, valid_loss: 0.015716385597792957\n",
      "FOLD: 6, EPOCH: 45, train_loss: 0.016188481885537103\n",
      "FOLD: 6, EPOCH: 45, valid_loss: 0.015709758234711792\n",
      "Epoch    46: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 6, EPOCH: 46, train_loss: 0.0159082466081993\n",
      "FOLD: 6, EPOCH: 46, valid_loss: 0.01572715648664878\n",
      "FOLD: 6, EPOCH: 47, train_loss: 0.015850299289701757\n",
      "FOLD: 6, EPOCH: 47, valid_loss: 0.01570550388155075\n",
      "FOLD: 6, EPOCH: 48, train_loss: 0.015632639148247404\n",
      "FOLD: 6, EPOCH: 48, valid_loss: 0.01570789121951048\n",
      "FOLD: 6, EPOCH: 49, train_loss: 0.015570857700564572\n",
      "FOLD: 6, EPOCH: 49, valid_loss: 0.01571989768686203\n",
      "FOLD: 0, EPOCH: 0, train_loss: 0.04353557431415932\n",
      "FOLD: 0, EPOCH: 0, valid_loss: 0.020289452746510506\n",
      "FOLD: 0, EPOCH: 1, train_loss: 0.022790599236818584\n",
      "FOLD: 0, EPOCH: 1, valid_loss: 0.01908446905704645\n",
      "FOLD: 0, EPOCH: 2, train_loss: 0.02184171013131335\n",
      "FOLD: 0, EPOCH: 2, valid_loss: 0.01815097263226142\n",
      "FOLD: 0, EPOCH: 3, train_loss: 0.021198950808595966\n",
      "FOLD: 0, EPOCH: 3, valid_loss: 0.01770714484155178\n",
      "FOLD: 0, EPOCH: 4, train_loss: 0.020893201911570253\n",
      "FOLD: 0, EPOCH: 4, valid_loss: 0.01739794660646182\n",
      "FOLD: 0, EPOCH: 5, train_loss: 0.02070134485492835\n",
      "FOLD: 0, EPOCH: 5, valid_loss: 0.017231149813876703\n",
      "FOLD: 0, EPOCH: 6, train_loss: 0.02044524612358293\n",
      "FOLD: 0, EPOCH: 6, valid_loss: 0.017110314220190048\n",
      "FOLD: 0, EPOCH: 7, train_loss: 0.020282679187083565\n",
      "FOLD: 0, EPOCH: 7, valid_loss: 0.01698174513876438\n",
      "FOLD: 0, EPOCH: 8, train_loss: 0.02008765480304892\n",
      "FOLD: 0, EPOCH: 8, valid_loss: 0.016842088088966332\n",
      "FOLD: 0, EPOCH: 9, train_loss: 0.019932561629527324\n",
      "FOLD: 0, EPOCH: 9, valid_loss: 0.016698642204014156\n",
      "FOLD: 0, EPOCH: 10, train_loss: 0.019790390405703236\n",
      "FOLD: 0, EPOCH: 10, valid_loss: 0.016642743721604347\n",
      "FOLD: 0, EPOCH: 11, train_loss: 0.019653926427299913\n",
      "FOLD: 0, EPOCH: 11, valid_loss: 0.016508006848967992\n",
      "FOLD: 0, EPOCH: 12, train_loss: 0.019514283089823014\n",
      "FOLD: 0, EPOCH: 12, valid_loss: 0.016424082577801667\n",
      "FOLD: 0, EPOCH: 13, train_loss: 0.019429016807997548\n",
      "FOLD: 0, EPOCH: 13, valid_loss: 0.016371459485246584\n",
      "FOLD: 0, EPOCH: 14, train_loss: 0.01927705907036324\n",
      "FOLD: 0, EPOCH: 14, valid_loss: 0.01628549497288007\n",
      "FOLD: 0, EPOCH: 15, train_loss: 0.019143284827068046\n",
      "FOLD: 0, EPOCH: 15, valid_loss: 0.01632358434681709\n",
      "FOLD: 0, EPOCH: 16, train_loss: 0.01903602244282091\n",
      "FOLD: 0, EPOCH: 16, valid_loss: 0.016152108947818097\n",
      "FOLD: 0, EPOCH: 17, train_loss: 0.0188985959047804\n",
      "FOLD: 0, EPOCH: 17, valid_loss: 0.01616649780995571\n",
      "FOLD: 0, EPOCH: 18, train_loss: 0.018781472073012107\n",
      "FOLD: 0, EPOCH: 18, valid_loss: 0.01610853859724907\n",
      "FOLD: 0, EPOCH: 19, train_loss: 0.01870341583885051\n",
      "FOLD: 0, EPOCH: 19, valid_loss: 0.01605501462920354\n",
      "FOLD: 0, EPOCH: 20, train_loss: 0.01863849611097091\n",
      "FOLD: 0, EPOCH: 20, valid_loss: 0.016006051920927487\n",
      "FOLD: 0, EPOCH: 21, train_loss: 0.018470980453531485\n",
      "FOLD: 0, EPOCH: 21, valid_loss: 0.0160165113182022\n",
      "FOLD: 0, EPOCH: 22, train_loss: 0.018363268592873135\n",
      "FOLD: 0, EPOCH: 22, valid_loss: 0.015973756519647744\n",
      "FOLD: 0, EPOCH: 23, train_loss: 0.01831589301897062\n",
      "FOLD: 0, EPOCH: 23, valid_loss: 0.015956496604933187\n",
      "FOLD: 0, EPOCH: 24, train_loss: 0.018213979446807423\n",
      "FOLD: 0, EPOCH: 24, valid_loss: 0.01594622476169696\n",
      "FOLD: 0, EPOCH: 25, train_loss: 0.018093731833269466\n",
      "FOLD: 0, EPOCH: 25, valid_loss: 0.01588271988125948\n",
      "FOLD: 0, EPOCH: 26, train_loss: 0.018006755869734933\n",
      "FOLD: 0, EPOCH: 26, valid_loss: 0.015922815467302617\n",
      "FOLD: 0, EPOCH: 27, train_loss: 0.01788233343914554\n",
      "FOLD: 0, EPOCH: 27, valid_loss: 0.015889349823387768\n",
      "FOLD: 0, EPOCH: 28, train_loss: 0.017810242326074355\n",
      "FOLD: 0, EPOCH: 28, valid_loss: 0.015828041861263607\n",
      "FOLD: 0, EPOCH: 29, train_loss: 0.017669163984120696\n",
      "FOLD: 0, EPOCH: 29, valid_loss: 0.015826060006824825\n",
      "FOLD: 0, EPOCH: 30, train_loss: 0.0175833986266642\n",
      "FOLD: 0, EPOCH: 30, valid_loss: 0.0157929457580814\n",
      "FOLD: 0, EPOCH: 31, train_loss: 0.017511042187342774\n",
      "FOLD: 0, EPOCH: 31, valid_loss: 0.015768009667786267\n",
      "FOLD: 0, EPOCH: 32, train_loss: 0.017347948248120578\n",
      "FOLD: 0, EPOCH: 32, valid_loss: 0.01576687739445613\n",
      "FOLD: 0, EPOCH: 33, train_loss: 0.01725927789418681\n",
      "FOLD: 0, EPOCH: 33, valid_loss: 0.01577311295729417\n",
      "FOLD: 0, EPOCH: 34, train_loss: 0.017166638369294437\n",
      "FOLD: 0, EPOCH: 34, valid_loss: 0.01576764337145365\n",
      "FOLD: 0, EPOCH: 35, train_loss: 0.017104446460065002\n",
      "FOLD: 0, EPOCH: 35, valid_loss: 0.015747257866538487\n",
      "FOLD: 0, EPOCH: 36, train_loss: 0.0169895115006413\n",
      "FOLD: 0, EPOCH: 36, valid_loss: 0.015751309406298857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 0, EPOCH: 37, train_loss: 0.016880733904907026\n",
      "FOLD: 0, EPOCH: 37, valid_loss: 0.015756685573321123\n",
      "FOLD: 0, EPOCH: 38, train_loss: 0.016799295044227225\n",
      "FOLD: 0, EPOCH: 38, valid_loss: 0.015774532077977292\n",
      "FOLD: 0, EPOCH: 39, train_loss: 0.016668770009198704\n",
      "FOLD: 0, EPOCH: 39, valid_loss: 0.015752864858278863\n",
      "FOLD: 0, EPOCH: 40, train_loss: 0.016615505267337367\n",
      "FOLD: 0, EPOCH: 40, valid_loss: 0.015750863660986606\n",
      "FOLD: 0, EPOCH: 41, train_loss: 0.01646681082107731\n",
      "FOLD: 0, EPOCH: 41, valid_loss: 0.015753307093221407\n",
      "FOLD: 0, EPOCH: 42, train_loss: 0.016433504523357022\n",
      "FOLD: 0, EPOCH: 42, valid_loss: 0.01575416111602233\n",
      "Epoch    43: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 0, EPOCH: 43, train_loss: 0.016199531436369225\n",
      "FOLD: 0, EPOCH: 43, valid_loss: 0.01576621551066637\n",
      "FOLD: 0, EPOCH: 44, train_loss: 0.016051681196266734\n",
      "FOLD: 0, EPOCH: 44, valid_loss: 0.015751414502469394\n",
      "FOLD: 0, EPOCH: 45, train_loss: 0.015985969403708302\n",
      "FOLD: 0, EPOCH: 45, valid_loss: 0.01575121462631684\n",
      "FOLD: 0, EPOCH: 46, train_loss: 0.01587203343877116\n",
      "FOLD: 0, EPOCH: 46, valid_loss: 0.015764101694982786\n",
      "FOLD: 0, EPOCH: 47, train_loss: 0.01578098242296963\n",
      "FOLD: 0, EPOCH: 47, valid_loss: 0.01573685692766538\n",
      "FOLD: 0, EPOCH: 48, train_loss: 0.01562887123106299\n",
      "FOLD: 0, EPOCH: 48, valid_loss: 0.015776810236275196\n",
      "FOLD: 0, EPOCH: 49, train_loss: 0.015579241151745254\n",
      "FOLD: 0, EPOCH: 49, valid_loss: 0.01577982048575695\n",
      "FOLD: 1, EPOCH: 0, train_loss: 0.044741841586860454\n",
      "FOLD: 1, EPOCH: 0, valid_loss: 0.019987990220005695\n",
      "FOLD: 1, EPOCH: 1, train_loss: 0.02281278291264096\n",
      "FOLD: 1, EPOCH: 1, valid_loss: 0.018727160846957795\n",
      "FOLD: 1, EPOCH: 2, train_loss: 0.021903104400513945\n",
      "FOLD: 1, EPOCH: 2, valid_loss: 0.01806349565203373\n",
      "FOLD: 1, EPOCH: 3, train_loss: 0.021383682442073886\n",
      "FOLD: 1, EPOCH: 3, valid_loss: 0.017539646906348374\n",
      "FOLD: 1, EPOCH: 4, train_loss: 0.021027585944613895\n",
      "FOLD: 1, EPOCH: 4, valid_loss: 0.017311731210121743\n",
      "FOLD: 1, EPOCH: 5, train_loss: 0.02082291610438276\n",
      "FOLD: 1, EPOCH: 5, valid_loss: 0.017134731062329732\n",
      "FOLD: 1, EPOCH: 6, train_loss: 0.020582573135962356\n",
      "FOLD: 1, EPOCH: 6, valid_loss: 0.01703670320029442\n",
      "FOLD: 1, EPOCH: 7, train_loss: 0.02043680172111537\n",
      "FOLD: 1, EPOCH: 7, valid_loss: 0.016912420351917926\n",
      "FOLD: 1, EPOCH: 8, train_loss: 0.02026921118030677\n",
      "FOLD: 1, EPOCH: 8, valid_loss: 0.016855942371946115\n",
      "FOLD: 1, EPOCH: 9, train_loss: 0.02011444339075604\n",
      "FOLD: 1, EPOCH: 9, valid_loss: 0.016688763378904417\n",
      "FOLD: 1, EPOCH: 10, train_loss: 0.019963461679179926\n",
      "FOLD: 1, EPOCH: 10, valid_loss: 0.016464860536731206\n",
      "FOLD: 1, EPOCH: 11, train_loss: 0.019835405731321993\n",
      "FOLD: 1, EPOCH: 11, valid_loss: 0.016418364257193528\n",
      "FOLD: 1, EPOCH: 12, train_loss: 0.01969403443807686\n",
      "FOLD: 1, EPOCH: 12, valid_loss: 0.016300343335247956\n",
      "FOLD: 1, EPOCH: 13, train_loss: 0.01956813274001753\n",
      "FOLD: 1, EPOCH: 13, valid_loss: 0.016315434629527423\n",
      "FOLD: 1, EPOCH: 14, train_loss: 0.01946015623272271\n",
      "FOLD: 1, EPOCH: 14, valid_loss: 0.016169059448517285\n",
      "FOLD: 1, EPOCH: 15, train_loss: 0.019310640800442244\n",
      "FOLD: 1, EPOCH: 15, valid_loss: 0.01616128247517806\n",
      "FOLD: 1, EPOCH: 16, train_loss: 0.019218305050319916\n",
      "FOLD: 1, EPOCH: 16, valid_loss: 0.016107560207064334\n",
      "FOLD: 1, EPOCH: 17, train_loss: 0.019115159514586668\n",
      "FOLD: 1, EPOCH: 17, valid_loss: 0.016021610882419806\n",
      "FOLD: 1, EPOCH: 18, train_loss: 0.018956479465437902\n",
      "FOLD: 1, EPOCH: 18, valid_loss: 0.01591676838982564\n",
      "FOLD: 1, EPOCH: 19, train_loss: 0.01889288123394992\n",
      "FOLD: 1, EPOCH: 19, valid_loss: 0.016228955024136946\n",
      "FOLD: 1, EPOCH: 20, train_loss: 0.018729510666752183\n",
      "FOLD: 1, EPOCH: 20, valid_loss: 0.01588390438029399\n",
      "FOLD: 1, EPOCH: 21, train_loss: 0.018696176190231298\n",
      "FOLD: 1, EPOCH: 21, valid_loss: 0.015894022340384815\n",
      "FOLD: 1, EPOCH: 22, train_loss: 0.018555013840464322\n",
      "FOLD: 1, EPOCH: 22, valid_loss: 0.01583009635886321\n",
      "FOLD: 1, EPOCH: 23, train_loss: 0.018440251490352926\n",
      "FOLD: 1, EPOCH: 23, valid_loss: 0.015769824743843995\n",
      "FOLD: 1, EPOCH: 24, train_loss: 0.018356982392032404\n",
      "FOLD: 1, EPOCH: 24, valid_loss: 0.01574872677715925\n",
      "FOLD: 1, EPOCH: 25, train_loss: 0.018265779985970742\n",
      "FOLD: 1, EPOCH: 25, valid_loss: 0.015733240100626763\n",
      "FOLD: 1, EPOCH: 26, train_loss: 0.01814537658985402\n",
      "FOLD: 1, EPOCH: 26, valid_loss: 0.015686414634379055\n",
      "FOLD: 1, EPOCH: 27, train_loss: 0.01805242823084464\n",
      "FOLD: 1, EPOCH: 27, valid_loss: 0.015679212645269357\n",
      "FOLD: 1, EPOCH: 28, train_loss: 0.017914397981823295\n",
      "FOLD: 1, EPOCH: 28, valid_loss: 0.015658723262067024\n",
      "FOLD: 1, EPOCH: 29, train_loss: 0.017888272105640656\n",
      "FOLD: 1, EPOCH: 29, valid_loss: 0.015649096897015206\n",
      "FOLD: 1, EPOCH: 30, train_loss: 0.01779645960777998\n",
      "FOLD: 1, EPOCH: 30, valid_loss: 0.01563721766265539\n",
      "FOLD: 1, EPOCH: 31, train_loss: 0.01769510148143446\n",
      "FOLD: 1, EPOCH: 31, valid_loss: 0.015640895097301558\n",
      "FOLD: 1, EPOCH: 32, train_loss: 0.01759713193452036\n",
      "FOLD: 1, EPOCH: 32, valid_loss: 0.015615194821013855\n",
      "FOLD: 1, EPOCH: 33, train_loss: 0.01749362767598516\n",
      "FOLD: 1, EPOCH: 33, valid_loss: 0.01563943284921921\n",
      "FOLD: 1, EPOCH: 34, train_loss: 0.017389605400731433\n",
      "FOLD: 1, EPOCH: 34, valid_loss: 0.0155797152278515\n",
      "FOLD: 1, EPOCH: 35, train_loss: 0.01728679402102087\n",
      "FOLD: 1, EPOCH: 35, valid_loss: 0.015591406908172827\n",
      "FOLD: 1, EPOCH: 36, train_loss: 0.017191044872979056\n",
      "FOLD: 1, EPOCH: 36, valid_loss: 0.015578212431417061\n",
      "FOLD: 1, EPOCH: 37, train_loss: 0.017101676543117374\n",
      "FOLD: 1, EPOCH: 37, valid_loss: 0.015530422329902649\n",
      "FOLD: 1, EPOCH: 38, train_loss: 0.016981517984154256\n",
      "FOLD: 1, EPOCH: 38, valid_loss: 0.015566374605091719\n",
      "FOLD: 1, EPOCH: 39, train_loss: 0.01693710809372164\n",
      "FOLD: 1, EPOCH: 39, valid_loss: 0.015548625459464697\n",
      "FOLD: 1, EPOCH: 40, train_loss: 0.016772187025462452\n",
      "FOLD: 1, EPOCH: 40, valid_loss: 0.015568941043546567\n",
      "FOLD: 1, EPOCH: 41, train_loss: 0.01669338345527649\n",
      "FOLD: 1, EPOCH: 41, valid_loss: 0.01557821615670736\n",
      "FOLD: 1, EPOCH: 42, train_loss: 0.016603438070396315\n",
      "FOLD: 1, EPOCH: 42, valid_loss: 0.015575077313070115\n",
      "FOLD: 1, EPOCH: 43, train_loss: 0.016520336698237305\n",
      "FOLD: 1, EPOCH: 43, valid_loss: 0.015585482478714906\n",
      "FOLD: 1, EPOCH: 44, train_loss: 0.0163968743857097\n",
      "FOLD: 1, EPOCH: 44, valid_loss: 0.015579745101814087\n",
      "Epoch    45: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 1, EPOCH: 45, train_loss: 0.016216503003159084\n",
      "FOLD: 1, EPOCH: 45, valid_loss: 0.015538132821138088\n",
      "FOLD: 1, EPOCH: 46, train_loss: 0.016079222368126787\n",
      "FOLD: 1, EPOCH: 46, valid_loss: 0.01556276186154439\n",
      "FOLD: 1, EPOCH: 47, train_loss: 0.016051452002815297\n",
      "FOLD: 1, EPOCH: 47, valid_loss: 0.01555582845153717\n",
      "FOLD: 1, EPOCH: 48, train_loss: 0.015909709804062103\n",
      "FOLD: 1, EPOCH: 48, valid_loss: 0.015528792801957864\n",
      "FOLD: 1, EPOCH: 49, train_loss: 0.015828480113398383\n",
      "FOLD: 1, EPOCH: 49, valid_loss: 0.015557846699196558\n",
      "FOLD: 2, EPOCH: 0, train_loss: 0.04452940756203355\n",
      "FOLD: 2, EPOCH: 0, valid_loss: 0.020428725303365633\n",
      "FOLD: 2, EPOCH: 1, train_loss: 0.02280538363029828\n",
      "FOLD: 2, EPOCH: 1, valid_loss: 0.018657926040200088\n",
      "FOLD: 2, EPOCH: 2, train_loss: 0.02181621077092918\n",
      "FOLD: 2, EPOCH: 2, valid_loss: 0.0186034539857736\n",
      "FOLD: 2, EPOCH: 3, train_loss: 0.021353732291105633\n",
      "FOLD: 2, EPOCH: 3, valid_loss: 0.017568230270766295\n",
      "FOLD: 2, EPOCH: 4, train_loss: 0.020969203346081683\n",
      "FOLD: 2, EPOCH: 4, valid_loss: 0.01740461616561963\n",
      "FOLD: 2, EPOCH: 5, train_loss: 0.02073809111843238\n",
      "FOLD: 2, EPOCH: 5, valid_loss: 0.01709719331791768\n",
      "FOLD: 2, EPOCH: 6, train_loss: 0.0204699492031658\n",
      "FOLD: 2, EPOCH: 6, valid_loss: 0.0169584104934564\n",
      "FOLD: 2, EPOCH: 7, train_loss: 0.020317438838852418\n",
      "FOLD: 2, EPOCH: 7, valid_loss: 0.016963461987101115\n",
      "FOLD: 2, EPOCH: 8, train_loss: 0.02017326169722789\n",
      "FOLD: 2, EPOCH: 8, valid_loss: 0.016735744017821092\n",
      "FOLD: 2, EPOCH: 9, train_loss: 0.02003303061969377\n",
      "FOLD: 2, EPOCH: 9, valid_loss: 0.016536188741716053\n",
      "FOLD: 2, EPOCH: 10, train_loss: 0.019811166198672476\n",
      "FOLD: 2, EPOCH: 10, valid_loss: 0.01651408043331825\n",
      "FOLD: 2, EPOCH: 11, train_loss: 0.019718191812972765\n",
      "FOLD: 2, EPOCH: 11, valid_loss: 0.016438645740541127\n",
      "FOLD: 2, EPOCH: 12, train_loss: 0.019632095370340993\n",
      "FOLD: 2, EPOCH: 12, valid_loss: 0.016451706966528527\n",
      "FOLD: 2, EPOCH: 13, train_loss: 0.019491890797743928\n",
      "FOLD: 2, EPOCH: 13, valid_loss: 0.016337584489240095\n",
      "FOLD: 2, EPOCH: 14, train_loss: 0.01934930485849445\n",
      "FOLD: 2, EPOCH: 14, valid_loss: 0.016326884309259746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 2, EPOCH: 15, train_loss: 0.019238017700813913\n",
      "FOLD: 2, EPOCH: 15, valid_loss: 0.01616989620603048\n",
      "FOLD: 2, EPOCH: 16, train_loss: 0.019131653690458956\n",
      "FOLD: 2, EPOCH: 16, valid_loss: 0.01611455909621257\n",
      "FOLD: 2, EPOCH: 17, train_loss: 0.018958369219625317\n",
      "FOLD: 2, EPOCH: 17, valid_loss: 0.01607300477245679\n",
      "FOLD: 2, EPOCH: 18, train_loss: 0.0189001826770805\n",
      "FOLD: 2, EPOCH: 18, valid_loss: 0.016016691493300293\n",
      "FOLD: 2, EPOCH: 19, train_loss: 0.0187450929771404\n",
      "FOLD: 2, EPOCH: 19, valid_loss: 0.01597966583302388\n",
      "FOLD: 2, EPOCH: 20, train_loss: 0.018677360403376655\n",
      "FOLD: 2, EPOCH: 20, valid_loss: 0.015936339990450785\n",
      "FOLD: 2, EPOCH: 21, train_loss: 0.018523945875868603\n",
      "FOLD: 2, EPOCH: 21, valid_loss: 0.01593017979310109\n",
      "FOLD: 2, EPOCH: 22, train_loss: 0.018477206219088386\n",
      "FOLD: 2, EPOCH: 22, valid_loss: 0.015856819800459422\n",
      "FOLD: 2, EPOCH: 23, train_loss: 0.018298478797078133\n",
      "FOLD: 2, EPOCH: 23, valid_loss: 0.015864833902854186\n",
      "FOLD: 2, EPOCH: 24, train_loss: 0.018236880543062817\n",
      "FOLD: 2, EPOCH: 24, valid_loss: 0.015857351299088735\n",
      "FOLD: 2, EPOCH: 25, train_loss: 0.018184135063878587\n",
      "FOLD: 2, EPOCH: 25, valid_loss: 0.015830464517840974\n",
      "FOLD: 2, EPOCH: 26, train_loss: 0.018024748568800656\n",
      "FOLD: 2, EPOCH: 26, valid_loss: 0.01580331216637905\n",
      "FOLD: 2, EPOCH: 27, train_loss: 0.01793745452085057\n",
      "FOLD: 2, EPOCH: 27, valid_loss: 0.01583442693719497\n",
      "FOLD: 2, EPOCH: 28, train_loss: 0.017855042465836614\n",
      "FOLD: 2, EPOCH: 28, valid_loss: 0.015784792530422028\n",
      "FOLD: 2, EPOCH: 29, train_loss: 0.01771118990271478\n",
      "FOLD: 2, EPOCH: 29, valid_loss: 0.01575889753607603\n",
      "FOLD: 2, EPOCH: 30, train_loss: 0.017632520518492203\n",
      "FOLD: 2, EPOCH: 30, valid_loss: 0.015731718534460433\n",
      "FOLD: 2, EPOCH: 31, train_loss: 0.01750864617123797\n",
      "FOLD: 2, EPOCH: 31, valid_loss: 0.015664672693954065\n",
      "FOLD: 2, EPOCH: 32, train_loss: 0.017467384125936677\n",
      "FOLD: 2, EPOCH: 32, valid_loss: 0.015678702781979855\n",
      "FOLD: 2, EPOCH: 33, train_loss: 0.017370996103194113\n",
      "FOLD: 2, EPOCH: 33, valid_loss: 0.015677764367025632\n",
      "FOLD: 2, EPOCH: 34, train_loss: 0.017253104219766887\n",
      "FOLD: 2, EPOCH: 34, valid_loss: 0.015654702097750627\n",
      "FOLD: 2, EPOCH: 35, train_loss: 0.017159804689219675\n",
      "FOLD: 2, EPOCH: 35, valid_loss: 0.015677088871598244\n",
      "FOLD: 2, EPOCH: 36, train_loss: 0.017047301639576216\n",
      "FOLD: 2, EPOCH: 36, valid_loss: 0.015676734037697315\n",
      "FOLD: 2, EPOCH: 37, train_loss: 0.016987058925568253\n",
      "FOLD: 2, EPOCH: 37, valid_loss: 0.01565845153079583\n",
      "FOLD: 2, EPOCH: 38, train_loss: 0.01684331687519679\n",
      "FOLD: 2, EPOCH: 38, valid_loss: 0.015660229568871167\n",
      "FOLD: 2, EPOCH: 39, train_loss: 0.016727813846758893\n",
      "FOLD: 2, EPOCH: 39, valid_loss: 0.01565942533600789\n",
      "FOLD: 2, EPOCH: 40, train_loss: 0.016653688284694344\n",
      "FOLD: 2, EPOCH: 40, valid_loss: 0.015634074950447448\n",
      "FOLD: 2, EPOCH: 41, train_loss: 0.016546847688889987\n",
      "FOLD: 2, EPOCH: 41, valid_loss: 0.01565860090060876\n",
      "FOLD: 2, EPOCH: 42, train_loss: 0.016406932090585295\n",
      "FOLD: 2, EPOCH: 42, valid_loss: 0.015689526183100846\n",
      "FOLD: 2, EPOCH: 43, train_loss: 0.016297396196908242\n",
      "FOLD: 2, EPOCH: 43, valid_loss: 0.015651294603370704\n",
      "FOLD: 2, EPOCH: 44, train_loss: 0.016236221279344848\n",
      "FOLD: 2, EPOCH: 44, valid_loss: 0.015636361132447537\n",
      "FOLD: 2, EPOCH: 45, train_loss: 0.016149247367237066\n",
      "FOLD: 2, EPOCH: 45, valid_loss: 0.015630806008210547\n",
      "FOLD: 2, EPOCH: 46, train_loss: 0.015979918235963262\n",
      "FOLD: 2, EPOCH: 46, valid_loss: 0.01560187404258893\n",
      "FOLD: 2, EPOCH: 47, train_loss: 0.015947704769771646\n",
      "FOLD: 2, EPOCH: 47, valid_loss: 0.015678427898539946\n",
      "FOLD: 2, EPOCH: 48, train_loss: 0.01585204024622972\n",
      "FOLD: 2, EPOCH: 48, valid_loss: 0.015664318576455116\n",
      "FOLD: 2, EPOCH: 49, train_loss: 0.015736677545450023\n",
      "FOLD: 2, EPOCH: 49, valid_loss: 0.015616399236023426\n",
      "FOLD: 3, EPOCH: 0, train_loss: 0.04459159823788984\n",
      "FOLD: 3, EPOCH: 0, valid_loss: 0.021238591951819565\n",
      "FOLD: 3, EPOCH: 1, train_loss: 0.022921900818678172\n",
      "FOLD: 3, EPOCH: 1, valid_loss: 0.019677116463963803\n",
      "FOLD: 3, EPOCH: 2, train_loss: 0.021933827599560893\n",
      "FOLD: 3, EPOCH: 2, valid_loss: 0.018133883722699605\n",
      "FOLD: 3, EPOCH: 3, train_loss: 0.02134963615822631\n",
      "FOLD: 3, EPOCH: 3, valid_loss: 0.017891853474653684\n",
      "FOLD: 3, EPOCH: 4, train_loss: 0.021042571403086185\n",
      "FOLD: 3, EPOCH: 4, valid_loss: 0.01756968076985616\n",
      "FOLD: 3, EPOCH: 5, train_loss: 0.0207415524920499\n",
      "FOLD: 3, EPOCH: 5, valid_loss: 0.01735224761068821\n",
      "FOLD: 3, EPOCH: 6, train_loss: 0.02050321634758163\n",
      "FOLD: 3, EPOCH: 6, valid_loss: 0.01718037253102431\n",
      "FOLD: 3, EPOCH: 7, train_loss: 0.020295292768325354\n",
      "FOLD: 3, EPOCH: 7, valid_loss: 0.01703953850441254\n",
      "FOLD: 3, EPOCH: 8, train_loss: 0.020128518569509726\n",
      "FOLD: 3, EPOCH: 8, valid_loss: 0.016894537668961745\n",
      "FOLD: 3, EPOCH: 9, train_loss: 0.019987209976927656\n",
      "FOLD: 3, EPOCH: 9, valid_loss: 0.016843088114490874\n",
      "FOLD: 3, EPOCH: 10, train_loss: 0.019863176612636528\n",
      "FOLD: 3, EPOCH: 10, valid_loss: 0.01674171257764101\n",
      "FOLD: 3, EPOCH: 11, train_loss: 0.019718958341793436\n",
      "FOLD: 3, EPOCH: 11, valid_loss: 0.0166597617073701\n",
      "FOLD: 3, EPOCH: 12, train_loss: 0.019537157555286948\n",
      "FOLD: 3, EPOCH: 12, valid_loss: 0.016525400515932303\n",
      "FOLD: 3, EPOCH: 13, train_loss: 0.019475574907217477\n",
      "FOLD: 3, EPOCH: 13, valid_loss: 0.016501435436881505\n",
      "FOLD: 3, EPOCH: 14, train_loss: 0.019361874315182905\n",
      "FOLD: 3, EPOCH: 14, valid_loss: 0.01647202756542426\n",
      "FOLD: 3, EPOCH: 15, train_loss: 0.019263210681241913\n",
      "FOLD: 3, EPOCH: 15, valid_loss: 0.01637737352687579\n",
      "FOLD: 3, EPOCH: 16, train_loss: 0.019093668037975155\n",
      "FOLD: 3, EPOCH: 16, valid_loss: 0.016333352846021835\n",
      "FOLD: 3, EPOCH: 17, train_loss: 0.018972954788320773\n",
      "FOLD: 3, EPOCH: 17, valid_loss: 0.0162602081321753\n",
      "FOLD: 3, EPOCH: 18, train_loss: 0.01886258805422364\n",
      "FOLD: 3, EPOCH: 18, valid_loss: 0.016251395384852704\n",
      "FOLD: 3, EPOCH: 19, train_loss: 0.01879714207874762\n",
      "FOLD: 3, EPOCH: 19, valid_loss: 0.01623645446334894\n",
      "FOLD: 3, EPOCH: 20, train_loss: 0.018669860789904725\n",
      "FOLD: 3, EPOCH: 20, valid_loss: 0.01614817425322074\n",
      "FOLD: 3, EPOCH: 21, train_loss: 0.018557449223826062\n",
      "FOLD: 3, EPOCH: 21, valid_loss: 0.016135755496529434\n",
      "FOLD: 3, EPOCH: 22, train_loss: 0.0184727235873406\n",
      "FOLD: 3, EPOCH: 22, valid_loss: 0.016056818744310968\n",
      "FOLD: 3, EPOCH: 23, train_loss: 0.018403910226314456\n",
      "FOLD: 3, EPOCH: 23, valid_loss: 0.016058194952515457\n",
      "FOLD: 3, EPOCH: 24, train_loss: 0.018269307861054265\n",
      "FOLD: 3, EPOCH: 24, valid_loss: 0.016077069494013604\n",
      "FOLD: 3, EPOCH: 25, train_loss: 0.01818942595776674\n",
      "FOLD: 3, EPOCH: 25, valid_loss: 0.016012047560742267\n",
      "FOLD: 3, EPOCH: 26, train_loss: 0.018077848428809964\n",
      "FOLD: 3, EPOCH: 26, valid_loss: 0.015989484193806466\n",
      "FOLD: 3, EPOCH: 27, train_loss: 0.017968577181769384\n",
      "FOLD: 3, EPOCH: 27, valid_loss: 0.01598433620081498\n",
      "FOLD: 3, EPOCH: 28, train_loss: 0.017890355197360385\n",
      "FOLD: 3, EPOCH: 28, valid_loss: 0.01594743003638891\n",
      "FOLD: 3, EPOCH: 29, train_loss: 0.017769779808617926\n",
      "FOLD: 3, EPOCH: 29, valid_loss: 0.015924334454421814\n",
      "FOLD: 3, EPOCH: 30, train_loss: 0.017674645207620954\n",
      "FOLD: 3, EPOCH: 30, valid_loss: 0.015945511440245006\n",
      "FOLD: 3, EPOCH: 31, train_loss: 0.01760314203597404\n",
      "FOLD: 3, EPOCH: 31, valid_loss: 0.015927344703903563\n",
      "FOLD: 3, EPOCH: 32, train_loss: 0.017493989822026844\n",
      "FOLD: 3, EPOCH: 32, valid_loss: 0.015912874816701963\n",
      "FOLD: 3, EPOCH: 33, train_loss: 0.017387718729976867\n",
      "FOLD: 3, EPOCH: 33, valid_loss: 0.01594591076270892\n",
      "FOLD: 3, EPOCH: 34, train_loss: 0.017282207395780732\n",
      "FOLD: 3, EPOCH: 34, valid_loss: 0.015899838521503486\n",
      "FOLD: 3, EPOCH: 35, train_loss: 0.017213228347434384\n",
      "FOLD: 3, EPOCH: 35, valid_loss: 0.015895621492885627\n",
      "FOLD: 3, EPOCH: 36, train_loss: 0.017087318053519405\n",
      "FOLD: 3, EPOCH: 36, valid_loss: 0.01587261574772688\n",
      "FOLD: 3, EPOCH: 37, train_loss: 0.01699331998069947\n",
      "FOLD: 3, EPOCH: 37, valid_loss: 0.015867547275355227\n",
      "FOLD: 3, EPOCH: 38, train_loss: 0.016909658468353586\n",
      "FOLD: 3, EPOCH: 38, valid_loss: 0.015864606731786177\n",
      "FOLD: 3, EPOCH: 39, train_loss: 0.016788808049986493\n",
      "FOLD: 3, EPOCH: 39, valid_loss: 0.015900398461291425\n",
      "FOLD: 3, EPOCH: 40, train_loss: 0.01667983887868153\n",
      "FOLD: 3, EPOCH: 40, valid_loss: 0.015888192117787324\n",
      "FOLD: 3, EPOCH: 41, train_loss: 0.016607033378268417\n",
      "FOLD: 3, EPOCH: 41, valid_loss: 0.015893327000622567\n",
      "FOLD: 3, EPOCH: 42, train_loss: 0.016475874414617144\n",
      "FOLD: 3, EPOCH: 42, valid_loss: 0.015860851424244735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 3, EPOCH: 43, train_loss: 0.016401446751646093\n",
      "FOLD: 3, EPOCH: 43, valid_loss: 0.015878880468125526\n",
      "FOLD: 3, EPOCH: 44, train_loss: 0.016321707365883363\n",
      "FOLD: 3, EPOCH: 44, valid_loss: 0.015896554893025987\n",
      "FOLD: 3, EPOCH: 45, train_loss: 0.016210347376260405\n",
      "FOLD: 3, EPOCH: 45, valid_loss: 0.015888083869448073\n",
      "FOLD: 3, EPOCH: 46, train_loss: 0.01609137233950802\n",
      "FOLD: 3, EPOCH: 46, valid_loss: 0.0158843701132215\n",
      "FOLD: 3, EPOCH: 47, train_loss: 0.0160370267222862\n",
      "FOLD: 3, EPOCH: 47, valid_loss: 0.015894478903366968\n",
      "FOLD: 3, EPOCH: 48, train_loss: 0.01591046841664089\n",
      "FOLD: 3, EPOCH: 48, valid_loss: 0.015875262279923145\n",
      "FOLD: 3, EPOCH: 49, train_loss: 0.01581121179451411\n",
      "FOLD: 3, EPOCH: 49, valid_loss: 0.015882248273835733\n",
      "Epoch    50: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 4, EPOCH: 0, train_loss: 0.04279072149782567\n",
      "FOLD: 4, EPOCH: 0, valid_loss: 0.02017496726833857\n",
      "FOLD: 4, EPOCH: 1, train_loss: 0.022595249544325714\n",
      "FOLD: 4, EPOCH: 1, valid_loss: 0.01873279993350689\n",
      "FOLD: 4, EPOCH: 2, train_loss: 0.021705182244044705\n",
      "FOLD: 4, EPOCH: 2, valid_loss: 0.018283235768859204\n",
      "FOLD: 4, EPOCH: 3, train_loss: 0.021212481289497903\n",
      "FOLD: 4, EPOCH: 3, valid_loss: 0.018632718720115148\n",
      "FOLD: 4, EPOCH: 4, train_loss: 0.02095220680977847\n",
      "FOLD: 4, EPOCH: 4, valid_loss: 0.01758779499393243\n",
      "FOLD: 4, EPOCH: 5, train_loss: 0.0206517698746678\n",
      "FOLD: 4, EPOCH: 5, valid_loss: 0.01739615947008133\n",
      "FOLD: 4, EPOCH: 6, train_loss: 0.02043978582966972\n",
      "FOLD: 4, EPOCH: 6, valid_loss: 0.017232921834175404\n",
      "FOLD: 4, EPOCH: 7, train_loss: 0.02025116891977755\n",
      "FOLD: 4, EPOCH: 7, valid_loss: 0.017089406888072308\n",
      "FOLD: 4, EPOCH: 8, train_loss: 0.02006714421047552\n",
      "FOLD: 4, EPOCH: 8, valid_loss: 0.016947754443838045\n",
      "FOLD: 4, EPOCH: 9, train_loss: 0.019961815629456495\n",
      "FOLD: 4, EPOCH: 9, valid_loss: 0.016838362583747275\n",
      "FOLD: 4, EPOCH: 10, train_loss: 0.019816980747556365\n",
      "FOLD: 4, EPOCH: 10, valid_loss: 0.01678488288934414\n",
      "FOLD: 4, EPOCH: 11, train_loss: 0.01967740154548271\n",
      "FOLD: 4, EPOCH: 11, valid_loss: 0.016719342997440927\n",
      "FOLD: 4, EPOCH: 12, train_loss: 0.01956710755522992\n",
      "FOLD: 4, EPOCH: 12, valid_loss: 0.016889553946944382\n",
      "FOLD: 4, EPOCH: 13, train_loss: 0.019379542511258577\n",
      "FOLD: 4, EPOCH: 13, valid_loss: 0.016655374963123065\n",
      "FOLD: 4, EPOCH: 14, train_loss: 0.01926047692226397\n",
      "FOLD: 4, EPOCH: 14, valid_loss: 0.01653134314200053\n",
      "FOLD: 4, EPOCH: 15, train_loss: 0.01918079296277987\n",
      "FOLD: 4, EPOCH: 15, valid_loss: 0.01648290383701141\n",
      "FOLD: 4, EPOCH: 16, train_loss: 0.019060396941731107\n",
      "FOLD: 4, EPOCH: 16, valid_loss: 0.016391534429903213\n",
      "FOLD: 4, EPOCH: 17, train_loss: 0.018896378627097286\n",
      "FOLD: 4, EPOCH: 17, valid_loss: 0.016378776886715338\n",
      "FOLD: 4, EPOCH: 18, train_loss: 0.01880288536886911\n",
      "FOLD: 4, EPOCH: 18, valid_loss: 0.016342456452548504\n",
      "FOLD: 4, EPOCH: 19, train_loss: 0.01868516861184223\n",
      "FOLD: 4, EPOCH: 19, valid_loss: 0.016290183179080486\n",
      "FOLD: 4, EPOCH: 20, train_loss: 0.018585004793429696\n",
      "FOLD: 4, EPOCH: 20, valid_loss: 0.016249924181745604\n",
      "FOLD: 4, EPOCH: 21, train_loss: 0.018479819988479484\n",
      "FOLD: 4, EPOCH: 21, valid_loss: 0.0162364449352026\n",
      "FOLD: 4, EPOCH: 22, train_loss: 0.018367475678993238\n",
      "FOLD: 4, EPOCH: 22, valid_loss: 0.016212336145914517\n",
      "FOLD: 4, EPOCH: 23, train_loss: 0.018316051910153112\n",
      "FOLD: 4, EPOCH: 23, valid_loss: 0.016217762604355812\n",
      "FOLD: 4, EPOCH: 24, train_loss: 0.01814561417779407\n",
      "FOLD: 4, EPOCH: 24, valid_loss: 0.016199446426561244\n",
      "FOLD: 4, EPOCH: 25, train_loss: 0.01801549519940808\n",
      "FOLD: 4, EPOCH: 25, valid_loss: 0.01616522927696888\n",
      "FOLD: 4, EPOCH: 26, train_loss: 0.01795707260434692\n",
      "FOLD: 4, EPOCH: 26, valid_loss: 0.016143528386377372\n",
      "FOLD: 4, EPOCH: 27, train_loss: 0.01781259200258835\n",
      "FOLD: 4, EPOCH: 27, valid_loss: 0.016121392209942523\n",
      "FOLD: 4, EPOCH: 28, train_loss: 0.017781439438663626\n",
      "FOLD: 4, EPOCH: 28, valid_loss: 0.016210466981507264\n",
      "FOLD: 4, EPOCH: 29, train_loss: 0.017616943154181983\n",
      "FOLD: 4, EPOCH: 29, valid_loss: 0.016124471377294797\n",
      "FOLD: 4, EPOCH: 30, train_loss: 0.01753042231792131\n",
      "FOLD: 4, EPOCH: 30, valid_loss: 0.016108936701829616\n",
      "FOLD: 4, EPOCH: 31, train_loss: 0.01744036390320272\n",
      "FOLD: 4, EPOCH: 31, valid_loss: 0.016088280826807022\n",
      "FOLD: 4, EPOCH: 32, train_loss: 0.017343064266684895\n",
      "FOLD: 4, EPOCH: 32, valid_loss: 0.01610950265939419\n",
      "FOLD: 4, EPOCH: 33, train_loss: 0.01721916062052588\n",
      "FOLD: 4, EPOCH: 33, valid_loss: 0.016103237867355347\n",
      "FOLD: 4, EPOCH: 34, train_loss: 0.017145491482035535\n",
      "FOLD: 4, EPOCH: 34, valid_loss: 0.016106961868130244\n",
      "FOLD: 4, EPOCH: 35, train_loss: 0.017003622736681153\n",
      "FOLD: 4, EPOCH: 35, valid_loss: 0.01606521440240053\n",
      "FOLD: 4, EPOCH: 36, train_loss: 0.016948993884127687\n",
      "FOLD: 4, EPOCH: 36, valid_loss: 0.016086336296911422\n",
      "FOLD: 4, EPOCH: 37, train_loss: 0.016806141108375142\n",
      "FOLD: 4, EPOCH: 37, valid_loss: 0.016071425679211434\n",
      "FOLD: 4, EPOCH: 38, train_loss: 0.01666320601125827\n",
      "FOLD: 4, EPOCH: 38, valid_loss: 0.016064989022337474\n",
      "FOLD: 4, EPOCH: 39, train_loss: 0.016649325214628433\n",
      "FOLD: 4, EPOCH: 39, valid_loss: 0.01605669853205864\n",
      "FOLD: 4, EPOCH: 40, train_loss: 0.01651722866085333\n",
      "FOLD: 4, EPOCH: 40, valid_loss: 0.016061860996370133\n",
      "FOLD: 4, EPOCH: 41, train_loss: 0.016428998155469023\n",
      "FOLD: 4, EPOCH: 41, valid_loss: 0.016043875079888564\n",
      "FOLD: 4, EPOCH: 42, train_loss: 0.016287534875241486\n",
      "FOLD: 4, EPOCH: 42, valid_loss: 0.016053786071447227\n",
      "FOLD: 4, EPOCH: 43, train_loss: 0.016196259297430515\n",
      "FOLD: 4, EPOCH: 43, valid_loss: 0.016069506366665546\n",
      "FOLD: 4, EPOCH: 44, train_loss: 0.016108125913888216\n",
      "FOLD: 4, EPOCH: 44, valid_loss: 0.016081375571397636\n",
      "FOLD: 4, EPOCH: 45, train_loss: 0.016023008945725253\n",
      "FOLD: 4, EPOCH: 45, valid_loss: 0.016071934467897966\n",
      "FOLD: 4, EPOCH: 46, train_loss: 0.01591429309416059\n",
      "FOLD: 4, EPOCH: 46, valid_loss: 0.016089025168464735\n",
      "FOLD: 4, EPOCH: 47, train_loss: 0.01578283356502652\n",
      "FOLD: 4, EPOCH: 47, valid_loss: 0.016061335658797853\n",
      "FOLD: 4, EPOCH: 48, train_loss: 0.01570078862730313\n",
      "FOLD: 4, EPOCH: 48, valid_loss: 0.016099821561231062\n",
      "Epoch    49: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 4, EPOCH: 49, train_loss: 0.015437501092516893\n",
      "FOLD: 4, EPOCH: 49, valid_loss: 0.01608828748934544\n",
      "FOLD: 5, EPOCH: 0, train_loss: 0.043919486899835034\n",
      "FOLD: 5, EPOCH: 0, valid_loss: 0.020288600371434137\n",
      "FOLD: 5, EPOCH: 1, train_loss: 0.022671690110016515\n",
      "FOLD: 5, EPOCH: 1, valid_loss: 0.019392497264421903\n",
      "FOLD: 5, EPOCH: 2, train_loss: 0.021852932618679228\n",
      "FOLD: 5, EPOCH: 2, valid_loss: 0.01822696912747163\n",
      "FOLD: 5, EPOCH: 3, train_loss: 0.021290694061364676\n",
      "FOLD: 5, EPOCH: 3, valid_loss: 0.017773495700496893\n",
      "FOLD: 5, EPOCH: 4, train_loss: 0.02095552986940822\n",
      "FOLD: 5, EPOCH: 4, valid_loss: 0.017692717651908215\n",
      "FOLD: 5, EPOCH: 5, train_loss: 0.0207137596113859\n",
      "FOLD: 5, EPOCH: 5, valid_loss: 0.017457613434929114\n",
      "FOLD: 5, EPOCH: 6, train_loss: 0.020481614874222794\n",
      "FOLD: 5, EPOCH: 6, valid_loss: 0.017240525868076544\n",
      "FOLD: 5, EPOCH: 7, train_loss: 0.020305643185369066\n",
      "FOLD: 5, EPOCH: 7, valid_loss: 0.01706214965536044\n",
      "FOLD: 5, EPOCH: 8, train_loss: 0.02013616049913941\n",
      "FOLD: 5, EPOCH: 8, valid_loss: 0.016961316793010786\n",
      "FOLD: 5, EPOCH: 9, train_loss: 0.019985917955636978\n",
      "FOLD: 5, EPOCH: 9, valid_loss: 0.016897035762667656\n",
      "FOLD: 5, EPOCH: 10, train_loss: 0.019846533802715508\n",
      "FOLD: 5, EPOCH: 10, valid_loss: 0.016802829045515794\n",
      "FOLD: 5, EPOCH: 11, train_loss: 0.01970015710370766\n",
      "FOLD: 5, EPOCH: 11, valid_loss: 0.01660716590973047\n",
      "FOLD: 5, EPOCH: 12, train_loss: 0.019592114170459477\n",
      "FOLD: 5, EPOCH: 12, valid_loss: 0.016557180752547886\n",
      "FOLD: 5, EPOCH: 13, train_loss: 0.019462681803348904\n",
      "FOLD: 5, EPOCH: 13, valid_loss: 0.016527895385829303\n",
      "FOLD: 5, EPOCH: 14, train_loss: 0.019322423692289238\n",
      "FOLD: 5, EPOCH: 14, valid_loss: 0.016493157411997136\n",
      "FOLD: 5, EPOCH: 15, train_loss: 0.019227058249148162\n",
      "FOLD: 5, EPOCH: 15, valid_loss: 0.016360977951150674\n",
      "FOLD: 5, EPOCH: 16, train_loss: 0.019067211227642523\n",
      "FOLD: 5, EPOCH: 16, valid_loss: 0.016366626136004925\n",
      "FOLD: 5, EPOCH: 17, train_loss: 0.018969012525033305\n",
      "FOLD: 5, EPOCH: 17, valid_loss: 0.016255050754317872\n",
      "FOLD: 5, EPOCH: 18, train_loss: 0.018892185309448757\n",
      "FOLD: 5, EPOCH: 18, valid_loss: 0.016220280399116185\n",
      "FOLD: 5, EPOCH: 19, train_loss: 0.01875725607513576\n",
      "FOLD: 5, EPOCH: 19, valid_loss: 0.016222314049418155\n",
      "FOLD: 5, EPOCH: 20, train_loss: 0.018647891016223946\n",
      "FOLD: 5, EPOCH: 20, valid_loss: 0.016160441705813773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 5, EPOCH: 21, train_loss: 0.01855366279345912\n",
      "FOLD: 5, EPOCH: 21, valid_loss: 0.016127004359777156\n",
      "FOLD: 5, EPOCH: 22, train_loss: 0.018432472454937728\n",
      "FOLD: 5, EPOCH: 22, valid_loss: 0.016128851745564204\n",
      "FOLD: 5, EPOCH: 23, train_loss: 0.01834076874562212\n",
      "FOLD: 5, EPOCH: 23, valid_loss: 0.016077903672479667\n",
      "FOLD: 5, EPOCH: 24, train_loss: 0.01823099994578877\n",
      "FOLD: 5, EPOCH: 24, valid_loss: 0.01603463335106006\n",
      "FOLD: 5, EPOCH: 25, train_loss: 0.018132918010893707\n",
      "FOLD: 5, EPOCH: 25, valid_loss: 0.01601272212484708\n",
      "FOLD: 5, EPOCH: 26, train_loss: 0.018043692475436506\n",
      "FOLD: 5, EPOCH: 26, valid_loss: 0.016001687170221254\n",
      "FOLD: 5, EPOCH: 27, train_loss: 0.01793524804147514\n",
      "FOLD: 5, EPOCH: 27, valid_loss: 0.015999598428606987\n",
      "FOLD: 5, EPOCH: 28, train_loss: 0.0177825785468559\n",
      "FOLD: 5, EPOCH: 28, valid_loss: 0.015956143347116616\n",
      "FOLD: 5, EPOCH: 29, train_loss: 0.017710982570172968\n",
      "FOLD: 5, EPOCH: 29, valid_loss: 0.01595881789063032\n",
      "FOLD: 5, EPOCH: 30, train_loss: 0.01758947749496312\n",
      "FOLD: 5, EPOCH: 30, valid_loss: 0.015933818828601103\n",
      "FOLD: 5, EPOCH: 31, train_loss: 0.017463366526204185\n",
      "FOLD: 5, EPOCH: 31, valid_loss: 0.015936887393204067\n",
      "FOLD: 5, EPOCH: 32, train_loss: 0.017458399203983514\n",
      "FOLD: 5, EPOCH: 32, valid_loss: 0.015909138278892405\n",
      "FOLD: 5, EPOCH: 33, train_loss: 0.017298022398372758\n",
      "FOLD: 5, EPOCH: 33, valid_loss: 0.01593842515005515\n",
      "FOLD: 5, EPOCH: 34, train_loss: 0.017173661070095526\n",
      "FOLD: 5, EPOCH: 34, valid_loss: 0.015937682097921006\n",
      "FOLD: 5, EPOCH: 35, train_loss: 0.017053866482062918\n",
      "FOLD: 5, EPOCH: 35, valid_loss: 0.015909194874648865\n",
      "FOLD: 5, EPOCH: 36, train_loss: 0.01698008583657242\n",
      "FOLD: 5, EPOCH: 36, valid_loss: 0.015885959594295576\n",
      "FOLD: 5, EPOCH: 37, train_loss: 0.016847983933985233\n",
      "FOLD: 5, EPOCH: 37, valid_loss: 0.015905861241313126\n",
      "FOLD: 5, EPOCH: 38, train_loss: 0.016740692452200362\n",
      "FOLD: 5, EPOCH: 38, valid_loss: 0.015867202184521235\n",
      "FOLD: 5, EPOCH: 39, train_loss: 0.016613696789922746\n",
      "FOLD: 5, EPOCH: 39, valid_loss: 0.01586287124798848\n",
      "FOLD: 5, EPOCH: 40, train_loss: 0.01657845185616532\n",
      "FOLD: 5, EPOCH: 40, valid_loss: 0.015885933159062497\n",
      "FOLD: 5, EPOCH: 41, train_loss: 0.016487357647133036\n",
      "FOLD: 5, EPOCH: 41, valid_loss: 0.015896219115417737\n",
      "FOLD: 5, EPOCH: 42, train_loss: 0.016345701708986953\n",
      "FOLD: 5, EPOCH: 42, valid_loss: 0.015833688684954092\n",
      "FOLD: 5, EPOCH: 43, train_loss: 0.016244916018803377\n",
      "FOLD: 5, EPOCH: 43, valid_loss: 0.015882914670957968\n",
      "FOLD: 5, EPOCH: 44, train_loss: 0.016103564673480956\n",
      "FOLD: 5, EPOCH: 44, valid_loss: 0.015897374958373033\n",
      "FOLD: 5, EPOCH: 45, train_loss: 0.016018793830094306\n",
      "FOLD: 5, EPOCH: 45, valid_loss: 0.015849473527990855\n",
      "FOLD: 5, EPOCH: 46, train_loss: 0.0159747655679648\n",
      "FOLD: 5, EPOCH: 46, valid_loss: 0.01590351753223401\n",
      "FOLD: 5, EPOCH: 47, train_loss: 0.015866081268098707\n",
      "FOLD: 5, EPOCH: 47, valid_loss: 0.015921817590984013\n",
      "FOLD: 5, EPOCH: 48, train_loss: 0.015722424962331314\n",
      "FOLD: 5, EPOCH: 48, valid_loss: 0.015966342618832223\n",
      "FOLD: 5, EPOCH: 49, train_loss: 0.015646703944012925\n",
      "FOLD: 5, EPOCH: 49, valid_loss: 0.015944413410929534\n",
      "Epoch    50: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 6, EPOCH: 0, train_loss: 0.04243531108305261\n",
      "FOLD: 6, EPOCH: 0, valid_loss: 0.020052013632196646\n",
      "FOLD: 6, EPOCH: 1, train_loss: 0.022664721098703308\n",
      "FOLD: 6, EPOCH: 1, valid_loss: 0.018790962246174995\n",
      "FOLD: 6, EPOCH: 2, train_loss: 0.02177827908481295\n",
      "FOLD: 6, EPOCH: 2, valid_loss: 0.017826664691361096\n",
      "FOLD: 6, EPOCH: 3, train_loss: 0.021250591109934693\n",
      "FOLD: 6, EPOCH: 3, valid_loss: 0.017505092116502616\n",
      "FOLD: 6, EPOCH: 4, train_loss: 0.020999130541207018\n",
      "FOLD: 6, EPOCH: 4, valid_loss: 0.017283538929544963\n",
      "FOLD: 6, EPOCH: 5, train_loss: 0.02067095514487576\n",
      "FOLD: 6, EPOCH: 5, valid_loss: 0.017017863834133513\n",
      "FOLD: 6, EPOCH: 6, train_loss: 0.02050763542285642\n",
      "FOLD: 6, EPOCH: 6, valid_loss: 0.016856378159270838\n",
      "FOLD: 6, EPOCH: 7, train_loss: 0.020308670990571782\n",
      "FOLD: 6, EPOCH: 7, valid_loss: 0.01672549882473854\n",
      "FOLD: 6, EPOCH: 8, train_loss: 0.020123905275721807\n",
      "FOLD: 6, EPOCH: 8, valid_loss: 0.016650672142322246\n",
      "FOLD: 6, EPOCH: 9, train_loss: 0.020001596292933903\n",
      "FOLD: 6, EPOCH: 9, valid_loss: 0.016566392249212816\n",
      "FOLD: 6, EPOCH: 10, train_loss: 0.019838479800602875\n",
      "FOLD: 6, EPOCH: 10, valid_loss: 0.01649802901710455\n",
      "FOLD: 6, EPOCH: 11, train_loss: 0.019712329892491973\n",
      "FOLD: 6, EPOCH: 11, valid_loss: 0.01643142827714865\n",
      "FOLD: 6, EPOCH: 12, train_loss: 0.01957151298788754\n",
      "FOLD: 6, EPOCH: 12, valid_loss: 0.01640361948655202\n",
      "FOLD: 6, EPOCH: 13, train_loss: 0.01941854730751869\n",
      "FOLD: 6, EPOCH: 13, valid_loss: 0.0162369658310826\n",
      "FOLD: 6, EPOCH: 14, train_loss: 0.019318278476193145\n",
      "FOLD: 6, EPOCH: 14, valid_loss: 0.01620425584797676\n",
      "FOLD: 6, EPOCH: 15, train_loss: 0.019155420463632892\n",
      "FOLD: 6, EPOCH: 15, valid_loss: 0.016150782959392436\n",
      "FOLD: 6, EPOCH: 16, train_loss: 0.0190667609961049\n",
      "FOLD: 6, EPOCH: 16, valid_loss: 0.0161217159519975\n",
      "FOLD: 6, EPOCH: 17, train_loss: 0.01896724391829323\n",
      "FOLD: 6, EPOCH: 17, valid_loss: 0.016081432453714885\n",
      "FOLD: 6, EPOCH: 18, train_loss: 0.018847622807968308\n",
      "FOLD: 6, EPOCH: 18, valid_loss: 0.016031253509796582\n",
      "FOLD: 6, EPOCH: 19, train_loss: 0.018771146650652628\n",
      "FOLD: 6, EPOCH: 19, valid_loss: 0.016010199960034627\n",
      "FOLD: 6, EPOCH: 20, train_loss: 0.018639178871101624\n",
      "FOLD: 6, EPOCH: 20, valid_loss: 0.015977769087140378\n",
      "FOLD: 6, EPOCH: 21, train_loss: 0.018511371208807907\n",
      "FOLD: 6, EPOCH: 21, valid_loss: 0.01594131704992973\n",
      "FOLD: 6, EPOCH: 22, train_loss: 0.018427745917358913\n",
      "FOLD: 6, EPOCH: 22, valid_loss: 0.015895104967057705\n",
      "FOLD: 6, EPOCH: 23, train_loss: 0.018304499948548304\n",
      "FOLD: 6, EPOCH: 23, valid_loss: 0.015855271369218826\n",
      "FOLD: 6, EPOCH: 24, train_loss: 0.018198562685299565\n",
      "FOLD: 6, EPOCH: 24, valid_loss: 0.01579916928536617\n",
      "FOLD: 6, EPOCH: 25, train_loss: 0.01807857513729785\n",
      "FOLD: 6, EPOCH: 25, valid_loss: 0.015805139851111632\n",
      "FOLD: 6, EPOCH: 26, train_loss: 0.01795630675514002\n",
      "FOLD: 6, EPOCH: 26, valid_loss: 0.015810247152470626\n",
      "FOLD: 6, EPOCH: 27, train_loss: 0.017891642335500266\n",
      "FOLD: 6, EPOCH: 27, valid_loss: 0.015779428398953035\n",
      "FOLD: 6, EPOCH: 28, train_loss: 0.01778439057336466\n",
      "FOLD: 6, EPOCH: 28, valid_loss: 0.015759163464491185\n",
      "FOLD: 6, EPOCH: 29, train_loss: 0.017622474530661427\n",
      "FOLD: 6, EPOCH: 29, valid_loss: 0.015761131420731544\n",
      "FOLD: 6, EPOCH: 30, train_loss: 0.01757082904412134\n",
      "FOLD: 6, EPOCH: 30, valid_loss: 0.015745192264708188\n",
      "FOLD: 6, EPOCH: 31, train_loss: 0.017498567448677244\n",
      "FOLD: 6, EPOCH: 31, valid_loss: 0.01571542755342447\n",
      "FOLD: 6, EPOCH: 32, train_loss: 0.017409540738004284\n",
      "FOLD: 6, EPOCH: 32, valid_loss: 0.015687623204520114\n",
      "FOLD: 6, EPOCH: 33, train_loss: 0.017289772692665056\n",
      "FOLD: 6, EPOCH: 33, valid_loss: 0.015719611627551224\n",
      "FOLD: 6, EPOCH: 34, train_loss: 0.017141331931123056\n",
      "FOLD: 6, EPOCH: 34, valid_loss: 0.01568453235981556\n",
      "FOLD: 6, EPOCH: 35, train_loss: 0.017075088164592918\n",
      "FOLD: 6, EPOCH: 35, valid_loss: 0.01568689154317746\n",
      "FOLD: 6, EPOCH: 36, train_loss: 0.016962932875832996\n",
      "FOLD: 6, EPOCH: 36, valid_loss: 0.015642873083169643\n",
      "FOLD: 6, EPOCH: 37, train_loss: 0.01683453397472968\n",
      "FOLD: 6, EPOCH: 37, valid_loss: 0.01571501734165045\n",
      "FOLD: 6, EPOCH: 38, train_loss: 0.016766797159672588\n",
      "FOLD: 6, EPOCH: 38, valid_loss: 0.015672435984015465\n",
      "FOLD: 6, EPOCH: 39, train_loss: 0.016624736423427995\n",
      "FOLD: 6, EPOCH: 39, valid_loss: 0.015716026895321332\n",
      "FOLD: 6, EPOCH: 40, train_loss: 0.01653811048615623\n",
      "FOLD: 6, EPOCH: 40, valid_loss: 0.015701437440629188\n",
      "FOLD: 6, EPOCH: 41, train_loss: 0.016422947025480302\n",
      "FOLD: 6, EPOCH: 41, valid_loss: 0.0156660617973942\n",
      "FOLD: 6, EPOCH: 42, train_loss: 0.016347466313557046\n",
      "FOLD: 6, EPOCH: 42, valid_loss: 0.015707149313619502\n",
      "FOLD: 6, EPOCH: 43, train_loss: 0.01621505122229054\n",
      "FOLD: 6, EPOCH: 43, valid_loss: 0.01567101815285591\n",
      "Epoch    44: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 6, EPOCH: 44, train_loss: 0.016044225959056937\n",
      "FOLD: 6, EPOCH: 44, valid_loss: 0.015668143303348467\n",
      "FOLD: 6, EPOCH: 45, train_loss: 0.01583927383998761\n",
      "FOLD: 6, EPOCH: 45, valid_loss: 0.015684635163499758\n",
      "FOLD: 6, EPOCH: 46, train_loss: 0.01578391985212629\n",
      "FOLD: 6, EPOCH: 46, valid_loss: 0.015681908322641484\n",
      "FOLD: 6, EPOCH: 47, train_loss: 0.015715888915331783\n",
      "FOLD: 6, EPOCH: 47, valid_loss: 0.01568790303113369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 6, EPOCH: 48, train_loss: 0.01554540525870146\n",
      "FOLD: 6, EPOCH: 48, valid_loss: 0.015716953847843867\n",
      "FOLD: 6, EPOCH: 49, train_loss: 0.015480161392809572\n",
      "FOLD: 6, EPOCH: 49, valid_loss: 0.015716306507014312\n",
      "FOLD: 0, EPOCH: 0, train_loss: 0.04310849477612489\n",
      "FOLD: 0, EPOCH: 0, valid_loss: 0.019976193228593238\n",
      "FOLD: 0, EPOCH: 1, train_loss: 0.022612012998276466\n",
      "FOLD: 0, EPOCH: 1, valid_loss: 0.01896993242777311\n",
      "FOLD: 0, EPOCH: 2, train_loss: 0.021742603036801558\n",
      "FOLD: 0, EPOCH: 2, valid_loss: 0.0180288150620002\n",
      "FOLD: 0, EPOCH: 3, train_loss: 0.02127743688588207\n",
      "FOLD: 0, EPOCH: 3, valid_loss: 0.017878076921288785\n",
      "FOLD: 0, EPOCH: 4, train_loss: 0.020959328079747187\n",
      "FOLD: 0, EPOCH: 4, valid_loss: 0.017617965546938088\n",
      "FOLD: 0, EPOCH: 5, train_loss: 0.02066002710646874\n",
      "FOLD: 0, EPOCH: 5, valid_loss: 0.01732860985570229\n",
      "FOLD: 0, EPOCH: 6, train_loss: 0.020485257453008276\n",
      "FOLD: 0, EPOCH: 6, valid_loss: 0.01720005660676039\n",
      "FOLD: 0, EPOCH: 7, train_loss: 0.020288947441086575\n",
      "FOLD: 0, EPOCH: 7, valid_loss: 0.016970001590939667\n",
      "FOLD: 0, EPOCH: 8, train_loss: 0.02009957923075637\n",
      "FOLD: 0, EPOCH: 8, valid_loss: 0.01685153041034937\n",
      "FOLD: 0, EPOCH: 9, train_loss: 0.01999469944653479\n",
      "FOLD: 0, EPOCH: 9, valid_loss: 0.016746139153838158\n",
      "FOLD: 0, EPOCH: 10, train_loss: 0.019791564149026934\n",
      "FOLD: 0, EPOCH: 10, valid_loss: 0.016623171404577218\n",
      "FOLD: 0, EPOCH: 11, train_loss: 0.01967918543094719\n",
      "FOLD: 0, EPOCH: 11, valid_loss: 0.016601241050431363\n",
      "FOLD: 0, EPOCH: 12, train_loss: 0.019545780217929465\n",
      "FOLD: 0, EPOCH: 12, valid_loss: 0.01648101991472336\n",
      "FOLD: 0, EPOCH: 13, train_loss: 0.01937144488801022\n",
      "FOLD: 0, EPOCH: 13, valid_loss: 0.016321449612195674\n",
      "FOLD: 0, EPOCH: 14, train_loss: 0.019276230570835037\n",
      "FOLD: 0, EPOCH: 14, valid_loss: 0.016300335741386965\n",
      "FOLD: 0, EPOCH: 15, train_loss: 0.019171431736164802\n",
      "FOLD: 0, EPOCH: 15, valid_loss: 0.01623760607953255\n",
      "FOLD: 0, EPOCH: 16, train_loss: 0.019020560927487707\n",
      "FOLD: 0, EPOCH: 16, valid_loss: 0.0161790969566657\n",
      "FOLD: 0, EPOCH: 17, train_loss: 0.018907394914610964\n",
      "FOLD: 0, EPOCH: 17, valid_loss: 0.01610862807585643\n",
      "FOLD: 0, EPOCH: 18, train_loss: 0.018807115511515655\n",
      "FOLD: 0, EPOCH: 18, valid_loss: 0.016117294677175008\n",
      "FOLD: 0, EPOCH: 19, train_loss: 0.01867503672838211\n",
      "FOLD: 0, EPOCH: 19, valid_loss: 0.016101811725932818\n",
      "FOLD: 0, EPOCH: 20, train_loss: 0.018589476702382434\n",
      "FOLD: 0, EPOCH: 20, valid_loss: 0.016079137101769447\n",
      "FOLD: 0, EPOCH: 21, train_loss: 0.018473601547648776\n",
      "FOLD: 0, EPOCH: 21, valid_loss: 0.016169778142984096\n",
      "FOLD: 0, EPOCH: 22, train_loss: 0.018362023540445277\n",
      "FOLD: 0, EPOCH: 22, valid_loss: 0.01596428919583559\n",
      "FOLD: 0, EPOCH: 23, train_loss: 0.01827032433671726\n",
      "FOLD: 0, EPOCH: 23, valid_loss: 0.01596040207032974\n",
      "FOLD: 0, EPOCH: 24, train_loss: 0.01815107708947884\n",
      "FOLD: 0, EPOCH: 24, valid_loss: 0.015904435243170995\n",
      "FOLD: 0, EPOCH: 25, train_loss: 0.018014314181699947\n",
      "FOLD: 0, EPOCH: 25, valid_loss: 0.015898515541966144\n",
      "FOLD: 0, EPOCH: 26, train_loss: 0.017959880944643472\n",
      "FOLD: 0, EPOCH: 26, valid_loss: 0.015843693955013387\n",
      "FOLD: 0, EPOCH: 27, train_loss: 0.01783065953468149\n",
      "FOLD: 0, EPOCH: 27, valid_loss: 0.015899642943762816\n",
      "FOLD: 0, EPOCH: 28, train_loss: 0.01772206708335796\n",
      "FOLD: 0, EPOCH: 28, valid_loss: 0.015842352492304947\n",
      "FOLD: 0, EPOCH: 29, train_loss: 0.017646175633008417\n",
      "FOLD: 0, EPOCH: 29, valid_loss: 0.015801275722109355\n",
      "FOLD: 0, EPOCH: 30, train_loss: 0.01753761392791529\n",
      "FOLD: 0, EPOCH: 30, valid_loss: 0.015825735834928658\n",
      "FOLD: 0, EPOCH: 31, train_loss: 0.01743392044728672\n",
      "FOLD: 0, EPOCH: 31, valid_loss: 0.015825350124102373\n",
      "FOLD: 0, EPOCH: 32, train_loss: 0.017339072014028963\n",
      "FOLD: 0, EPOCH: 32, valid_loss: 0.015789526514708996\n",
      "FOLD: 0, EPOCH: 33, train_loss: 0.017255841024421358\n",
      "FOLD: 0, EPOCH: 33, valid_loss: 0.01581647003499361\n",
      "FOLD: 0, EPOCH: 34, train_loss: 0.017167793536508404\n",
      "FOLD: 0, EPOCH: 34, valid_loss: 0.01578165855831825\n",
      "FOLD: 0, EPOCH: 35, train_loss: 0.01703906139811954\n",
      "FOLD: 0, EPOCH: 35, valid_loss: 0.015762674908798475\n",
      "FOLD: 0, EPOCH: 36, train_loss: 0.01697391859331244\n",
      "FOLD: 0, EPOCH: 36, valid_loss: 0.015756317629263952\n",
      "FOLD: 0, EPOCH: 37, train_loss: 0.016842749473210926\n",
      "FOLD: 0, EPOCH: 37, valid_loss: 0.015744946108987697\n",
      "FOLD: 0, EPOCH: 38, train_loss: 0.01675138174480683\n",
      "FOLD: 0, EPOCH: 38, valid_loss: 0.015788181111789666\n",
      "FOLD: 0, EPOCH: 39, train_loss: 0.016654982495851612\n",
      "FOLD: 0, EPOCH: 39, valid_loss: 0.01578113680275587\n",
      "FOLD: 0, EPOCH: 40, train_loss: 0.01652645123367374\n",
      "FOLD: 0, EPOCH: 40, valid_loss: 0.015756492502987385\n",
      "FOLD: 0, EPOCH: 41, train_loss: 0.01644443307776709\n",
      "FOLD: 0, EPOCH: 41, valid_loss: 0.015741396193894055\n",
      "FOLD: 0, EPOCH: 42, train_loss: 0.016325390494957164\n",
      "FOLD: 0, EPOCH: 42, valid_loss: 0.015796459279954433\n",
      "FOLD: 0, EPOCH: 43, train_loss: 0.01621841615367983\n",
      "FOLD: 0, EPOCH: 43, valid_loss: 0.015781946265353605\n",
      "FOLD: 0, EPOCH: 44, train_loss: 0.016120652643007202\n",
      "FOLD: 0, EPOCH: 44, valid_loss: 0.015774678223981306\n",
      "FOLD: 0, EPOCH: 45, train_loss: 0.016057100071496255\n",
      "FOLD: 0, EPOCH: 45, valid_loss: 0.015778235947856538\n",
      "FOLD: 0, EPOCH: 46, train_loss: 0.015914380802093325\n",
      "FOLD: 0, EPOCH: 46, valid_loss: 0.015793407049316626\n",
      "FOLD: 0, EPOCH: 47, train_loss: 0.015859462333390036\n",
      "FOLD: 0, EPOCH: 47, valid_loss: 0.015823587631950013\n",
      "FOLD: 0, EPOCH: 48, train_loss: 0.015698320257502632\n",
      "FOLD: 0, EPOCH: 48, valid_loss: 0.01583059153591211\n",
      "Epoch    49: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 0, EPOCH: 49, train_loss: 0.015498925126283555\n",
      "FOLD: 0, EPOCH: 49, valid_loss: 0.01581619930668519\n",
      "FOLD: 1, EPOCH: 0, train_loss: 0.044542263337486496\n",
      "FOLD: 1, EPOCH: 0, valid_loss: 0.020147236350637216\n",
      "FOLD: 1, EPOCH: 1, train_loss: 0.022794631687370507\n",
      "FOLD: 1, EPOCH: 1, valid_loss: 0.01895588808334791\n",
      "FOLD: 1, EPOCH: 2, train_loss: 0.0218878135689207\n",
      "FOLD: 1, EPOCH: 2, valid_loss: 0.01798195721438298\n",
      "FOLD: 1, EPOCH: 3, train_loss: 0.02135353042064486\n",
      "FOLD: 1, EPOCH: 3, valid_loss: 0.017562355989447005\n",
      "FOLD: 1, EPOCH: 4, train_loss: 0.02105808670859079\n",
      "FOLD: 1, EPOCH: 4, valid_loss: 0.017228181545551006\n",
      "FOLD: 1, EPOCH: 5, train_loss: 0.020737426355481148\n",
      "FOLD: 1, EPOCH: 5, valid_loss: 0.017116635751265746\n",
      "FOLD: 1, EPOCH: 6, train_loss: 0.020534476416336524\n",
      "FOLD: 1, EPOCH: 6, valid_loss: 0.016929144183030494\n",
      "FOLD: 1, EPOCH: 7, train_loss: 0.02034643025615731\n",
      "FOLD: 1, EPOCH: 7, valid_loss: 0.01675266629228225\n",
      "FOLD: 1, EPOCH: 8, train_loss: 0.020150954533066298\n",
      "FOLD: 1, EPOCH: 8, valid_loss: 0.01674720415702233\n",
      "FOLD: 1, EPOCH: 9, train_loss: 0.020049661095883395\n",
      "FOLD: 1, EPOCH: 9, valid_loss: 0.016545722332711402\n",
      "FOLD: 1, EPOCH: 10, train_loss: 0.019868566302230228\n",
      "FOLD: 1, EPOCH: 10, valid_loss: 0.016419886611402035\n",
      "FOLD: 1, EPOCH: 11, train_loss: 0.019739994275811558\n",
      "FOLD: 1, EPOCH: 11, valid_loss: 0.016334558837115765\n",
      "FOLD: 1, EPOCH: 12, train_loss: 0.019616375878654623\n",
      "FOLD: 1, EPOCH: 12, valid_loss: 0.016193556169477794\n",
      "FOLD: 1, EPOCH: 13, train_loss: 0.01947441530992856\n",
      "FOLD: 1, EPOCH: 13, valid_loss: 0.016205796542075965\n",
      "FOLD: 1, EPOCH: 14, train_loss: 0.019418871458116417\n",
      "FOLD: 1, EPOCH: 14, valid_loss: 0.016099270074986495\n",
      "FOLD: 1, EPOCH: 15, train_loss: 0.01927565083511778\n",
      "FOLD: 1, EPOCH: 15, valid_loss: 0.01607377318522105\n",
      "FOLD: 1, EPOCH: 16, train_loss: 0.019153879653360392\n",
      "FOLD: 1, EPOCH: 16, valid_loss: 0.015944212245253418\n",
      "FOLD: 1, EPOCH: 17, train_loss: 0.019080988634881134\n",
      "FOLD: 1, EPOCH: 17, valid_loss: 0.015974491763000306\n",
      "FOLD: 1, EPOCH: 18, train_loss: 0.01894392926447295\n",
      "FOLD: 1, EPOCH: 18, valid_loss: 0.015905539505183697\n",
      "FOLD: 1, EPOCH: 19, train_loss: 0.018795349509329408\n",
      "FOLD: 1, EPOCH: 19, valid_loss: 0.01588870069155326\n",
      "FOLD: 1, EPOCH: 20, train_loss: 0.018708142024037\n",
      "FOLD: 1, EPOCH: 20, valid_loss: 0.015821624332322523\n",
      "FOLD: 1, EPOCH: 21, train_loss: 0.018595190567744745\n",
      "FOLD: 1, EPOCH: 21, valid_loss: 0.015779894776642323\n",
      "FOLD: 1, EPOCH: 22, train_loss: 0.018517262956781966\n",
      "FOLD: 1, EPOCH: 22, valid_loss: 0.01573597310254207\n",
      "FOLD: 1, EPOCH: 23, train_loss: 0.018415484904638818\n",
      "FOLD: 1, EPOCH: 23, valid_loss: 0.015771832818595264\n",
      "FOLD: 1, EPOCH: 24, train_loss: 0.018300562165677547\n",
      "FOLD: 1, EPOCH: 24, valid_loss: 0.01573193409981636\n",
      "FOLD: 1, EPOCH: 25, train_loss: 0.018180715624947805\n",
      "FOLD: 1, EPOCH: 25, valid_loss: 0.01568715897603677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 1, EPOCH: 26, train_loss: 0.018084845782534498\n",
      "FOLD: 1, EPOCH: 26, valid_loss: 0.01564470026642084\n",
      "FOLD: 1, EPOCH: 27, train_loss: 0.017994374087130702\n",
      "FOLD: 1, EPOCH: 27, valid_loss: 0.01565960680062954\n",
      "FOLD: 1, EPOCH: 28, train_loss: 0.01794724979412717\n",
      "FOLD: 1, EPOCH: 28, valid_loss: 0.015663012145803526\n",
      "FOLD: 1, EPOCH: 29, train_loss: 0.01777899802687603\n",
      "FOLD: 1, EPOCH: 29, valid_loss: 0.015633107019731633\n",
      "FOLD: 1, EPOCH: 30, train_loss: 0.017722535022609943\n",
      "FOLD: 1, EPOCH: 30, valid_loss: 0.015623601869894909\n",
      "FOLD: 1, EPOCH: 31, train_loss: 0.017581746762467397\n",
      "FOLD: 1, EPOCH: 31, valid_loss: 0.015573444704596814\n",
      "FOLD: 1, EPOCH: 32, train_loss: 0.0174730544456759\n",
      "FOLD: 1, EPOCH: 32, valid_loss: 0.01557600125670433\n",
      "FOLD: 1, EPOCH: 33, train_loss: 0.01741067602022274\n",
      "FOLD: 1, EPOCH: 33, valid_loss: 0.01562482605759914\n",
      "FOLD: 1, EPOCH: 34, train_loss: 0.01731087765782266\n",
      "FOLD: 1, EPOCH: 34, valid_loss: 0.015581342463309947\n",
      "FOLD: 1, EPOCH: 35, train_loss: 0.017202129788898134\n",
      "FOLD: 1, EPOCH: 35, valid_loss: 0.01553540863096714\n",
      "FOLD: 1, EPOCH: 36, train_loss: 0.017124131133125442\n",
      "FOLD: 1, EPOCH: 36, valid_loss: 0.015627921129075382\n",
      "FOLD: 1, EPOCH: 37, train_loss: 0.017013534286839736\n",
      "FOLD: 1, EPOCH: 37, valid_loss: 0.015602862390761193\n",
      "FOLD: 1, EPOCH: 38, train_loss: 0.01688065256520703\n",
      "FOLD: 1, EPOCH: 38, valid_loss: 0.015577775784409963\n",
      "FOLD: 1, EPOCH: 39, train_loss: 0.016791710881768045\n",
      "FOLD: 1, EPOCH: 39, valid_loss: 0.015574483272547904\n",
      "FOLD: 1, EPOCH: 40, train_loss: 0.016687243786715978\n",
      "FOLD: 1, EPOCH: 40, valid_loss: 0.015610090528543178\n",
      "FOLD: 1, EPOCH: 41, train_loss: 0.01659780433653174\n",
      "FOLD: 1, EPOCH: 41, valid_loss: 0.015603199816093994\n",
      "FOLD: 1, EPOCH: 42, train_loss: 0.016487233328154764\n",
      "FOLD: 1, EPOCH: 42, valid_loss: 0.015594153163524775\n",
      "Epoch    43: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 1, EPOCH: 43, train_loss: 0.016228406136301724\n",
      "FOLD: 1, EPOCH: 43, valid_loss: 0.015557194630113931\n",
      "FOLD: 1, EPOCH: 44, train_loss: 0.016169777064508683\n",
      "FOLD: 1, EPOCH: 44, valid_loss: 0.015584376712258045\n",
      "FOLD: 1, EPOCH: 45, train_loss: 0.01610919591542837\n",
      "FOLD: 1, EPOCH: 45, valid_loss: 0.015557063671831902\n",
      "FOLD: 1, EPOCH: 46, train_loss: 0.01597650510233802\n",
      "FOLD: 1, EPOCH: 46, valid_loss: 0.015563379400051557\n",
      "FOLD: 1, EPOCH: 47, train_loss: 0.01588065324451875\n",
      "FOLD: 1, EPOCH: 47, valid_loss: 0.015544505288394598\n",
      "FOLD: 1, EPOCH: 48, train_loss: 0.01583518196098708\n",
      "FOLD: 1, EPOCH: 48, valid_loss: 0.015569221085080734\n",
      "FOLD: 1, EPOCH: 49, train_loss: 0.01570943810951871\n",
      "FOLD: 1, EPOCH: 49, valid_loss: 0.01556511029887658\n",
      "Epoch    50: reducing learning rate of group 0 to 1.1288e-01.\n",
      "FOLD: 2, EPOCH: 0, train_loss: 0.0432305378537323\n",
      "FOLD: 2, EPOCH: 0, valid_loss: 0.02052600194628422\n",
      "FOLD: 2, EPOCH: 1, train_loss: 0.022668479287342447\n",
      "FOLD: 2, EPOCH: 1, valid_loss: 0.018712095701350615\n",
      "FOLD: 2, EPOCH: 2, train_loss: 0.021737934769810858\n",
      "FOLD: 2, EPOCH: 2, valid_loss: 0.017906400733269177\n",
      "FOLD: 2, EPOCH: 3, train_loss: 0.021243067055537895\n",
      "FOLD: 2, EPOCH: 3, valid_loss: 0.017511431199426834\n",
      "FOLD: 2, EPOCH: 4, train_loss: 0.020919654568707622\n",
      "FOLD: 2, EPOCH: 4, valid_loss: 0.017247904020433243\n",
      "FOLD: 2, EPOCH: 5, train_loss: 0.020667251274996513\n",
      "FOLD: 2, EPOCH: 5, valid_loss: 0.017068996452368222\n",
      "FOLD: 2, EPOCH: 6, train_loss: 0.020492150146212126\n",
      "FOLD: 2, EPOCH: 6, valid_loss: 0.016931837854477074\n",
      "FOLD: 2, EPOCH: 7, train_loss: 0.020276871484679146\n",
      "FOLD: 2, EPOCH: 7, valid_loss: 0.01679167215927289\n",
      "FOLD: 2, EPOCH: 8, train_loss: 0.020060945704982087\n",
      "FOLD: 2, EPOCH: 8, valid_loss: 0.016674772979548343\n",
      "FOLD: 2, EPOCH: 9, train_loss: 0.019977702029250765\n",
      "FOLD: 2, EPOCH: 9, valid_loss: 0.01661615431881868\n",
      "FOLD: 2, EPOCH: 10, train_loss: 0.01979417250667875\n",
      "FOLD: 2, EPOCH: 10, valid_loss: 0.01654583366157917\n",
      "FOLD: 2, EPOCH: 11, train_loss: 0.019693500991608645\n",
      "FOLD: 2, EPOCH: 11, valid_loss: 0.016469562497849647\n",
      "FOLD: 2, EPOCH: 12, train_loss: 0.019549362411772884\n",
      "FOLD: 2, EPOCH: 12, valid_loss: 0.016385338125893704\n",
      "FOLD: 2, EPOCH: 13, train_loss: 0.01939893065876252\n",
      "FOLD: 2, EPOCH: 13, valid_loss: 0.01625988997805577\n",
      "FOLD: 2, EPOCH: 14, train_loss: 0.019309995393898036\n",
      "FOLD: 2, EPOCH: 14, valid_loss: 0.01615734104640209\n",
      "FOLD: 2, EPOCH: 15, train_loss: 0.0191575784387218\n",
      "FOLD: 2, EPOCH: 15, valid_loss: 0.016120112787645597\n",
      "FOLD: 2, EPOCH: 16, train_loss: 0.019057380186544882\n",
      "FOLD: 2, EPOCH: 16, valid_loss: 0.016084497436307944\n",
      "FOLD: 2, EPOCH: 17, train_loss: 0.01892484545808386\n",
      "FOLD: 2, EPOCH: 17, valid_loss: 0.01598545299986234\n",
      "FOLD: 2, EPOCH: 18, train_loss: 0.01880275531093011\n",
      "FOLD: 2, EPOCH: 18, valid_loss: 0.01596644864632533\n",
      "FOLD: 2, EPOCH: 19, train_loss: 0.01869361855190348\n",
      "FOLD: 2, EPOCH: 19, valid_loss: 0.01594071498570534\n",
      "FOLD: 2, EPOCH: 20, train_loss: 0.018589930206134513\n",
      "FOLD: 2, EPOCH: 20, valid_loss: 0.015870815644470546\n",
      "FOLD: 2, EPOCH: 21, train_loss: 0.01852331159485353\n",
      "FOLD: 2, EPOCH: 21, valid_loss: 0.015886561730160162\n",
      "FOLD: 2, EPOCH: 22, train_loss: 0.018426683202788636\n",
      "FOLD: 2, EPOCH: 22, valid_loss: 0.015848287381231785\n",
      "FOLD: 2, EPOCH: 23, train_loss: 0.01830405183136463\n",
      "FOLD: 2, EPOCH: 23, valid_loss: 0.015844068991450164\n",
      "FOLD: 2, EPOCH: 24, train_loss: 0.018155305218454952\n",
      "FOLD: 2, EPOCH: 24, valid_loss: 0.0167599546078306\n",
      "FOLD: 2, EPOCH: 25, train_loss: 0.01805327132948347\n",
      "FOLD: 2, EPOCH: 25, valid_loss: 0.01579201278778223\n",
      "FOLD: 2, EPOCH: 26, train_loss: 0.01796416037187383\n",
      "FOLD: 2, EPOCH: 26, valid_loss: 0.015764769453268785\n",
      "FOLD: 2, EPOCH: 27, train_loss: 0.017895379305087233\n",
      "FOLD: 2, EPOCH: 27, valid_loss: 0.015808001303902038\n",
      "FOLD: 2, EPOCH: 28, train_loss: 0.01777354207493969\n",
      "FOLD: 2, EPOCH: 28, valid_loss: 0.015718618192924902\n",
      "FOLD: 2, EPOCH: 29, train_loss: 0.017677351479997504\n",
      "FOLD: 2, EPOCH: 29, valid_loss: 0.015773459266011532\n",
      "FOLD: 2, EPOCH: 30, train_loss: 0.0175738123723784\n",
      "FOLD: 2, EPOCH: 30, valid_loss: 0.01567962049291684\n",
      "FOLD: 2, EPOCH: 31, train_loss: 0.01748913574359707\n",
      "FOLD: 2, EPOCH: 31, valid_loss: 0.015690447547687933\n",
      "FOLD: 2, EPOCH: 32, train_loss: 0.017392221208963846\n",
      "FOLD: 2, EPOCH: 32, valid_loss: 0.01567857153713703\n",
      "FOLD: 2, EPOCH: 33, train_loss: 0.017305403658365075\n",
      "FOLD: 2, EPOCH: 33, valid_loss: 0.0156514083680052\n",
      "FOLD: 2, EPOCH: 34, train_loss: 0.01716957678011543\n",
      "FOLD: 2, EPOCH: 34, valid_loss: 0.015649596659036782\n",
      "FOLD: 2, EPOCH: 35, train_loss: 0.017090892847123985\n",
      "FOLD: 2, EPOCH: 35, valid_loss: 0.015649181360808704\n",
      "FOLD: 2, EPOCH: 36, train_loss: 0.0169810631715164\n",
      "FOLD: 2, EPOCH: 36, valid_loss: 0.01568882683148751\n",
      "FOLD: 2, EPOCH: 37, train_loss: 0.01685017650644924\n",
      "FOLD: 2, EPOCH: 37, valid_loss: 0.015625975094735622\n",
      "FOLD: 2, EPOCH: 38, train_loss: 0.01680689788347966\n",
      "FOLD: 2, EPOCH: 38, valid_loss: 0.01565442341738022\n",
      "FOLD: 2, EPOCH: 39, train_loss: 0.016670576699480816\n",
      "FOLD: 2, EPOCH: 39, valid_loss: 0.015641812664958146\n",
      "FOLD: 2, EPOCH: 40, train_loss: 0.016536988292795582\n",
      "FOLD: 2, EPOCH: 40, valid_loss: 0.015659616328775883\n",
      "FOLD: 2, EPOCH: 41, train_loss: 0.016453361654704488\n",
      "FOLD: 2, EPOCH: 41, valid_loss: 0.015632163733243942\n",
      "FOLD: 2, EPOCH: 42, train_loss: 0.01629685066841744\n",
      "FOLD: 2, EPOCH: 42, valid_loss: 0.01563265246267502\n",
      "FOLD: 2, EPOCH: 43, train_loss: 0.01625774642200889\n",
      "FOLD: 2, EPOCH: 43, valid_loss: 0.015640002102232896\n",
      "FOLD: 2, EPOCH: 44, train_loss: 0.016139952415550076\n",
      "FOLD: 2, EPOCH: 44, valid_loss: 0.015651140362024307\n",
      "Epoch    45: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 2, EPOCH: 45, train_loss: 0.01596242602209787\n",
      "FOLD: 2, EPOCH: 45, valid_loss: 0.015609559316474658\n",
      "FOLD: 2, EPOCH: 46, train_loss: 0.01584889620190134\n",
      "FOLD: 2, EPOCH: 46, valid_loss: 0.015625666110561445\n",
      "FOLD: 2, EPOCH: 47, train_loss: 0.01570128751415256\n",
      "FOLD: 2, EPOCH: 47, valid_loss: 0.015624400013341354\n",
      "FOLD: 2, EPOCH: 48, train_loss: 0.015578893126567473\n",
      "FOLD: 2, EPOCH: 48, valid_loss: 0.01565455860243394\n",
      "FOLD: 2, EPOCH: 49, train_loss: 0.01554453296184137\n",
      "FOLD: 2, EPOCH: 49, valid_loss: 0.015640581098313514\n",
      "FOLD: 3, EPOCH: 0, train_loss: 0.044027099134148776\n",
      "FOLD: 3, EPOCH: 0, valid_loss: 0.02035010247849501\n",
      "FOLD: 3, EPOCH: 1, train_loss: 0.022753050802527246\n",
      "FOLD: 3, EPOCH: 1, valid_loss: 0.019189706358772058\n",
      "FOLD: 3, EPOCH: 2, train_loss: 0.021791879943496472\n",
      "FOLD: 3, EPOCH: 2, valid_loss: 0.018077535124925468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 3, EPOCH: 3, train_loss: 0.021289080633102236\n",
      "FOLD: 3, EPOCH: 3, valid_loss: 0.017772382984940823\n",
      "FOLD: 3, EPOCH: 4, train_loss: 0.02095199897381905\n",
      "FOLD: 3, EPOCH: 4, valid_loss: 0.017516905513520423\n",
      "FOLD: 3, EPOCH: 5, train_loss: 0.020712004999655323\n",
      "FOLD: 3, EPOCH: 5, valid_loss: 0.01726744667841838\n",
      "FOLD: 3, EPOCH: 6, train_loss: 0.020481778233236558\n",
      "FOLD: 3, EPOCH: 6, valid_loss: 0.017157433912731133\n",
      "FOLD: 3, EPOCH: 7, train_loss: 0.020336692573855054\n",
      "FOLD: 3, EPOCH: 7, valid_loss: 0.016975354546537764\n",
      "FOLD: 3, EPOCH: 8, train_loss: 0.020140419789665454\n",
      "FOLD: 3, EPOCH: 8, valid_loss: 0.016943871330183286\n",
      "FOLD: 3, EPOCH: 9, train_loss: 0.020016768695534887\n",
      "FOLD: 3, EPOCH: 9, valid_loss: 0.01675512971213231\n",
      "FOLD: 3, EPOCH: 10, train_loss: 0.019831350048047466\n",
      "FOLD: 3, EPOCH: 10, valid_loss: 0.016732905633174457\n",
      "FOLD: 3, EPOCH: 11, train_loss: 0.019680214265512454\n",
      "FOLD: 3, EPOCH: 11, valid_loss: 0.016589128627226904\n",
      "FOLD: 3, EPOCH: 12, train_loss: 0.019564569147454726\n",
      "FOLD: 3, EPOCH: 12, valid_loss: 0.016541278491226528\n",
      "FOLD: 3, EPOCH: 13, train_loss: 0.01941239254901538\n",
      "FOLD: 3, EPOCH: 13, valid_loss: 0.01652174937323882\n",
      "FOLD: 3, EPOCH: 14, train_loss: 0.019292006193584687\n",
      "FOLD: 3, EPOCH: 14, valid_loss: 0.016452788303677853\n",
      "FOLD: 3, EPOCH: 15, train_loss: 0.01921534077641932\n",
      "FOLD: 3, EPOCH: 15, valid_loss: 0.016315829796859853\n",
      "FOLD: 3, EPOCH: 16, train_loss: 0.01910627864905306\n",
      "FOLD: 3, EPOCH: 16, valid_loss: 0.016288417319838818\n",
      "FOLD: 3, EPOCH: 17, train_loss: 0.01894691100696454\n",
      "FOLD: 3, EPOCH: 17, valid_loss: 0.01622955372127203\n",
      "FOLD: 3, EPOCH: 18, train_loss: 0.018819627377229767\n",
      "FOLD: 3, EPOCH: 18, valid_loss: 0.01621539597041332\n",
      "FOLD: 3, EPOCH: 19, train_loss: 0.01880469570892888\n",
      "FOLD: 3, EPOCH: 19, valid_loss: 0.016193607034018405\n",
      "FOLD: 3, EPOCH: 20, train_loss: 0.018620753207722225\n",
      "FOLD: 3, EPOCH: 20, valid_loss: 0.016110093547747686\n",
      "FOLD: 3, EPOCH: 21, train_loss: 0.01853001772149189\n",
      "FOLD: 3, EPOCH: 21, valid_loss: 0.016114644777889434\n",
      "FOLD: 3, EPOCH: 22, train_loss: 0.018406659792605286\n",
      "FOLD: 3, EPOCH: 22, valid_loss: 0.016097536740394738\n",
      "FOLD: 3, EPOCH: 23, train_loss: 0.01828399162135414\n",
      "FOLD: 3, EPOCH: 23, valid_loss: 0.016070913666715987\n",
      "FOLD: 3, EPOCH: 24, train_loss: 0.01815709292989325\n",
      "FOLD: 3, EPOCH: 24, valid_loss: 0.01602041807312232\n",
      "FOLD: 3, EPOCH: 25, train_loss: 0.018081291503197438\n",
      "FOLD: 3, EPOCH: 25, valid_loss: 0.01600144674571661\n",
      "FOLD: 3, EPOCH: 26, train_loss: 0.01796794224631142\n",
      "FOLD: 3, EPOCH: 26, valid_loss: 0.016014207369433\n",
      "FOLD: 3, EPOCH: 27, train_loss: 0.017897430266178137\n",
      "FOLD: 3, EPOCH: 27, valid_loss: 0.01601297938479827\n",
      "FOLD: 3, EPOCH: 28, train_loss: 0.017771716254788475\n",
      "FOLD: 3, EPOCH: 28, valid_loss: 0.015950050204992294\n",
      "FOLD: 3, EPOCH: 29, train_loss: 0.017687439490613098\n",
      "FOLD: 3, EPOCH: 29, valid_loss: 0.015927713221082322\n",
      "FOLD: 3, EPOCH: 30, train_loss: 0.017588809635993595\n",
      "FOLD: 3, EPOCH: 30, valid_loss: 0.015952113227775462\n",
      "FOLD: 3, EPOCH: 31, train_loss: 0.017468285167942178\n",
      "FOLD: 3, EPOCH: 31, valid_loss: 0.01598910729472454\n",
      "FOLD: 3, EPOCH: 32, train_loss: 0.017396524976435547\n",
      "FOLD: 3, EPOCH: 32, valid_loss: 0.015916776700088613\n",
      "FOLD: 3, EPOCH: 33, train_loss: 0.017237884823132207\n",
      "FOLD: 3, EPOCH: 33, valid_loss: 0.01593183260411024\n",
      "FOLD: 3, EPOCH: 34, train_loss: 0.017172546516097075\n",
      "FOLD: 3, EPOCH: 34, valid_loss: 0.015912488389473695\n",
      "FOLD: 3, EPOCH: 35, train_loss: 0.017043177642532298\n",
      "FOLD: 3, EPOCH: 35, valid_loss: 0.015866813894647818\n",
      "FOLD: 3, EPOCH: 36, train_loss: 0.016966046853544744\n",
      "FOLD: 3, EPOCH: 36, valid_loss: 0.015900765259105425\n",
      "FOLD: 3, EPOCH: 37, train_loss: 0.016839794562877836\n",
      "FOLD: 3, EPOCH: 37, valid_loss: 0.01589988236530469\n",
      "FOLD: 3, EPOCH: 38, train_loss: 0.016742776550755307\n",
      "FOLD: 3, EPOCH: 38, valid_loss: 0.015933509916067123\n",
      "FOLD: 3, EPOCH: 39, train_loss: 0.01666720227866962\n",
      "FOLD: 3, EPOCH: 39, valid_loss: 0.015878384574674644\n",
      "FOLD: 3, EPOCH: 40, train_loss: 0.016529377776424627\n",
      "FOLD: 3, EPOCH: 40, valid_loss: 0.015908381901681423\n",
      "FOLD: 3, EPOCH: 41, train_loss: 0.016404318610659323\n",
      "FOLD: 3, EPOCH: 41, valid_loss: 0.01587662960474308\n",
      "FOLD: 3, EPOCH: 42, train_loss: 0.016337036230676883\n",
      "FOLD: 3, EPOCH: 42, valid_loss: 0.015905802711271323\n",
      "Epoch    43: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 3, EPOCH: 43, train_loss: 0.01615133587069608\n",
      "FOLD: 3, EPOCH: 43, valid_loss: 0.015896857286301944\n",
      "FOLD: 3, EPOCH: 44, train_loss: 0.015998089119691302\n",
      "FOLD: 3, EPOCH: 44, valid_loss: 0.015891239047050476\n",
      "FOLD: 3, EPOCH: 45, train_loss: 0.015908112573261197\n",
      "FOLD: 3, EPOCH: 45, valid_loss: 0.01588716974052099\n",
      "FOLD: 3, EPOCH: 46, train_loss: 0.015835328405169217\n",
      "FOLD: 3, EPOCH: 46, valid_loss: 0.015875645841543492\n",
      "FOLD: 3, EPOCH: 47, train_loss: 0.015675724672808033\n",
      "FOLD: 3, EPOCH: 47, valid_loss: 0.015908411632363614\n",
      "FOLD: 3, EPOCH: 48, train_loss: 0.015627064792489685\n",
      "FOLD: 3, EPOCH: 48, valid_loss: 0.015930117967610177\n",
      "FOLD: 3, EPOCH: 49, train_loss: 0.015545831099655983\n",
      "FOLD: 3, EPOCH: 49, valid_loss: 0.01591152432732857\n",
      "Epoch    50: reducing learning rate of group 0 to 1.1288e-01.\n",
      "FOLD: 4, EPOCH: 0, train_loss: 0.042339913093963184\n",
      "FOLD: 4, EPOCH: 0, valid_loss: 0.020349249100455873\n",
      "FOLD: 4, EPOCH: 1, train_loss: 0.022718589224324032\n",
      "FOLD: 4, EPOCH: 1, valid_loss: 0.019048751833347175\n",
      "FOLD: 4, EPOCH: 2, train_loss: 0.021901786453216464\n",
      "FOLD: 4, EPOCH: 2, valid_loss: 0.018495251496250812\n",
      "FOLD: 4, EPOCH: 3, train_loss: 0.021320283287078946\n",
      "FOLD: 4, EPOCH: 3, valid_loss: 0.01785987228728258\n",
      "FOLD: 4, EPOCH: 4, train_loss: 0.020942996709129295\n",
      "FOLD: 4, EPOCH: 4, valid_loss: 0.017586980015039444\n",
      "FOLD: 4, EPOCH: 5, train_loss: 0.02067739863854808\n",
      "FOLD: 4, EPOCH: 5, valid_loss: 0.017375874118163034\n",
      "FOLD: 4, EPOCH: 6, train_loss: 0.020540280343108886\n",
      "FOLD: 4, EPOCH: 6, valid_loss: 0.017225128383590624\n",
      "FOLD: 4, EPOCH: 7, train_loss: 0.020303557425536013\n",
      "FOLD: 4, EPOCH: 7, valid_loss: 0.0171083604487089\n",
      "FOLD: 4, EPOCH: 8, train_loss: 0.020190371351467597\n",
      "FOLD: 4, EPOCH: 8, valid_loss: 0.016958887473895\n",
      "FOLD: 4, EPOCH: 9, train_loss: 0.019992863608373178\n",
      "FOLD: 4, EPOCH: 9, valid_loss: 0.016816068154114943\n",
      "FOLD: 4, EPOCH: 10, train_loss: 0.019818315106267866\n",
      "FOLD: 4, EPOCH: 10, valid_loss: 0.016795985543957122\n",
      "FOLD: 4, EPOCH: 11, train_loss: 0.019696781361425244\n",
      "FOLD: 4, EPOCH: 11, valid_loss: 0.016701206994744446\n",
      "FOLD: 4, EPOCH: 12, train_loss: 0.01955507051300358\n",
      "FOLD: 4, EPOCH: 12, valid_loss: 0.016614087785665806\n",
      "FOLD: 4, EPOCH: 13, train_loss: 0.01942958015747167\n",
      "FOLD: 4, EPOCH: 13, valid_loss: 0.01654415548993991\n",
      "FOLD: 4, EPOCH: 14, train_loss: 0.01926859951502568\n",
      "FOLD: 4, EPOCH: 14, valid_loss: 0.0164516895579604\n",
      "FOLD: 4, EPOCH: 15, train_loss: 0.019189816295496515\n",
      "FOLD: 4, EPOCH: 15, valid_loss: 0.016431151316143\n",
      "FOLD: 4, EPOCH: 16, train_loss: 0.019013221048422763\n",
      "FOLD: 4, EPOCH: 16, valid_loss: 0.016332852510878675\n",
      "FOLD: 4, EPOCH: 17, train_loss: 0.018929785344044905\n",
      "FOLD: 4, EPOCH: 17, valid_loss: 0.016308693358531363\n",
      "FOLD: 4, EPOCH: 18, train_loss: 0.018808754840614023\n",
      "FOLD: 4, EPOCH: 18, valid_loss: 0.016316537100535173\n",
      "FOLD: 4, EPOCH: 19, train_loss: 0.01870659622993018\n",
      "FOLD: 4, EPOCH: 19, valid_loss: 0.016221986510432683\n",
      "FOLD: 4, EPOCH: 20, train_loss: 0.018546607446026157\n",
      "FOLD: 4, EPOCH: 20, valid_loss: 0.01620391720476059\n",
      "FOLD: 4, EPOCH: 21, train_loss: 0.018490999332956365\n",
      "FOLD: 4, EPOCH: 21, valid_loss: 0.01619770363546335\n",
      "FOLD: 4, EPOCH: 22, train_loss: 0.018394943175686372\n",
      "FOLD: 4, EPOCH: 22, valid_loss: 0.01616999664558814\n",
      "FOLD: 4, EPOCH: 23, train_loss: 0.018216307702902203\n",
      "FOLD: 4, EPOCH: 23, valid_loss: 0.016133655722324666\n",
      "FOLD: 4, EPOCH: 24, train_loss: 0.0181404136977083\n",
      "FOLD: 4, EPOCH: 24, valid_loss: 0.016131072161862485\n",
      "FOLD: 4, EPOCH: 25, train_loss: 0.018004680115326837\n",
      "FOLD: 4, EPOCH: 25, valid_loss: 0.01610131296687401\n",
      "FOLD: 4, EPOCH: 26, train_loss: 0.017960852188234393\n",
      "FOLD: 4, EPOCH: 26, valid_loss: 0.016112477375337712\n",
      "FOLD: 4, EPOCH: 27, train_loss: 0.01784212524826462\n",
      "FOLD: 4, EPOCH: 27, valid_loss: 0.0160925045896035\n",
      "FOLD: 4, EPOCH: 28, train_loss: 0.017756821134605923\n",
      "FOLD: 4, EPOCH: 28, valid_loss: 0.016054930953452222\n",
      "FOLD: 4, EPOCH: 29, train_loss: 0.017647204744453367\n",
      "FOLD: 4, EPOCH: 29, valid_loss: 0.01606208995844309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 4, EPOCH: 30, train_loss: 0.017512010939016536\n",
      "FOLD: 4, EPOCH: 30, valid_loss: 0.01607514975162653\n",
      "FOLD: 4, EPOCH: 31, train_loss: 0.017444819639864807\n",
      "FOLD: 4, EPOCH: 31, valid_loss: 0.016016272326501515\n",
      "FOLD: 4, EPOCH: 32, train_loss: 0.017283484792789898\n",
      "FOLD: 4, EPOCH: 32, valid_loss: 0.016052726584558304\n",
      "FOLD: 4, EPOCH: 33, train_loss: 0.017225608783396514\n",
      "FOLD: 4, EPOCH: 33, valid_loss: 0.016030168805557948\n",
      "FOLD: 4, EPOCH: 34, train_loss: 0.017064027249108295\n",
      "FOLD: 4, EPOCH: 34, valid_loss: 0.016027372473707564\n",
      "FOLD: 4, EPOCH: 35, train_loss: 0.01702087314648403\n",
      "FOLD: 4, EPOCH: 35, valid_loss: 0.0163924338725897\n",
      "FOLD: 4, EPOCH: 36, train_loss: 0.01688576623092632\n",
      "FOLD: 4, EPOCH: 36, valid_loss: 0.016031375011572473\n",
      "FOLD: 4, EPOCH: 37, train_loss: 0.016801027644022897\n",
      "FOLD: 4, EPOCH: 37, valid_loss: 0.016013886134784956\n",
      "FOLD: 4, EPOCH: 38, train_loss: 0.01672727697705095\n",
      "FOLD: 4, EPOCH: 38, valid_loss: 0.016054262048923053\n",
      "FOLD: 4, EPOCH: 39, train_loss: 0.016583757084869855\n",
      "FOLD: 4, EPOCH: 39, valid_loss: 0.01603619080896561\n",
      "FOLD: 4, EPOCH: 40, train_loss: 0.016475795566834307\n",
      "FOLD: 4, EPOCH: 40, valid_loss: 0.016014492139220238\n",
      "FOLD: 4, EPOCH: 41, train_loss: 0.01636534634776212\n",
      "FOLD: 4, EPOCH: 41, valid_loss: 0.01602908646544585\n",
      "FOLD: 4, EPOCH: 42, train_loss: 0.016254439144521148\n",
      "FOLD: 4, EPOCH: 42, valid_loss: 0.01603772878073729\n",
      "FOLD: 4, EPOCH: 43, train_loss: 0.016148463571192446\n",
      "FOLD: 4, EPOCH: 43, valid_loss: 0.01602376617777806\n",
      "FOLD: 4, EPOCH: 44, train_loss: 0.01606364716851228\n",
      "FOLD: 4, EPOCH: 44, valid_loss: 0.01599720184906171\n",
      "FOLD: 4, EPOCH: 45, train_loss: 0.015977157883950183\n",
      "FOLD: 4, EPOCH: 45, valid_loss: 0.016021511015983727\n",
      "FOLD: 4, EPOCH: 46, train_loss: 0.015853405074291938\n",
      "FOLD: 4, EPOCH: 46, valid_loss: 0.016058443257441886\n",
      "FOLD: 4, EPOCH: 47, train_loss: 0.015766610152314644\n",
      "FOLD: 4, EPOCH: 47, valid_loss: 0.016036779118271973\n",
      "FOLD: 4, EPOCH: 48, train_loss: 0.015620861202478409\n",
      "FOLD: 4, EPOCH: 48, valid_loss: 0.016070386609778956\n",
      "FOLD: 4, EPOCH: 49, train_loss: 0.015587916358600597\n",
      "FOLD: 4, EPOCH: 49, valid_loss: 0.016074450686573982\n",
      "FOLD: 5, EPOCH: 0, train_loss: 0.04291332515611036\n",
      "FOLD: 5, EPOCH: 0, valid_loss: 0.02036175676263296\n",
      "FOLD: 5, EPOCH: 1, train_loss: 0.022812973682743473\n",
      "FOLD: 5, EPOCH: 1, valid_loss: 0.01900531566486909\n",
      "FOLD: 5, EPOCH: 2, train_loss: 0.021929634281912365\n",
      "FOLD: 5, EPOCH: 2, valid_loss: 0.018245907213825446\n",
      "FOLD: 5, EPOCH: 3, train_loss: 0.02133304706296405\n",
      "FOLD: 5, EPOCH: 3, valid_loss: 0.017817083316353653\n",
      "FOLD: 5, EPOCH: 4, train_loss: 0.02100137812463013\n",
      "FOLD: 5, EPOCH: 4, valid_loss: 0.017690948425577238\n",
      "FOLD: 5, EPOCH: 5, train_loss: 0.020731828402023058\n",
      "FOLD: 5, EPOCH: 5, valid_loss: 0.017448857283362977\n",
      "FOLD: 5, EPOCH: 6, train_loss: 0.020500786879376787\n",
      "FOLD: 5, EPOCH: 6, valid_loss: 0.017259574137054957\n",
      "FOLD: 5, EPOCH: 7, train_loss: 0.020297558223073546\n",
      "FOLD: 5, EPOCH: 7, valid_loss: 0.0170985169422168\n",
      "FOLD: 5, EPOCH: 8, train_loss: 0.02018262987100595\n",
      "FOLD: 5, EPOCH: 8, valid_loss: 0.017008741601155355\n",
      "FOLD: 5, EPOCH: 9, train_loss: 0.020049385072008985\n",
      "FOLD: 5, EPOCH: 9, valid_loss: 0.016876229299948767\n",
      "FOLD: 5, EPOCH: 10, train_loss: 0.019852717784610955\n",
      "FOLD: 5, EPOCH: 10, valid_loss: 0.016812811104150917\n",
      "FOLD: 5, EPOCH: 11, train_loss: 0.01978127973003162\n",
      "FOLD: 5, EPOCH: 11, valid_loss: 0.016701024312239427\n",
      "FOLD: 5, EPOCH: 12, train_loss: 0.019628243596368545\n",
      "FOLD: 5, EPOCH: 12, valid_loss: 0.01660457267784155\n",
      "FOLD: 5, EPOCH: 13, train_loss: 0.019468953556104285\n",
      "FOLD: 5, EPOCH: 13, valid_loss: 0.016550937667489052\n",
      "FOLD: 5, EPOCH: 14, train_loss: 0.019372987284048182\n",
      "FOLD: 5, EPOCH: 14, valid_loss: 0.016458280599461153\n",
      "FOLD: 5, EPOCH: 15, train_loss: 0.019257310979269648\n",
      "FOLD: 5, EPOCH: 15, valid_loss: 0.016408673201042872\n",
      "FOLD: 5, EPOCH: 16, train_loss: 0.01912723043681802\n",
      "FOLD: 5, EPOCH: 16, valid_loss: 0.016366921723462068\n",
      "FOLD: 5, EPOCH: 17, train_loss: 0.019013446151606134\n",
      "FOLD: 5, EPOCH: 17, valid_loss: 0.01628485730347725\n",
      "FOLD: 5, EPOCH: 18, train_loss: 0.01889310188188746\n",
      "FOLD: 5, EPOCH: 18, valid_loss: 0.016228487213643696\n",
      "FOLD: 5, EPOCH: 19, train_loss: 0.01878506373110655\n",
      "FOLD: 5, EPOCH: 19, valid_loss: 0.016215464673363246\n",
      "FOLD: 5, EPOCH: 20, train_loss: 0.018659514475714515\n",
      "FOLD: 5, EPOCH: 20, valid_loss: 0.016203866841701362\n",
      "FOLD: 5, EPOCH: 21, train_loss: 0.018579027766512858\n",
      "FOLD: 5, EPOCH: 21, valid_loss: 0.016138454182789877\n",
      "FOLD: 5, EPOCH: 22, train_loss: 0.018484707519009307\n",
      "FOLD: 5, EPOCH: 22, valid_loss: 0.01613669298016108\n",
      "FOLD: 5, EPOCH: 23, train_loss: 0.018352347451287346\n",
      "FOLD: 5, EPOCH: 23, valid_loss: 0.016062036299934752\n",
      "FOLD: 5, EPOCH: 24, train_loss: 0.018259838122773816\n",
      "FOLD: 5, EPOCH: 24, valid_loss: 0.016053479093198594\n",
      "FOLD: 5, EPOCH: 25, train_loss: 0.0181590777796668\n",
      "FOLD: 5, EPOCH: 25, valid_loss: 0.016023781437140245\n",
      "FOLD: 5, EPOCH: 26, train_loss: 0.018050306275285578\n",
      "FOLD: 5, EPOCH: 26, valid_loss: 0.016005020373715803\n",
      "FOLD: 5, EPOCH: 27, train_loss: 0.017956171914733744\n",
      "FOLD: 5, EPOCH: 27, valid_loss: 0.016038973314257767\n",
      "FOLD: 5, EPOCH: 28, train_loss: 0.01780666783452034\n",
      "FOLD: 5, EPOCH: 28, valid_loss: 0.015955394707047023\n",
      "FOLD: 5, EPOCH: 29, train_loss: 0.017736018874455948\n",
      "FOLD: 5, EPOCH: 29, valid_loss: 0.015960216952057984\n",
      "FOLD: 5, EPOCH: 30, train_loss: 0.017632448013771226\n",
      "FOLD: 5, EPOCH: 30, valid_loss: 0.01593203111909903\n",
      "FOLD: 5, EPOCH: 31, train_loss: 0.0175298839253751\n",
      "FOLD: 5, EPOCH: 31, valid_loss: 0.015889009460806847\n",
      "FOLD: 5, EPOCH: 32, train_loss: 0.017452889726170012\n",
      "FOLD: 5, EPOCH: 32, valid_loss: 0.0159031546029907\n",
      "FOLD: 5, EPOCH: 33, train_loss: 0.017302767386206903\n",
      "FOLD: 5, EPOCH: 33, valid_loss: 0.015893009992746208\n",
      "FOLD: 5, EPOCH: 34, train_loss: 0.017192390118096326\n",
      "FOLD: 5, EPOCH: 34, valid_loss: 0.015883822352267228\n",
      "FOLD: 5, EPOCH: 35, train_loss: 0.017107032491145906\n",
      "FOLD: 5, EPOCH: 35, valid_loss: 0.015929437099167935\n",
      "FOLD: 5, EPOCH: 36, train_loss: 0.01700744333651823\n",
      "FOLD: 5, EPOCH: 36, valid_loss: 0.0158920672793801\n",
      "FOLD: 5, EPOCH: 37, train_loss: 0.016936337449462026\n",
      "FOLD: 5, EPOCH: 37, valid_loss: 0.015889091560473807\n",
      "FOLD: 5, EPOCH: 38, train_loss: 0.01683742123832171\n",
      "FOLD: 5, EPOCH: 38, valid_loss: 0.01588879432529211\n",
      "FOLD: 5, EPOCH: 39, train_loss: 0.016682088702312997\n",
      "FOLD: 5, EPOCH: 39, valid_loss: 0.015901460383947078\n",
      "FOLD: 5, EPOCH: 40, train_loss: 0.016630697869569867\n",
      "FOLD: 5, EPOCH: 40, valid_loss: 0.015883057091671687\n",
      "FOLD: 5, EPOCH: 41, train_loss: 0.01647826301789767\n",
      "FOLD: 5, EPOCH: 41, valid_loss: 0.0158843112966189\n",
      "FOLD: 5, EPOCH: 42, train_loss: 0.01642445644766495\n",
      "FOLD: 5, EPOCH: 42, valid_loss: 0.015872660379570264\n",
      "FOLD: 5, EPOCH: 43, train_loss: 0.016323847293451026\n",
      "FOLD: 5, EPOCH: 43, valid_loss: 0.015894113251796134\n",
      "FOLD: 5, EPOCH: 44, train_loss: 0.016219305050735537\n",
      "FOLD: 5, EPOCH: 44, valid_loss: 0.015881919158765904\n",
      "FOLD: 5, EPOCH: 45, train_loss: 0.016146271036484756\n",
      "FOLD: 5, EPOCH: 45, valid_loss: 0.015887426284070198\n",
      "FOLD: 5, EPOCH: 46, train_loss: 0.016013298410217505\n",
      "FOLD: 5, EPOCH: 46, valid_loss: 0.015898134989234116\n",
      "FOLD: 5, EPOCH: 47, train_loss: 0.015896493757798058\n",
      "FOLD: 5, EPOCH: 47, valid_loss: 0.015913194690186244\n",
      "FOLD: 5, EPOCH: 48, train_loss: 0.015808995209030202\n",
      "FOLD: 5, EPOCH: 48, valid_loss: 0.01593470910134224\n",
      "FOLD: 5, EPOCH: 49, train_loss: 0.015714446409932664\n",
      "FOLD: 5, EPOCH: 49, valid_loss: 0.015951604654009525\n",
      "Epoch    50: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 6, EPOCH: 0, train_loss: 0.04395803184928121\n",
      "FOLD: 6, EPOCH: 0, valid_loss: 0.02110594195815233\n",
      "FOLD: 6, EPOCH: 1, train_loss: 0.022732262071725483\n",
      "FOLD: 6, EPOCH: 1, valid_loss: 0.01879518973426177\n",
      "FOLD: 6, EPOCH: 2, train_loss: 0.021835117902908777\n",
      "FOLD: 6, EPOCH: 2, valid_loss: 0.01797672275167245\n",
      "FOLD: 6, EPOCH: 3, train_loss: 0.021295387700602814\n",
      "FOLD: 6, EPOCH: 3, valid_loss: 0.01809366405583345\n",
      "FOLD: 6, EPOCH: 4, train_loss: 0.02097615076077951\n",
      "FOLD: 6, EPOCH: 4, valid_loss: 0.017334545604311503\n",
      "FOLD: 6, EPOCH: 5, train_loss: 0.020717363603211737\n",
      "FOLD: 6, EPOCH: 5, valid_loss: 0.017019595234439924\n",
      "FOLD: 6, EPOCH: 6, train_loss: 0.02049767437416154\n",
      "FOLD: 6, EPOCH: 6, valid_loss: 0.017011063531614266\n",
      "FOLD: 6, EPOCH: 7, train_loss: 0.02029095756242404\n",
      "FOLD: 6, EPOCH: 7, valid_loss: 0.016777006909251213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 6, EPOCH: 8, train_loss: 0.02017180241543699\n",
      "FOLD: 6, EPOCH: 8, valid_loss: 0.016769368702975605\n",
      "FOLD: 6, EPOCH: 9, train_loss: 0.020027759082212642\n",
      "FOLD: 6, EPOCH: 9, valid_loss: 0.016610408345094092\n",
      "FOLD: 6, EPOCH: 10, train_loss: 0.01983382598169752\n",
      "FOLD: 6, EPOCH: 10, valid_loss: 0.016547901914096795\n",
      "FOLD: 6, EPOCH: 11, train_loss: 0.019706026827161376\n",
      "FOLD: 6, EPOCH: 11, valid_loss: 0.01647917188417453\n",
      "FOLD: 6, EPOCH: 12, train_loss: 0.019544546542739547\n",
      "FOLD: 6, EPOCH: 12, valid_loss: 0.016391687095165253\n",
      "FOLD: 6, EPOCH: 13, train_loss: 0.01944319755342361\n",
      "FOLD: 6, EPOCH: 13, valid_loss: 0.016302313870535448\n",
      "FOLD: 6, EPOCH: 14, train_loss: 0.01936382324611013\n",
      "FOLD: 6, EPOCH: 14, valid_loss: 0.0162341041633716\n",
      "FOLD: 6, EPOCH: 15, train_loss: 0.01922489851209763\n",
      "FOLD: 6, EPOCH: 15, valid_loss: 0.016158778721896503\n",
      "FOLD: 6, EPOCH: 16, train_loss: 0.019078091114155343\n",
      "FOLD: 6, EPOCH: 16, valid_loss: 0.016132339691886537\n",
      "FOLD: 6, EPOCH: 17, train_loss: 0.018967921895956672\n",
      "FOLD: 6, EPOCH: 17, valid_loss: 0.01604790835139843\n",
      "FOLD: 6, EPOCH: 18, train_loss: 0.018845197518129606\n",
      "FOLD: 6, EPOCH: 18, valid_loss: 0.016028763726353645\n",
      "FOLD: 6, EPOCH: 19, train_loss: 0.0187632776545109\n",
      "FOLD: 6, EPOCH: 19, valid_loss: 0.015998779294582512\n",
      "FOLD: 6, EPOCH: 20, train_loss: 0.018679078341134497\n",
      "FOLD: 6, EPOCH: 20, valid_loss: 0.01598283204321678\n",
      "FOLD: 6, EPOCH: 21, train_loss: 0.018549563136656542\n",
      "FOLD: 6, EPOCH: 21, valid_loss: 0.015917286563378114\n",
      "FOLD: 6, EPOCH: 22, train_loss: 0.01839549985487719\n",
      "FOLD: 6, EPOCH: 22, valid_loss: 0.015956461286315553\n",
      "FOLD: 6, EPOCH: 23, train_loss: 0.018366312054363457\n",
      "FOLD: 6, EPOCH: 23, valid_loss: 0.015889211772726133\n",
      "FOLD: 6, EPOCH: 24, train_loss: 0.018213380556050186\n",
      "FOLD: 6, EPOCH: 24, valid_loss: 0.015937879753227416\n",
      "FOLD: 6, EPOCH: 25, train_loss: 0.018132456553143425\n",
      "FOLD: 6, EPOCH: 25, valid_loss: 0.015893125548385657\n",
      "FOLD: 6, EPOCH: 26, train_loss: 0.01802376894330656\n",
      "FOLD: 6, EPOCH: 26, valid_loss: 0.01584283348459464\n",
      "FOLD: 6, EPOCH: 27, train_loss: 0.017890145926660782\n",
      "FOLD: 6, EPOCH: 27, valid_loss: 0.015805758320941374\n",
      "FOLD: 6, EPOCH: 28, train_loss: 0.017802271891284634\n",
      "FOLD: 6, EPOCH: 28, valid_loss: 0.01578015533204262\n",
      "FOLD: 6, EPOCH: 29, train_loss: 0.01776043720845435\n",
      "FOLD: 6, EPOCH: 29, valid_loss: 0.015789401144362412\n",
      "FOLD: 6, EPOCH: 30, train_loss: 0.017618312726955156\n",
      "FOLD: 6, EPOCH: 30, valid_loss: 0.015759994060947344\n",
      "FOLD: 6, EPOCH: 31, train_loss: 0.01746798143092845\n",
      "FOLD: 6, EPOCH: 31, valid_loss: 0.01573556769066132\n",
      "FOLD: 6, EPOCH: 32, train_loss: 0.017428114582356567\n",
      "FOLD: 6, EPOCH: 32, valid_loss: 0.01578509943703046\n",
      "FOLD: 6, EPOCH: 33, train_loss: 0.017318570948633795\n",
      "FOLD: 6, EPOCH: 33, valid_loss: 0.01569255391279092\n",
      "FOLD: 6, EPOCH: 34, train_loss: 0.01720099584073634\n",
      "FOLD: 6, EPOCH: 34, valid_loss: 0.015722600026772574\n",
      "FOLD: 6, EPOCH: 35, train_loss: 0.01709674787078355\n",
      "FOLD: 6, EPOCH: 35, valid_loss: 0.01569699152157857\n",
      "FOLD: 6, EPOCH: 36, train_loss: 0.016990883074499464\n",
      "FOLD: 6, EPOCH: 36, valid_loss: 0.015718263000822984\n",
      "FOLD: 6, EPOCH: 37, train_loss: 0.01690266955945943\n",
      "FOLD: 6, EPOCH: 37, valid_loss: 0.01569286877146134\n",
      "FOLD: 6, EPOCH: 38, train_loss: 0.016824393860391668\n",
      "FOLD: 6, EPOCH: 38, valid_loss: 0.015694135083602026\n",
      "FOLD: 6, EPOCH: 39, train_loss: 0.016664740189003782\n",
      "FOLD: 6, EPOCH: 39, valid_loss: 0.01567192920125448\n",
      "FOLD: 6, EPOCH: 40, train_loss: 0.016625519124539318\n",
      "FOLD: 6, EPOCH: 40, valid_loss: 0.01567657134280755\n",
      "FOLD: 6, EPOCH: 41, train_loss: 0.01653459286820647\n",
      "FOLD: 6, EPOCH: 41, valid_loss: 0.015703544951975346\n",
      "FOLD: 6, EPOCH: 42, train_loss: 0.016400064429822\n",
      "FOLD: 6, EPOCH: 42, valid_loss: 0.015713654673443392\n",
      "FOLD: 6, EPOCH: 43, train_loss: 0.016287544087783712\n",
      "FOLD: 6, EPOCH: 43, valid_loss: 0.015687970014718864\n",
      "FOLD: 6, EPOCH: 44, train_loss: 0.016233054052635625\n",
      "FOLD: 6, EPOCH: 44, valid_loss: 0.015688138655745067\n",
      "FOLD: 6, EPOCH: 45, train_loss: 0.016123407142790588\n",
      "FOLD: 6, EPOCH: 45, valid_loss: 0.015763335288144074\n",
      "FOLD: 6, EPOCH: 46, train_loss: 0.016005693079047912\n",
      "FOLD: 6, EPOCH: 46, valid_loss: 0.015735438380103845\n",
      "Epoch    47: reducing learning rate of group 0 to 1.6513e-01.\n",
      "FOLD: 6, EPOCH: 47, train_loss: 0.015784956540006237\n",
      "FOLD: 6, EPOCH: 47, valid_loss: 0.015717146559976615\n",
      "FOLD: 6, EPOCH: 48, train_loss: 0.015691503530015815\n",
      "FOLD: 6, EPOCH: 48, valid_loss: 0.015702621868023507\n",
      "FOLD: 6, EPOCH: 49, train_loss: 0.015610178995474771\n",
      "FOLD: 6, EPOCH: 49, valid_loss: 0.01572344982280181\n"
     ]
    }
   ],
   "source": [
    "# Averaging on multiple SEEDS\n",
    "\n",
    "SEED = [0,1,2,3,4,5,6] #<-- Update\n",
    "oof = np.zeros((len(train), len(target_cols)))\n",
    "predictions = np.zeros((len(test), len(target_cols)))\n",
    "\n",
    "for seed in SEED:\n",
    "    params_= params_192\n",
    "    oof_, predictions_ = run_k_fold(NFOLDS, params_, seed)\n",
    "    oof += oof_ / len(SEED)\n",
    "    predictions += predictions_ / len(SEED)\n",
    "\n",
    "train[target_cols] = oof\n",
    "test[target_cols] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T03:15:19.010939Z",
     "iopub.status.busy": "2020-11-04T03:15:19.009758Z",
     "iopub.status.idle": "2020-11-04T03:15:20.222128Z",
     "shell.execute_reply": "2020-11-04T03:15:20.222788Z"
    },
    "papermill": {
     "duration": 2.143111,
     "end_time": "2020-11-04T03:15:20.222959",
     "exception": false,
     "start_time": "2020-11-04T03:15:18.079848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV log_loss:  0.014384479622231678\n"
     ]
    }
   ],
   "source": [
    "valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n",
    "\n",
    "y_true = train_targets_scored[target_cols].values\n",
    "y_pred = valid_results[target_cols].values\n",
    "\n",
    "score = 0\n",
    "for i in range(len(target_cols)):\n",
    "    score_ = log_loss(y_true[:, i], y_pred[:, i])\n",
    "    score += score_ / target.shape[1]\n",
    "    \n",
    "print(\"CV log_loss: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T03:15:23.994705Z",
     "iopub.status.busy": "2020-11-04T03:15:23.993083Z",
     "iopub.status.idle": "2020-11-04T03:15:26.924752Z",
     "shell.execute_reply": "2020-11-04T03:15:26.923948Z"
    },
    "papermill": {
     "duration": 3.879147,
     "end_time": "2020-11-04T03:15:26.924872",
     "exception": false,
     "start_time": "2020-11-04T03:15:23.045725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n",
    "sub.to_csv('submission3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-04T03:15:28.830070Z",
     "iopub.status.busy": "2020-11-04T03:15:28.828835Z",
     "iopub.status.idle": "2020-11-04T03:15:28.835337Z",
     "shell.execute_reply": "2020-11-04T03:15:28.834703Z"
    },
    "papermill": {
     "duration": 0.953107,
     "end_time": "2020-11-04T03:15:28.835456",
     "exception": false,
     "start_time": "2020-11-04T03:15:27.882349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3982, 207)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "duration": 3368.738279,
   "end_time": "2020-11-04T03:15:30.371411",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-04T02:19:21.633132",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
